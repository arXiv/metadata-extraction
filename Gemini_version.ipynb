{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed3a971-84d8-4947-b7a9-19b5a9f23987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "856b9bd6-3960-4078-9e49-90ae039f8513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_pre_abstract_content(tex_file_path):\n",
    "    \"\"\"\n",
    "    Parses a .tex file:\n",
    "    - Removes LaTeX comments\n",
    "    - Extracts institution names\n",
    "    - Extracts text before the abstract\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(tex_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(tex_file_path, \"rb\") as f:\n",
    "            raw_data = f.read(10000)  \n",
    "            result = chardet.detect(raw_data)\n",
    "            detected_encoding = result[\"encoding\"]\n",
    "        try:\n",
    "            with open(tex_file_path, \"r\", encoding=detected_encoding, errors=\"replace\") as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"❌ Failed to read {tex_file_path} with detected encoding {detected_encoding}: {e}\")\n",
    "            return \"\"  \n",
    "\n",
    "    # Remove LaTeX comments\n",
    "    content = re.sub(r\"(?<!\\\\)%.*\", \"\", content)\n",
    "\n",
    "    # Extract institution names\n",
    "    institution_patterns = [\n",
    "        r\"\\\\affiliation{([^}]*)}\",\n",
    "        r\"\\\\institute{([^}]*)}\",\n",
    "        r\"\\\\address{([^}]*)}\",\n",
    "        r\"\\\\inst{([^}]*)}\",\n",
    "        r\"\\\\author\\s*{[^}]+}{([^}]+)}\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    extracted_institutions = []\n",
    "    for pattern in institution_patterns:\n",
    "        matches = re.findall(pattern, content)\n",
    "        if matches:\n",
    "            extracted_institutions.extend(matches)\n",
    "\n",
    "    if extracted_institutions:\n",
    "        return \"\\n\".join(set(extracted_institutions)).strip()\n",
    "\n",
    "    # Extract text before the abstract\n",
    "    match = re.split(r\"\\\\begin\\s*{\\s*abstract\\s*}|\\\\s*\\\\section\\s*{\\s*Abstract\\s*}\", content, maxsplit=1, flags=re.IGNORECASE)\n",
    "    if len(match) > 1:\n",
    "        return match[0].strip()\n",
    "\n",
    "    # Return the first third of the document as a fallback\n",
    "    content_length = len(content)\n",
    "    if content_length > 0:\n",
    "        one_third_length = content_length // 3\n",
    "        return content[:one_third_length].strip()\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5b7f78b-aa7a-42a1-986d-0ca630271a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main.tex']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_files(directory=\".\"):\n",
    "    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "files = list_files(\"2311_tex/2311.12617\")\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08b8594d-50ee-4dca-b28a-832618fab422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$^{1\n"
     ]
    }
   ],
   "source": [
    "match_txt = extract_pre_abstract_content('2311_tex/2311.12617/main.tex')\n",
    "print(match_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df843168-1fe7-4b57-95ba-dd8fd2099083",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"Extract the institutions that the authors of the following LaTeX document are affiliated with.\\n\\n\"\n",
    "    \"### STRICT OUTPUT REQUIREMENTS:\\n\"\n",
    "    \"1. Extract ONLY the institution names associated with the authors, with no additional text.\\n\"\n",
    "    \"2. Ignore research collaborations, projects, or experiments. \\n\"\n",
    "    \"3. Format: Each academic institution must be numbered on a new line, exactly as follows:\\n\"\n",
    "    \"   1. Institution Name 1\\n\"\n",
    "    \"   2. Institution Name 2\\n\"\n",
    "    \"4. If NO institutions can be found, return exactly:\\n\"\n",
    "    \"   null\\n\"\n",
    "    \"5. DO NOT include explanations, descriptions, or any other text—ONLY the numbered list or 'null'.\\n\\n\"\n",
    "    \"### INPUT TEXT:\\n\"\n",
    "    \"{input_text}\\n\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af057e9-389e-4d56-b646-5ae2eedc9131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "PROJECT_ID = \"arxiv-development\" \n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "\n",
    "def query_gemini_api(input_text):\n",
    "    \"\"\"\n",
    "    Sends a request to the Gemini API to extract potential institution names.\n",
    "    \"\"\"\n",
    "    prompt = PROMPT_TEMPLATE.format(input_text=input_text)\n",
    "    \n",
    "    start_time = time.time()  \n",
    "    response = model.generate_content(prompt)  \n",
    "    end_time = time.time()  \n",
    "    \n",
    "    timecost = end_time - start_time \n",
    "\n",
    "    if response and response.text:\n",
    "        clean_response = response.text  \n",
    "        # print(f\"Execution time: {timecost:.4f} seconds\")\n",
    "        return clean_response\n",
    "    else:\n",
    "        print(\"API request failed or empty response\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64c3928-1054-4137-b2eb-1d075abd2d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'match_txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_gemini_api(\u001b[43mmatch_txt\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'match_txt' is not defined"
     ]
    }
   ],
   "source": [
    "query_gemini_api(match_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e6c7370-7adf-4257-8f73-20a515467c65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Total Tokens for 1000 folders: 1876526\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "PROJECT_ID = \"arxiv-development\"\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "\n",
    "def estimate_total_tokens(root_dir, max_files=500):\n",
    "    \"\"\"\n",
    "     Estimates the total number of tokens for the max_files file\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    valid_folders = []\n",
    "    \n",
    "    for folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            tex_files = [f for f in os.listdir(folder_path) if f.endswith(\".tex\")]\n",
    "            if tex_files:\n",
    "                valid_folders.append(folder)\n",
    "\n",
    "    valid_folders = valid_folders[:max_files]\n",
    "\n",
    "    for folder in valid_folders:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        tex_files = [f for f in os.listdir(folder_path) if f.endswith(\".tex\")]\n",
    "\n",
    "        if \"main.tex\" in tex_files:\n",
    "            tex_files = [\"main.tex\"]\n",
    "\n",
    "        for tex_file in tex_files:\n",
    "            tex_path = os.path.join(folder_path, tex_file)\n",
    "            extracted_text = extract_pre_abstract_content(tex_path)\n",
    "            if not extracted_text:\n",
    "                continue\n",
    "            \n",
    "            response = model.count_tokens(extracted_text)\n",
    "            total_tokens += response.total_tokens\n",
    "\n",
    "    print(f\"Estimated Total Tokens for {max_files} folders: {total_tokens}\")\n",
    "    return total_tokens\n",
    "\n",
    "root_directory = \"2311_tex\"\n",
    "estimated_tokens = estimate_total_tokens(root_directory, max_files=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da5f5238-c7f5-4738-8a59-3f7d1df34fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_single_folder(folder_path, folder, pbar, lock):\n",
    "    results = []\n",
    "    tex_files = [f for f in os.listdir(folder_path) if f.endswith(\".tex\")]\n",
    "    if \"main.tex\" in tex_files:\n",
    "        tex_files = [\"main.tex\"]\n",
    "\n",
    "    found_institutions = False\n",
    "\n",
    "    for tex_file in tex_files:\n",
    "        if found_institutions:\n",
    "            break  \n",
    "\n",
    "        tex_path = os.path.join(folder_path, tex_file)\n",
    "        \n",
    "        extracted_text = extract_pre_abstract_content(tex_path)\n",
    "        if not extracted_text:\n",
    "            with lock:\n",
    "                pbar.update(1)  \n",
    "            continue\n",
    "\n",
    "        institutions = query_gemini_api(extracted_text)\n",
    "        if institutions and institutions.strip().lower() != \"null\":\n",
    "            for institution in institutions.split(\"\\n\"):\n",
    "                clean_name = institution.strip()\n",
    "                if clean_name:\n",
    "                    results.append((folder, clean_name))\n",
    "                    found_institutions = True\n",
    "            break  \n",
    "\n",
    "        with lock:\n",
    "            pbar.update(1) \n",
    "\n",
    "    if not found_institutions:\n",
    "        results.append((folder, \"null\"))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7862d7df-5065-42f6-9b06-0108f731a73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import threading  \n",
    "\n",
    "def process_tex_files(root_dir, max_files=500, max_workers=5):\n",
    "    start_time = time.time()\n",
    "    valid_folders = []\n",
    "\n",
    "    for folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            tex_files = [f for f in os.listdir(folder_path) if f.endswith(\".tex\")]\n",
    "            if tex_files:\n",
    "                valid_folders.append(folder)  \n",
    "    valid_folders = valid_folders[:max_files] \n",
    "    estimated_tex_files = len(valid_folders)  \n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock() \n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor, tqdm(total=estimated_tex_files, desc=\"Processing .tex files\") as pbar:\n",
    "        future_to_folder = {\n",
    "            executor.submit(process_single_folder, os.path.join(root_dir, folder), folder, pbar, lock): folder\n",
    "            for folder in valid_folders\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_folder):\n",
    "            folder = future_to_folder[future]\n",
    "            try:\n",
    "                folder_results = future.result()\n",
    "                results.extend(folder_results)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing folder '{folder}': {e}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"✅ Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9389563-e2ad-49bc-89e9-b95ee78c020e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_to_excel(results, output_file):\n",
    "    \"\"\"\n",
    "    Save the results to an Excel file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results, columns=[\"File ID\", \"Institution\"])\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Results have been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05686d09-0c56-4516-a63a-c746dba2771a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing .tex files:  61%|██████▏   | 614/1000 [00:24<00:15, 25.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total processing time: 24.35 seconds\n",
      "Results have been saved to affiliations_gemini_1000.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"2311_tex\"\n",
    "output_excel = \"affiliations_gemini_1000.xlsx\"\n",
    "\n",
    "\n",
    "results = process_tex_files(root_directory, max_files=1000, max_workers=20)\n",
    "save_to_excel(results, output_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60710df3-cdf6-44c3-84d6-0ab8dd63246f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Institution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2311.12617</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>1. The Chinese University of Hong Kong, Shenzhen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>2. Tsinghua University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>3. Simon Fraser University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2311.14992</td>\n",
       "      <td>1. China University of Petroleum (East China)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>2311.09042</td>\n",
       "      <td>1. University of West Bohemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>2311.09042</td>\n",
       "      <td>2. Kitasato University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>2311.09042</td>\n",
       "      <td>3. Ishinomaki Senshu University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>2311.09042</td>\n",
       "      <td>4. Yokohama National University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>2311.09042</td>\n",
       "      <td>5. Seikei Universtiy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1141 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         File ID                                       Institution\n",
       "0     2311.12617                                               NaN\n",
       "1     2311.06812  1. The Chinese University of Hong Kong, Shenzhen\n",
       "2     2311.06812                            2. Tsinghua University\n",
       "3     2311.06812                        3. Simon Fraser University\n",
       "4     2311.14992     1. China University of Petroleum (East China)\n",
       "...          ...                                               ...\n",
       "1136  2311.09042                     1. University of West Bohemia\n",
       "1137  2311.09042                            2. Kitasato University\n",
       "1138  2311.09042                   3. Ishinomaki Senshu University\n",
       "1139  2311.09042                   4. Yokohama National University\n",
       "1140  2311.09042                              5. Seikei Universtiy\n",
       "\n",
       "[1141 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"affiliations_test.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acb86dd4-48d6-43e4-b481-ca0d4fb33ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "max_workers = min(32, multiprocessing.cpu_count() + 4)\n",
    "print(max_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23fe018e-5dc2-431d-a4cb-a7c2f2eb2976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_paper_extraction(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    num_papers = df[\"File ID\"].nunique()\n",
    "    num_nulls = df[\"Institution\"].isna().sum()\n",
    "    null_percentage = (num_nulls / len(df)) * 100\n",
    "    print(f\"Total Papers Extracted: {num_papers}\")\n",
    "    print(f\"Total Null Entries: {num_nulls}\")\n",
    "    print(f\"Null Percentage: {null_percentage:.2f}%\")\n",
    "    \n",
    "    return num_papers, num_nulls, null_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43f33566-adf5-440b-ab27-6bdaac24a5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Papers Extracted: 1000\n",
      "Total Null Entries: 129\n",
      "Null Percentage: 5.17%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 129, 5.168269230769231)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"affiliations_gemini_1000.xlsx\"  \n",
    "analyze_paper_extraction(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c797c8-27b8-4800-93f4-ad2e6059fdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
