{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  9 22:34:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API server address\n",
    "API_URL = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "def extract_pre_abstract_content(tex_file_path):\n",
    "    \"\"\"\n",
    "    Parses a .tex file:\n",
    "    1. Removes LaTeX comments (everything after '%').\n",
    "    2. Prioritizes extracting content from institution-related tags.\n",
    "    3. If no institution tags are found, extracts content before the abstract.\n",
    "    4. If there is no abstract, returns the first one-third of the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(tex_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(tex_file_path, \"rb\") as f:\n",
    "            raw_data = f.read(10000)  \n",
    "            result = chardet.detect(raw_data)\n",
    "            detected_encoding = result[\"encoding\"]\n",
    "        \n",
    "        print(f\"Detected encoding: {detected_encoding} for file {tex_file_path}\")\n",
    "        try:\n",
    "            with open(tex_file_path, \"r\", encoding=detected_encoding, errors=\"replace\") as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {tex_file_path} with detected encoding {detected_encoding}: {e}\")\n",
    "            return \"\"  \n",
    "\n",
    "    #  Remove LaTeX comments\n",
    "    content = re.sub(r\"(?<!\\\\)%.*\", \"\", content)\n",
    "\n",
    "    institution_patterns = [\n",
    "        r\"\\\\affiliation{([^}]*)}\",\n",
    "        r\"\\\\institute{([^}]*)}\",\n",
    "        r\"\\\\address{([^}]*)}\",\n",
    "        r\"\\\\inst{([^}]*)}\",\n",
    "        r\"\\\\author\\s*{[^}]+}{([^}]+)}\"\n",
    "    ]\n",
    "\n",
    "    extracted_institutions = []\n",
    "    for pattern in institution_patterns:\n",
    "        matches = re.findall(pattern, content)\n",
    "        if matches:\n",
    "            extracted_institutions.extend(matches)\n",
    "\n",
    "    if extracted_institutions:\n",
    "        return \"\\n\".join(set(extracted_institutions)).strip()\n",
    "\n",
    "    # Extract the content before abstract\n",
    "    match = re.split(r\"\\\\begin\\s*{\\s*abstract\\s*}|\\\\s*\\\\section\\s*{\\s*Abstract\\s*}\", content, maxsplit=1, flags=re.IGNORECASE)\n",
    "\n",
    "    if len(match) > 1:\n",
    "        return match[0].strip()\n",
    "\n",
    "    # Return 1/3 content\n",
    "    content_length = len(content)\n",
    "    if content_length > 0:\n",
    "        one_third_length = content_length // 3\n",
    "        return content[:one_third_length].strip()\n",
    "\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\documentclass[conference,9pt]{IEEEtran}\n",
      "\\IEEEoverridecommandlockouts\n",
      "\n",
      "\\usepackage{cite}\n",
      "\\usepackage{amsmath,amssymb,amsfonts}\n",
      "\\usepackage{algorithmic}\n",
      "\\usepackage{graphicx}\n",
      "\\usepackage{textcomp}\n",
      "\\usepackage{xcolor}\n",
      "\\usepackage{algorithm}\n",
      "\\usepackage{booktabs}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{array}\n",
      "\\usepackage{colortbl}\n",
      "\\usepackage{makecell}\n",
      "\\usepackage{pifont}\n",
      "\\usepackage{makecell}\n",
      "\\usepackage{xcolor,colortbl}\n",
      "\n",
      "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
      "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
      "\n",
      "\n",
      "    \\colorlet{mygreen}{green!60!gray}\n",
      "\n",
      "    \\newif\\ifcomments\n",
      "\n",
      "\n",
      "\\commentstrue\n",
      "\n",
      "\n",
      "\\ifcomments\n",
      "\n",
      "    \\newcommand{\\BTcomm}[1]{\\textcolor{mygreen}{{#1}}}\n",
      "    \\newcommand{\\MB}[1]{\\textcolor{magenta}{{MB: #1}}}\n",
      "    \\newcommand{\\fjwresp}[1]{\\textcolor{blue}{#1}}\n",
      "    \\newcommand{\\fjwrespv}[1]{\\textcolor{red}{#1}}\n",
      "\n",
      "\\else\n",
      "    \\newcommand{\\BTcomm}[1]{}\n",
      "    \\newcommand{\\MB}[1]{}\n",
      "    \\newcommand{\\fjwresp}[1]{}\n",
      "    \\newcommand{\\fjwrespv}[1]{}\n",
      "\n",
      "\\fi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\title{Robust Retraining-free GAN Fingerprinting via Personalized Normalization}\n",
      "\n",
      "\\author{\n",
      "\\IEEEauthorblockN{Jianwei Fei}\n",
      "\\IEEEauthorblockA{\\textit{Jinan University} \\\\\n",
      "Guangzhou, China}\n",
      "\\and\n",
      "\\IEEEauthorblockN{Zhihua Xia \\thanks{ Zhihua Xia is the corresponding author.}\\IEEEauthorrefmark{2}}\n",
      "\\IEEEauthorblockA{\\textit{Jinan University} \\\\\n",
      "Guangzhou, China}\n",
      "\\and\n",
      "\\IEEEauthorblockN{Benedetta Tondi}\n",
      "\\IEEEauthorblockA{\\textit{University of Siena} \\\\\n",
      "Siena, Italy}\n",
      "\\and\n",
      "\\IEEEauthorblockN{Mauro Barni}\n",
      "\\IEEEauthorblockA{\\textit{University of Siena} \\\\\n",
      "Siena, Italy}\n",
      "}\n",
      "\n",
      "\\maketitle\n"
     ]
    }
   ],
   "source": [
    "match_txt = extract_pre_abstract_content('2311_tex/2311.05478/Submission4-v3.tex')\n",
    "print(match_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION: FUNCTION preprocess_tex_content DO NOT USE ANY MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_tex_content(content):\n",
    "    \"\"\"\n",
    "    Preprocesses LaTeX text by removing elements that may mislead an LLM, \n",
    "    such as section titles, headings, and figure captions.\n",
    "    \"\"\"\n",
    "    # Remove \\title{} to prevent misinterpretation\n",
    "    content = re.sub(r\"\\\\title{.*?}\", \"\", content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove \\section{} and \\subsection{} to avoid incorrect classification\n",
    "    content = re.sub(r\"\\\\(sub)*section{.*?}\", \"\", content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove \\caption{} to prevent confusion with institution names\n",
    "    content = re.sub(r\"\\\\caption{.*?}\", \"\", content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove LaTeX comments\n",
    "    content = re.sub(r\"%.*\", \"\", content)\n",
    "\n",
    "    return content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{latexsym}\n",
      "\\usepackage{amscd, amsfonts,\n",
      "  eucal, mathrsfs, amsmath, amssymb, amsthm}\n",
      "\\input xy\n",
      "\\xyoption{all}\n",
      "\\def\\li{\\baselineskip}\n",
      "\n",
      "\n",
      "\n",
      "\\usepackage{fullpage}\n",
      "\\usepackage{tikz-cd}\n",
      "\\usepackage{extarrows}\n",
      "\\tikzset{\n",
      "  labl/.style={anchor=south, rotate=90, inner sep=.5mm}\n",
      "}\n",
      "\\tikzset{\n",
      "  labr/.style={anchor=north, rotate=90, inner sep=1mm}\n",
      "}\n",
      "\n",
      "\\usepackage{mathtools}\n",
      "\\usepackage{bm}\n",
      "\n",
      "\\usepackage{array}\n",
      "\\setlength{\\extrarowheight}{4pt}\n",
      "\n",
      "\\setlength{\\multlinegap}{50pt}\n",
      "\n",
      "\n",
      "\n",
      "\\usepackage{color}\n",
      "\n",
      "\n",
      "\\usepackage[cal=boondoxo, scr=boondoxo]{mathalfa}\n",
      "\n",
      "\\usepackage[sort,nocompress]{cite}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\usepackage{hyperref}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\frenchspacing\n",
      "\n",
      "\\newcommand{\\perf}{\\text{\\rm perf}}\n",
      "\n",
      "\\DeclareMathOperator{\\BaseLocus}{Bs}\n",
      "\\DeclareMathOperator{\\End}{End}\n",
      "\n",
      "\\newcommand{\\Comp}{\\operatorname{\\text{\\rm Comp}}}\n",
      "\\newcommand{\\ConnComp}{\\operatorname{\\text{\\rm ConnComp}}}\n",
      "\n",
      "\n",
      "\\newcommand{\\pic}[0]{\\operatorname{Pic}}\n",
      "\\newcommand{\\chow}[0]{\\operatorname{Chow}}\n",
      "\\newcommand{\\tsum}[0]{\\textstyle{\\sum}}\n",
      "\\DeclareMathOperator{\\im}{Im}\n",
      "\\DeclareMathOperator{\\supp}{supp}\n",
      "\\DeclareMathOperator{\\Supp}{Supp}\n",
      "\n",
      "\\DeclareMathOperator{\\Span}{Span}\n",
      "\n",
      "\\DeclareMathOperator{\\Alb}{Alb}\n",
      "\\DeclareMathOperator{\\Hilb}{Hilb}\n",
      "\n",
      "\\DeclareMathOperator{\\Bl}{Bl}\n",
      "\n",
      "\n",
      "\\newcommand{\\field}[1]{\\mathbb #1}\n",
      "\\newcommand{\\mf}[1]{\\mathfrak #1}\n",
      "\\newcommand{\\mc}[1]{\\mathcal #1}\n",
      "\\newcommand{\\ms}[1]{\\mathcal #1}\n",
      "\\newcommand{\\widebar}[1]{\\overline{#1}}\n",
      "\\newcommand{\\met}{\\text{\\rm \\'et}}\n",
      "\n",
      "\\renewcommand{\\phi}{\\varphi}\n",
      "\n",
      "\\newcommand{\\red}{\\text{\\rm red}}\n",
      "\n",
      "\\def\\risom{\\overset{\\sim}{\\rightarrow}}\n",
      "\\def\\lisom{\\overset{\\sim}{\\leftarrow}}\n",
      "\\def\\liml{\\varprojlim}\n",
      "\\def\\limr{\\varinjlim}\n",
      "\\DeclareMathOperator{\\Proj}{Proj}\n",
      "\\DeclareMathOperator{\\Sym}{Sym}\n",
      "\n",
      "\\newcommand{\\lovely}{definable}\n",
      "\n",
      "\\newcommand{\\Torelli}{\\ms T}\n",
      "\\newcommand{\\STorelli}{\\widetilde{\\ms T}}\n",
      "\\newcommand{\\AbVar}{\\mathbf{\\mathcal{V\\!a\\!r}}}\n",
      "\\newcommand{\\NAbVar}{\\mathbf{\\mathcal N\\!}\\AbVar}\n",
      "\\newcommand{\\Definable}{\\mathbf{\\mathcal{D\\!e\\!f}}}\n",
      "\n",
      "\n",
      "\\DeclareMathOperator{\\PicCat}{\\mathscr P}\n",
      "\\DeclareMathOperator{\\ProjPicCat}{\\overline{\\mathscr P}}\n",
      "\\DeclareMathOperator{\\RatPicCat}{\\mathscr R}\n",
      "\\DeclareMathOperator{\\DivGp}{\\mathscr D}\n",
      "\\DeclareMathOperator{\\Div}{Div}\n",
      "\\DeclareMathOperator{\\Eff}{Eff}\n",
      "\\DeclareMathOperator{\\AmpleBPF}{Abpf}\n",
      "\\DeclareMathOperator{\\AmpleBPFI}{Abpfi}\n",
      "\\DeclareMathOperator{\\Cl}{Cl}\n",
      "\\DeclareMathOperator{\\Flab}{\\mathcal{F\\!l\\!\\!a\\!b}}\n",
      "\\DeclareMathOperator{\\INS}{\\mathbf{INS}}\n",
      "\\DeclareMathOperator{\\IP}{\\mathbf{IP}\n"
     ]
    }
   ],
   "source": [
    "txt = preprocess_tex_content(match_txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_deepseek_response(response_text):\n",
    "    # print(\"🔥 Original LLM Response:\")\n",
    "    # print(response_text)\n",
    "    # print(\"=\" * 50)\n",
    "    \n",
    "    # 1) Remove the <think>...</think> block\n",
    "    cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", response_text, flags=re.DOTALL).strip()\n",
    "    \n",
    "    # 2) Split by lines and remove empty lines\n",
    "    lines = cleaned_text.splitlines()\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    # 3) (Optional) Remove numbering at the beginning such as \"1. \"\n",
    "    #    If you want to keep the original lines, skip this step\n",
    "    institution_lines = [re.sub(r\"^\\d+\\.\\s*\", \"\", line) for line in lines]\n",
    "    \n",
    "    # 4) If there are no lines left, return \"null\"\n",
    "    extracted_institutions = \"\\n\".join(institution_lines) if institution_lines else \"null\"\n",
    "    \n",
    "    print(\"✅ Extracted Institution Names:\")\n",
    "    print(extracted_institutions)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return extracted_institutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS PROMPT IS FOR THE DEEPSEEK 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"Extract the academic institutions that the authors of the following LaTeX document are affiliated with.\\n\\n\"\n",
    "    \"### STRICT OUTPUT REQUIREMENTS:\\n\"\n",
    "    \"1. Extract ONLY the academic institution names associated with the authors, with no additional text.\\n\"\n",
    "    \"2. Ignore research collaborations, projects, or experiments. \\n\"\n",
    "    \"3. Format: Each academic institution must be numbered on a new line, exactly as follows:\\n\"\n",
    "    \"   1. Institution Name 1\\n\"\n",
    "    \"   2. Institution Name 2\\n\"\n",
    "    \"4. If NO academic institutions can be found, return exactly:\\n\"\n",
    "    \"   null\\n\"\n",
    "    \"5. DO NOT include explanations, descriptions, or any other text—ONLY the numbered list or 'null'.\\n\\n\"\n",
    "    \"### INPUT TEXT:\\n\"\n",
    "    \"{input_text}\\n\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS PROMPT IS FOR THE DEEPSEEK 1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"### TASK:\\n\"\n",
    "    \"Extract the name of the institution (university or institute, etc.) to which the author belongs in the given article.\\n\"\n",
    "    \"If no institution is found, output exactly `null`.\\n\\n\"\n",
    "    \"There is no need to consider the correspondence between the author and the institution, only the name of the institution needs to be extracted.\"\n",
    "\n",
    "    \"### RULES:\\n\"\n",
    "    \"1. Do not include author names, emails, or any other text in your final answer.\\n\"\n",
    "    \"2. Output each institution on a new line, numbered sequentially.\"\n",
    "    \"3. The only two possibilities for the output are null or the name of the organization.\"\n",
    "\n",
    "    \"### Article :\\n\"\n",
    "    \"{input_text}\\n\\n\"\n",
    "     \"Once you find all the author and their institutions, just stop thinking and output the institutino. Do Not think too much\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "def query_deepseek_api(input_text):\n",
    "    \"\"\"\n",
    "    Sends a request to the DeepSeek API to extract potential institution names.\n",
    "    \"\"\"\n",
    "    # input_text = preprocess_tex_content(input_text)\n",
    "    prompt = PROMPT_TEMPLATE.format(input_text=input_text)\n",
    "    start_time = time.time()  \n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1:7b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,  # Retrieve the full response\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_k\": 40,\n",
    "        \"top_p\": 1.0,\n",
    "        \"repeat_penalty\": 1.1,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, json=payload)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    timecost = end_time - start_time\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        clean_response = clean_deepseek_response(data.get(\"response\", \"\"))\n",
    "        print(f\"Execution time: {timecost:.4f} seconds\")\n",
    "        return clean_response\n",
    "    else:\n",
    "        print(\"API request failed:\", response.status_code, response.text)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted Institution Names:\n",
      "Jinan University\n",
      "University of Siena\n",
      "==================================================\n",
      "Execution time: 6.8690 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jinan University\\nUniversity of Siena'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_deepseek_api(match_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def process_tex_files(root_dir, max_files=5):\n",
    "#     \"\"\"\n",
    "#     Processes multiple .tex files and store the results up to max_files directories).\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     counter = 0  # Counter to limit processing to max_files directories\n",
    "\n",
    "#     for folder in os.listdir(root_dir):\n",
    "#         folder_path = os.path.join(root_dir, folder)\n",
    "#         if not os.path.isdir(folder_path):\n",
    "#             continue  # Skip non-directory entries\n",
    "\n",
    "#         tex_files = [f for f in os.listdir(folder_path) if f.endswith(\".tex\")]\n",
    "\n",
    "#         if not tex_files:\n",
    "#             continue  # Skip directories without .tex files\n",
    "\n",
    "#         # Limit processing to max_files directories\n",
    "#         if counter >= max_files:\n",
    "#             break\n",
    "\n",
    "#         counter += 1\n",
    "\n",
    "#         # Select the .tex file(s) to process\n",
    "#         if \"main.tex\" in tex_files:\n",
    "#             tex_files = [\"main.tex\"]  # Prioritize processing main.tex\n",
    "#         else:\n",
    "#             tex_files = tex_files  # Process all .tex files\n",
    "\n",
    "#         found_institutions = False  # Flag to track if any institution names were found\n",
    "\n",
    "#         for tex_file in tex_files:\n",
    "#             tex_path = os.path.join(folder_path, tex_file)\n",
    "#             print(f\"Processing: {tex_path}\")\n",
    "\n",
    "#             # Extract relevant content from the .tex file\n",
    "#             extracted_text = extract_pre_abstract_content(tex_path)\n",
    "#             if not extracted_text:\n",
    "#                 continue\n",
    "\n",
    "#             # Query the DeepSeek API\n",
    "#             institutions = query_deepseek_api(extracted_text)\n",
    "\n",
    "#             # Split response into lines and remove empty entries\n",
    "#             if institutions and institutions.strip().lower() != \"null\":\n",
    "#                 for institution in institutions.split(\"\\n\"):\n",
    "#                     clean_name = institution.strip()\n",
    "#                     if clean_name:  # Ensure only non-empty names are stored\n",
    "#                         results.append((folder, clean_name))\n",
    "#                         found_institutions = True  # Mark that institutions were found\n",
    "                        \n",
    "#         # If no institution names were found in any .tex file, store \"null\"\n",
    "#         if not found_institutions:\n",
    "#             results.append((folder, \"null\"))\n",
    "#         print(results)\n",
    "\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_single_folder(folder_path, folder):\n",
    "    \"\"\"\n",
    "    step：\n",
    "    1. If there is a main.tex file in the folder, only process main.tex.\n",
    "    2. If there is no main.tex, then as soon as an institution name is successfully extracted,\n",
    "       stop processing subsequent .tex files.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    tex_files = [f for f in os.listdir(folder_path) if f.endswith(\".tex\")]\n",
    "    if \"main.tex\" in tex_files:\n",
    "        tex_files = [\"main.tex\"]\n",
    "\n",
    "    found_institutions = False\n",
    "\n",
    "    for tex_file in tex_files:\n",
    "        if found_institutions:\n",
    "            break\n",
    "\n",
    "        tex_path = os.path.join(folder_path, tex_file)\n",
    "        print(f\"[Thread] Processing: {tex_path}\")\n",
    "        \n",
    "        extracted_text = extract_pre_abstract_content(tex_path)\n",
    "        if not extracted_text:\n",
    "            continue\n",
    "\n",
    "        institutions = query_deepseek_api(extracted_text)\n",
    "        if institutions and institutions.strip().lower() != \"null\":\n",
    "            for institution in institutions.split(\"\\n\"):\n",
    "                clean_name = institution.strip()\n",
    "                if clean_name:\n",
    "                    results.append((folder, clean_name))\n",
    "                    found_institutions = True\n",
    "            # Once the first file with an institution is found, break immediately\n",
    "            break\n",
    "\n",
    "    if not found_institutions:\n",
    "        results.append((folder, \"null\"))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_tex_files(root_dir, max_files=5, max_workers=4):\n",
    "\n",
    "    start_time = time.time() \n",
    "\n",
    "    # Identify folders that contain .tex files\n",
    "    valid_folders = []\n",
    "    for folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            tex_files = [f for f in os.listdir(folder_path) if f.endswith(\".tex\")]\n",
    "            if tex_files:\n",
    "                valid_folders.append(folder)\n",
    "\n",
    "    # Limit the maximum number of folders to process\n",
    "    valid_folders = valid_folders[:max_files]\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_folder = {\n",
    "            executor.submit(process_single_folder, os.path.join(root_dir, folder), folder): folder\n",
    "            for folder in valid_folders\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_folder):\n",
    "            folder = future_to_folder[future]\n",
    "            try:\n",
    "                folder_results = future.result()\n",
    "                results.extend(folder_results)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing folder '{folder}': {e}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"🤔🤔Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_to_excel(results, output_file):\n",
    "    \"\"\"\n",
    "    Save the results to an Excel file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results, columns=[\"File ID\", \"Institution\"])\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Results have been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def save_to_xml(results, output_file):\n",
    "#     \"\"\" XML \"\"\"\n",
    "#     root = ET.Element(\"Institutions\")\n",
    "\n",
    "#     for file_id, institution in results:\n",
    "#         # make sure institution is not empty\n",
    "#         if not institution.strip():\n",
    "#             continue\n",
    "\n",
    "#         entry = ET.SubElement(root, \"Entry\")\n",
    "#         ET.SubElement(entry, \"FileID\").text = file_id\n",
    "#         ET.SubElement(entry, \"Institution\").text = institution\n",
    "\n",
    "#     tree = ET.ElementTree(root)\n",
    "#     tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "#     print(f\"The result has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread] Processing: 2311_tex/2311.06812/final.tex\n",
      "[Thread] Processing: 2311_tex/2311.14992/final_version.tex\n",
      "[Thread] Processing: 2311_tex/2311.12617/main.tex\n",
      "[Thread] Processing: 2311_tex/2311.17452/preamble.tex\n",
      "Detected encoding: GB2312 for file 2311_tex/2311.14992/final_version.tex\n",
      "✅ Extracted Institution Names:\n",
      "null\n",
      "==================================================\n",
      "Execution time: 21.2317 seconds\n",
      "✅ Extracted Institution Names:\n",
      "null\n",
      "==================================================\n",
      "Execution time: 33.0065 seconds\n",
      "[Thread] Processing: 2311_tex/2311.17452/abel_nat.tex\n",
      "✅ Extracted Institution Names:\n",
      "Shenzhen Future Network of Intelligence Institute (FNii-Shenzhen)\n",
      "School of Science and Engineering (SSE), The Chinese University of Hong Kong, China\n",
      "Shenzhen International Graduate School, Tsinghua University, China\n",
      "Simon Fraser University\n",
      "Guangdong Provincial Key Laboratory of Future Networks of Intelligence\n",
      "==================================================\n",
      "Execution time: 37.7870 seconds\n",
      "✅ Extracted Institution Names:\n",
      "College of New Energy, China University of Petroleum (East China)\n",
      "College of New Energy, China University of Petroleum (East China)\n",
      "College of New Energy, China University of Petroleum (East China)\n",
      "Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology\n",
      "==================================================\n",
      "Execution time: 41.3309 seconds\n",
      "✅ Extracted Institution Names:\n",
      "null\n",
      "==================================================\n",
      "Execution time: 12.9083 seconds\n",
      "🤔🤔Total processing time: 46.18 seconds\n",
      "结果已保存到 affiliations_test.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root_directory = \"2311_tex\"\n",
    "output_excel = \"affiliations_test.xlsx\"\n",
    "\n",
    "\n",
    "results = process_tex_files(root_directory, max_files=4, max_workers=4)\n",
    "save_to_excel(results, output_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Institution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2311.12617</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>Shenzhen Future Network of Intelligence Instit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>School of Science and Engineering (SSE), The C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>Shenzhen International Graduate School, Tsingh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>Simon Fraser University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2311.06812</td>\n",
       "      <td>Guangdong Provincial Key Laboratory of Future ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2311.14992</td>\n",
       "      <td>College of New Energy, China University of Pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2311.14992</td>\n",
       "      <td>College of New Energy, China University of Pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2311.14992</td>\n",
       "      <td>College of New Energy, China University of Pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2311.14992</td>\n",
       "      <td>Department of Electronic and Computer Engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2311.17452</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       File ID                                        Institution\n",
       "0   2311.12617                                                NaN\n",
       "1   2311.06812  Shenzhen Future Network of Intelligence Instit...\n",
       "2   2311.06812  School of Science and Engineering (SSE), The C...\n",
       "3   2311.06812  Shenzhen International Graduate School, Tsingh...\n",
       "4   2311.06812                            Simon Fraser University\n",
       "5   2311.06812  Guangdong Provincial Key Laboratory of Future ...\n",
       "6   2311.14992  College of New Energy, China University of Pet...\n",
       "7   2311.14992  College of New Energy, China University of Pet...\n",
       "8   2311.14992  College of New Energy, China University of Pet...\n",
       "9   2311.14992  Department of Electronic and Computer Engineer...\n",
       "10  2311.17452                                                NaN"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"affiliations_test.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$^{1\n"
     ]
    }
   ],
   "source": [
    "match_txt = extract_pre_abstract_content('2311_tex/2311.12617/main.tex')\n",
    "print(match_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\documentclass[12pt, a4paper]{amsart}\n",
      "\\input{preamble.tex}\n",
      "\n",
      "\n",
      "\\author{Yuya Sasaki}\n",
      "\\title{Nonnatural automorphisms of the Hilbert scheme of two points of\n",
      "some simple abelian variety}\n",
      "\n",
      "\\begin{document}\n"
     ]
    }
   ],
   "source": [
    "match_txt = extract_pre_abstract_content('2311_tex/2311.17452/abel_nat.tex')\n",
    "print(match_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
