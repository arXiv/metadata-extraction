{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "      Primary Org Id                              Primary Org Name  \\\n",
      "0           60030612           University of California, San Diego   \n",
      "1           60017604  Siberian Branch, Russian Academy of Sciences   \n",
      "2           60023932               University of Technology Sydney   \n",
      "3           60016849                         University of Toronto   \n",
      "4           60024542          South China University of Technology   \n",
      "...              ...                                           ...   \n",
      "2896        60017246           Eberhard Karls Universität Tübingen   \n",
      "2897        60123796                         Université Paris Cité   \n",
      "2898        60123796                         Université Paris Cité   \n",
      "2899        60025578                             Xidian University   \n",
      "2900        60005816                 South China Normal University   \n",
      "\n",
      "     Primary Org City   Primary Org State Primary Org Country      ArXiv Id  \\\n",
      "0            La Jolla                  CA       United States  2201.00850v2   \n",
      "1         Novosibirsk  Novosibirsk Oblast  Russian Federation  2201.00465v2   \n",
      "2              Sydney                 NSW           Australia  2201.00989v2   \n",
      "3             Toronto                  ON              Canada  2201.00382v3   \n",
      "4           Guangzhou           Guangdong               China  2201.00298v1   \n",
      "...               ...                 ...                 ...           ...   \n",
      "2896         Tubingen   Baden-Wurttemberg             Germany  2201.00069v1   \n",
      "2897            Paris       Ile-de-France              France  2201.00069v1   \n",
      "2898            Paris       Ile-de-France              France  2201.00885v3   \n",
      "2899            Xi'an             Shaanxi               China  2201.00122v2   \n",
      "2900        Guangzhou           Guangdong               China  2201.00623v2   \n",
      "\n",
      "      Affiliation Sequence Number    paper_id  \n",
      "0                               1  2201.00850  \n",
      "1                               2  2201.00465  \n",
      "2                               1  2201.00989  \n",
      "3                               1  2201.00382  \n",
      "4                               1  2201.00298  \n",
      "...                           ...         ...  \n",
      "2896                           35  2201.00069  \n",
      "2897                           25  2201.00069  \n",
      "2898                            6  2201.00885  \n",
      "2899                            1  2201.00122  \n",
      "2900                            1  2201.00623  \n",
      "\n",
      "[2901 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file. Adjust the path if needed.\n",
    "df = pd.read_csv(\"data/2201.00_scopus_931.csv\")\n",
    "\n",
    "# Read the blacklist file (assuming one organization per line)\n",
    "with open(\"data/blacklist_parent_organizations.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    blacklist = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Convert the blacklist to lowercase for case-insensitive matching.\n",
    "blacklist_lower = [org.lower() for org in blacklist]\n",
    "\n",
    "# Assuming the institution name is in a column called 'institution'.\n",
    "institution_col = 'Primary Org Name'\n",
    "\n",
    "# Create a boolean mask for institutions that appear in the blacklist.\n",
    "mask_blacklist = df[institution_col].str.lower().isin(blacklist_lower)\n",
    "\n",
    "# Create a boolean mask for institutions that contain both \"university\" and \"system\" (case-insensitive).\n",
    "mask_university_system = (\n",
    "    df[institution_col].str.lower().str.contains('university', na=False) &\n",
    "    df[institution_col].str.lower().str.contains('system', na=False)\n",
    ")\n",
    "\n",
    "# Create a boolean mask for institutions that contain \"Government of India\" (case-insensitive).\n",
    "mask_govt_india = df[institution_col].str.lower().str.contains('government of india', na=False)\n",
    "\n",
    "# Combine masks: mark for removal if any condition is met.\n",
    "mask_remove = mask_blacklist | mask_university_system | mask_govt_india\n",
    "\n",
    "# Filter out the records to be removed.\n",
    "df_filtered = df[~mask_remove].copy()\n",
    "\n",
    "# Optional: Reset index if desired.\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file.\n",
    "df_filtered.to_csv(\"data/2201.00_scopus_931.csv\", index=False)\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched 'Primary Org Id' values: 27\n",
      "0.9906928645294726\n",
      "Matching completed. The output is saved as 'data/2201.00_scopus_931_with_ror.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "scopus_data = pd.read_csv('data/2201.00_scopus_931.csv')\n",
    "ror_mapping = pd.read_csv('matching_data/matched_results_ror_api.csv')\n",
    "\n",
    "# Merge the datasets on 'Primary Org Id'\n",
    "merged_data = pd.merge(\n",
    "    scopus_data,\n",
    "    ror_mapping,\n",
    "    on='Primary Org Id',\n",
    "    how='left'  # Use 'left' to keep all rows from scopus_data even if no match is found\n",
    ")\n",
    "\n",
    "# Calculate the number of unmatched rows\n",
    "unmatched_count = merged_data['ROR ID'].isna().sum()\n",
    "\n",
    "print(f\"Number of unmatched 'Primary Org Id' values: {unmatched_count}\")\n",
    "print(1-unmatched_count/len(merged_data))\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('data/2201.00_scopus_931_with_ror.csv', index=False)\n",
    "\n",
    "print(\"Matching completed. The output is saved as 'data/2201.00_scopus_931_with_ror.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ArXiv IDs: 931\n",
      "Found text files: 931\n",
      "Percentage: 100.00%\n",
      "\n",
      "Missing files:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "csv_path = 'data/2201.00_scopus_931_with_ror.csv'\n",
    "text_folder = 'data/2201_00_text'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Assuming the column containing ArXiv IDs is named 'ArXiv Id'\n",
    "arxiv_ids = df['ArXiv Id'].dropna().unique()  # Drop NaN values and get unique IDs\n",
    "\n",
    "# Initialize counters and a list to store missing files\n",
    "found_count = 0\n",
    "total_count = len(arxiv_ids)\n",
    "missing_files = []\n",
    "\n",
    "# Check for each ArXiv ID\n",
    "for arxiv_id in arxiv_ids:\n",
    "    txt_file_path = os.path.join(text_folder, f\"{arxiv_id}.txt\")\n",
    "    if os.path.isfile(txt_file_path):\n",
    "        found_count += 1\n",
    "    else:\n",
    "        missing_files.append(arxiv_id)\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (found_count / total_count) * 100\n",
    "\n",
    "# Display results\n",
    "print(f\"Total ArXiv IDs: {total_count}\")\n",
    "print(f\"Found text files: {found_count}\")\n",
    "print(f\"Percentage: {percentage:.2f}%\")\n",
    "print(\"\\nMissing files:\")\n",
    "for missing_id in missing_files:\n",
    "    print(missing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to data/preprocessed_scopus_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data/2201.00_scopus_931_with_ror.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select the desired columns\n",
    "columns_to_keep = ['Primary Org Id', 'Primary Org Name_x', 'ArXiv Id', 'ROR ID']\n",
    "df_selected = df[columns_to_keep]\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "output_file_path = 'data/preprocessed_scopus_data.csv'\n",
    "df_selected.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from extractors.trie_extractor import TrieExtractor\n",
    "from utils.file_reader import read_file\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"data/2201.00_scopus_931_with_ror.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Initialize the TrieExtractor\n",
    "extractor = TrieExtractor(data_path=\"data/1.34_extracted_ror_data.csv\", common_words_path=\"data/common_english_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2901/2901 [00:07<00:00, 364.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Updated CSV saved.\n"
     ]
    }
   ],
   "source": [
    "# Function to extract ROR ID from a given text file\n",
    "def extract_ror_id(paper_id):\n",
    "    file_path = f\"data/2201_00_text/{paper_id}.txt\"\n",
    "    try:\n",
    "        text = read_file(file_path)\n",
    "        affiliations = extractor.extract_affiliations(text)\n",
    "        # Assuming the first affiliation's ROR ID is the one we want\n",
    "        return affiliations if affiliations else None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Apply the extraction function to each paper_id with tqdm progress bar\n",
    "tqdm.pandas()  # Enable tqdm for pandas\n",
    "df['Extracted ROR ID'] = df['ArXiv Id'].progress_apply(extract_ror_id)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Extraction complete. Updated CSV saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://ror.org/00q4vv597', 'https://ror.org/00se2k293', 'https://ror.org/023rhb549'}\n"
     ]
    }
   ],
   "source": [
    "affiliations = extractor.extract_affiliations(read_file(f\"data/2201_00_text/2201.00008v3.txt\"))\n",
    "print(affiliations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52\n",
      "Wrong Extraction Rate: 0.19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('data/2201.00_scopus_931_with_ror.csv')\n",
    "\n",
    "# Function to preprocess Extracted ROR ID\n",
    "def preprocess_extracted_ror_id(extracted_ror_id):\n",
    "    if pd.isna(extracted_ror_id):  # Skip NaN values\n",
    "        return set()\n",
    "    # Remove curly braces and split by comma\n",
    "    extracted_ror_id = extracted_ror_id.strip(\"{}\").split(\",\")\n",
    "    # Remove any leading/trailing whitespace and filter out empty strings\n",
    "    return set(item.strip().strip(\"''\") for item in extracted_ror_id if item.strip())\n",
    "\n",
    "# Step 2: Group by ArXiv ID and merge ROR ID and Extracted ROR ID into sets\n",
    "grouped = df.groupby('ArXiv Id').agg({\n",
    "    'ROR ID': lambda x: set(x.dropna()),  # Create a set of ROR IDs\n",
    "    'Extracted ROR ID': lambda x: set().union(*x.apply(preprocess_extracted_ror_id))  # Preprocess and union Extracted ROR IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Calculate accuracy and wrong extraction rate\n",
    "correct_extractions = 0\n",
    "wrong_extractions = 0\n",
    "total_ror_ids = 0\n",
    "\n",
    "for index, row in grouped.iterrows():\n",
    "    ror_ids = row['ROR ID']\n",
    "    extracted_ror_ids = row['Extracted ROR ID']\n",
    "    if ror_ids:  # Only consider rows with non-empty ROR ID\n",
    "        total_ror_ids += len(ror_ids)\n",
    "        correct_extractions += len(ror_ids.intersection(extracted_ror_ids))\n",
    "        wrong_extractions += len(extracted_ror_ids - ror_ids)  # IDs in extraction but not in ground truth\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "\n",
    "# Calculate wrong extraction rate\n",
    "wrong_extraction_rate = wrong_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Wrong Extraction Rate: {wrong_extraction_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatched cases: 596\n",
      "Mismatched cases saved to 'mismatched_cases.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('data/2201.00_scopus_931_with_ror.csv')\n",
    "\n",
    "# Function to preprocess Extracted ROR ID\n",
    "def preprocess_extracted_ror_id(extracted_ror_id):\n",
    "    if pd.isna(extracted_ror_id):  # Skip NaN values\n",
    "        return set()\n",
    "    # Remove curly braces and split by comma\n",
    "    extracted_ror_id = extracted_ror_id.strip(\"{}\").split(\",\")\n",
    "    # Remove any leading/trailing whitespace and filter out empty strings\n",
    "    return set(item.strip().strip(\"''\") for item in extracted_ror_id if item.strip())\n",
    "\n",
    "# Step 2: Group by ArXiv ID and merge ROR ID and Extracted ROR ID into sets\n",
    "grouped = df.groupby('ArXiv Id').agg({\n",
    "    'ROR ID': lambda x: set(x.dropna()),  # Create a set of ROR IDs\n",
    "    'Extracted ROR ID': lambda x: set().union(*x.apply(preprocess_extracted_ror_id))  # Preprocess and union Extracted ROR IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Collect cases where extracted ROR IDs do not perfectly match ground truth ROR IDs\n",
    "mismatched_cases = []\n",
    "\n",
    "for index, row in grouped.iterrows():\n",
    "    ror_ids = row['ROR ID']\n",
    "    extracted_ror_ids = row['Extracted ROR ID']\n",
    "    if ror_ids != extracted_ror_ids:  # Check for imperfect match\n",
    "        mismatched_cases.append({\n",
    "            'ArXiv Id': row['ArXiv Id'],\n",
    "            'Ground Truth ROR IDs': ror_ids,\n",
    "            'Extracted ROR IDs': extracted_ror_ids,\n",
    "            'Missing ROR IDs': ror_ids - extracted_ror_ids,  # IDs in ground truth but not in extraction\n",
    "            'Extra ROR IDs': extracted_ror_ids - ror_ids  # IDs in extraction but not in ground truth\n",
    "        })\n",
    "\n",
    "# Convert the list of mismatched cases to a DataFrame\n",
    "mismatched_df = pd.DataFrame(mismatched_cases)\n",
    "\n",
    "# Save the mismatched cases to a CSV file\n",
    "mismatched_df.to_csv('mismatched_cases.csv', index=False)\n",
    "\n",
    "print(f\"Total mismatched cases: {len(mismatched_df)}\")\n",
    "print(\"Mismatched cases saved to 'mismatched_cases.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent missing ROR IDs:\n",
      "ROR ID: https://ror.org/03k9qs827, Count: 20\n",
      "ROR ID: https://ror.org/03xjwb503, Count: 14\n",
      "ROR ID: https://ror.org/02en5vm52, Count: 12\n",
      "ROR ID: https://ror.org/05f82e368, Count: 10\n",
      "ROR ID: https://ror.org/04cw6st05, Count: 10\n",
      "ROR ID: https://ror.org/031npqv35, Count: 7\n",
      "ROR ID: https://ror.org/0281dp749, Count: 7\n",
      "ROR ID: https://ror.org/003hx7248, Count: 7\n",
      "ROR ID: https://ror.org/01rk35k63, Count: 7\n",
      "ROR ID: https://ror.org/0161xgx34, Count: 7\n",
      "\n",
      "Most frequent extra ROR IDs:\n",
      "ROR ID: https://ror.org/00hx57361, Count: 14\n",
      "ROR ID: https://ror.org/052gg0110, Count: 12\n",
      "ROR ID: https://ror.org/02eb0rk31, Count: 9\n",
      "ROR ID: https://ror.org/0257kt353, Count: 9\n",
      "ROR ID: https://ror.org/01t466c14, Count: 7\n",
      "ROR ID: https://ror.org/05g1zjw44, Count: 7\n",
      "ROR ID: https://ror.org/02zhqgq86, Count: 7\n",
      "ROR ID: https://ror.org/05vt9qd57, Count: 6\n",
      "ROR ID: https://ror.org/0090r4d87, Count: 6\n",
      "ROR ID: https://ror.org/04ynn1b95, Count: 6\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calculate the most frequent missing and extra ROR IDs\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten the lists of missing and extra ROR IDs\n",
    "missing_ror_ids = [ror_id for case in mismatched_cases for ror_id in case['Missing ROR IDs']]\n",
    "extra_ror_ids = [ror_id for case in mismatched_cases for ror_id in case['Extra ROR IDs']]\n",
    "\n",
    "# Count the frequency of each ROR ID\n",
    "missing_ror_id_counts = Counter(missing_ror_ids)\n",
    "extra_ror_id_counts = Counter(extra_ror_ids)\n",
    "\n",
    "# Get the most common missing and extra ROR IDs\n",
    "most_common_missing = missing_ror_id_counts.most_common(10)  # Adjust the number as needed\n",
    "most_common_extra = extra_ror_id_counts.most_common(10)  # Adjust the number as needed\n",
    "\n",
    "# Print the results\n",
    "print(\"Most frequent missing ROR IDs:\")\n",
    "for ror_id, count in most_common_missing:\n",
    "    print(f\"ROR ID: {ror_id}, Count: {count}\")\n",
    "\n",
    "print(\"\\nMost frequent extra ROR IDs:\")\n",
    "for ror_id, count in most_common_extra:\n",
    "    print(f\"ROR ID: {ror_id}, Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
