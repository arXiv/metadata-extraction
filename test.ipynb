{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched 'Primary Org Id' values: 24\n",
      "0.9930090300029129\n",
      "Matching completed. The output is saved as 'data/2201.00_scopus_931_with_ror.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "scopus_data = pd.read_csv('data/2201.00_scopus_931.csv')\n",
    "ror_mapping = pd.read_csv('matching_data/matched_results_fuzzy.csv')\n",
    "\n",
    "# Merge the datasets on 'Primary Org Id'\n",
    "merged_data = pd.merge(\n",
    "    scopus_data,\n",
    "    ror_mapping,\n",
    "    on='Primary Org Id',\n",
    "    how='left'  # Use 'left' to keep all rows from scopus_data even if no match is found\n",
    ")\n",
    "\n",
    "# Calculate the number of unmatched rows\n",
    "unmatched_count = merged_data['ROR ID'].isna().sum()\n",
    "\n",
    "print(f\"Number of unmatched 'Primary Org Id' values: {unmatched_count}\")\n",
    "print(1-unmatched_count/len(merged_data))\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('data/2201.00_scopus_931_with_ror.csv', index=False)\n",
    "\n",
    "print(\"Matching completed. The output is saved as 'data/2201.00_scopus_931_with_ror.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ArXiv IDs: 931\n",
      "Found text files: 931\n",
      "Percentage: 100.00%\n",
      "\n",
      "Missing files:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "csv_path = 'data/2201.00_scopus_931_with_ror.csv'\n",
    "text_folder = 'data/2201_00_text'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Assuming the column containing ArXiv IDs is named 'ArXiv Id'\n",
    "arxiv_ids = df['ArXiv Id'].dropna().unique()  # Drop NaN values and get unique IDs\n",
    "\n",
    "# Initialize counters and a list to store missing files\n",
    "found_count = 0\n",
    "total_count = len(arxiv_ids)\n",
    "missing_files = []\n",
    "\n",
    "# Check for each ArXiv ID\n",
    "for arxiv_id in arxiv_ids:\n",
    "    txt_file_path = os.path.join(text_folder, f\"{arxiv_id}.txt\")\n",
    "    if os.path.isfile(txt_file_path):\n",
    "        found_count += 1\n",
    "    else:\n",
    "        missing_files.append(arxiv_id)\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (found_count / total_count) * 100\n",
    "\n",
    "# Display results\n",
    "print(f\"Total ArXiv IDs: {total_count}\")\n",
    "print(f\"Found text files: {found_count}\")\n",
    "print(f\"Percentage: {percentage:.2f}%\")\n",
    "print(\"\\nMissing files:\")\n",
    "for missing_id in missing_files:\n",
    "    print(missing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to data/preprocessed_scopus_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data/2201.00_scopus_931_with_ror.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select the desired columns\n",
    "columns_to_keep = ['Primary Org Id', 'Primary Org Name_x', 'ArXiv Id', 'ROR ID']\n",
    "df_selected = df[columns_to_keep]\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "output_file_path = 'data/preprocessed_scopus_data.csv'\n",
    "df_selected.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from extractors.trie_extractor import TrieExtractor\n",
    "from utils.file_reader import read_file\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"data/2201.00_scopus_931_with_ror.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Initialize the TrieExtractor\n",
    "extractor = TrieExtractor(data_path=\"data/1.34_extracted_ror_data.csv\", common_words_path=\"data/common_english_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3433/3433 [00:09<00:00, 354.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Updated CSV saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to extract ROR ID from a given text file\n",
    "def extract_ror_id(paper_id):\n",
    "    file_path = f\"data/2201_00_text/{paper_id}.txt\"\n",
    "    try:\n",
    "        text = read_file(file_path)\n",
    "        affiliations = extractor.extract_affiliations(text)\n",
    "        # Assuming the first affiliation's ROR ID is the one we want\n",
    "        return affiliations if affiliations else None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Apply the extraction function to each paper_id with tqdm progress bar\n",
    "tqdm.pandas()  # Enable tqdm for pandas\n",
    "df['Extracted ROR ID'] = df['ArXiv Id'].progress_apply(extract_ror_id)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Extraction complete. Updated CSV saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41\n",
      "Wrong Extraction Rate: 0.17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('data/2201.00_scopus_931_with_ror.csv')\n",
    "\n",
    "# Function to preprocess Extracted ROR ID\n",
    "def preprocess_extracted_ror_id(extracted_ror_id):\n",
    "    if pd.isna(extracted_ror_id):  # Skip NaN values\n",
    "        return set()\n",
    "    # Remove curly braces and split by comma\n",
    "    extracted_ror_id = extracted_ror_id.strip(\"{}\").split(\",\")\n",
    "    # Remove any leading/trailing whitespace and filter out empty strings\n",
    "    return set(item.strip().strip(\"''\") for item in extracted_ror_id if item.strip())\n",
    "\n",
    "# Step 2: Group by ArXiv ID and merge ROR ID and Extracted ROR ID into sets\n",
    "grouped = df.groupby('ArXiv Id').agg({\n",
    "    'ROR ID': lambda x: set(x.dropna()),  # Create a set of ROR IDs\n",
    "    'Extracted ROR ID': lambda x: set().union(*x.apply(preprocess_extracted_ror_id))  # Preprocess and union Extracted ROR IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Calculate accuracy and wrong extraction rate\n",
    "correct_extractions = 0\n",
    "wrong_extractions = 0\n",
    "total_ror_ids = 0\n",
    "\n",
    "for index, row in grouped.iterrows():\n",
    "    ror_ids = row['ROR ID']\n",
    "    extracted_ror_ids = row['Extracted ROR ID']\n",
    "    if ror_ids:  # Only consider rows with non-empty ROR ID\n",
    "        total_ror_ids += len(ror_ids)\n",
    "        correct_extractions += len(ror_ids.intersection(extracted_ror_ids))\n",
    "        wrong_extractions += len(extracted_ror_ids - ror_ids)  # IDs in extraction but not in ground truth\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "\n",
    "# Calculate wrong extraction rate\n",
    "wrong_extraction_rate = wrong_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Wrong Extraction Rate: {wrong_extraction_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatched cases: 693\n",
      "Mismatched cases saved to 'mismatched_cases.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('data/2201.00_scopus_931_with_ror.csv')\n",
    "\n",
    "# Function to preprocess Extracted ROR ID\n",
    "def preprocess_extracted_ror_id(extracted_ror_id):\n",
    "    if pd.isna(extracted_ror_id):  # Skip NaN values\n",
    "        return set()\n",
    "    # Remove curly braces and split by comma\n",
    "    extracted_ror_id = extracted_ror_id.strip(\"{}\").split(\",\")\n",
    "    # Remove any leading/trailing whitespace and filter out empty strings\n",
    "    return set(item.strip().strip(\"''\") for item in extracted_ror_id if item.strip())\n",
    "\n",
    "# Step 2: Group by ArXiv ID and merge ROR ID and Extracted ROR ID into sets\n",
    "grouped = df.groupby('ArXiv Id').agg({\n",
    "    'ROR ID': lambda x: set(x.dropna()),  # Create a set of ROR IDs\n",
    "    'Extracted ROR ID': lambda x: set().union(*x.apply(preprocess_extracted_ror_id))  # Preprocess and union Extracted ROR IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Collect cases where extracted ROR IDs do not perfectly match ground truth ROR IDs\n",
    "mismatched_cases = []\n",
    "\n",
    "for index, row in grouped.iterrows():\n",
    "    ror_ids = row['ROR ID']\n",
    "    extracted_ror_ids = row['Extracted ROR ID']\n",
    "    if ror_ids != extracted_ror_ids:  # Check for imperfect match\n",
    "        mismatched_cases.append({\n",
    "            'ArXiv Id': row['ArXiv Id'],\n",
    "            'Ground Truth ROR IDs': ror_ids,\n",
    "            'Extracted ROR IDs': extracted_ror_ids,\n",
    "            'Missing ROR IDs': ror_ids - extracted_ror_ids,  # IDs in ground truth but not in extraction\n",
    "            'Extra ROR IDs': extracted_ror_ids - ror_ids  # IDs in extraction but not in ground truth\n",
    "        })\n",
    "\n",
    "# Convert the list of mismatched cases to a DataFrame\n",
    "mismatched_df = pd.DataFrame(mismatched_cases)\n",
    "\n",
    "# Save the mismatched cases to a CSV file\n",
    "mismatched_df.to_csv('mismatched_cases.csv', index=False)\n",
    "\n",
    "print(f\"Total mismatched cases: {len(mismatched_df)}\")\n",
    "print(\"Mismatched cases saved to 'mismatched_cases.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent missing ROR IDs:\n",
      "ROR ID: https://ror.org/01bj3aw27, Count: 23\n",
      "ROR ID: https://ror.org/030sjb889, Count: 22\n",
      "ROR ID: https://ror.org/01hhn8329, Count: 20\n",
      "ROR ID: https://ror.org/005ta0471, Count: 18\n",
      "ROR ID: https://ror.org/04kdfz702, Count: 18\n",
      "ROR ID: https://ror.org/00mmn6b08, Count: 17\n",
      "ROR ID: https://ror.org/00z54nq84, Count: 17\n",
      "ROR ID: https://ror.org/01rvn4p91, Count: 17\n",
      "ROR ID: https://ror.org/03qbxj466, Count: 16\n",
      "ROR ID: https://ror.org/00s19x989, Count: 15\n",
      "\n",
      "Most frequent extra ROR IDs:\n",
      "ROR ID: https://ror.org/013meh722, Count: 37\n",
      "ROR ID: https://ror.org/00hx57361, Count: 14\n",
      "ROR ID: https://ror.org/052gg0110, Count: 9\n",
      "ROR ID: https://ror.org/02eb0rk31, Count: 9\n",
      "ROR ID: https://ror.org/0257kt353, Count: 9\n",
      "ROR ID: https://ror.org/01t466c14, Count: 8\n",
      "ROR ID: https://ror.org/02dqehb95, Count: 7\n",
      "ROR ID: https://ror.org/04ynn1b95, Count: 6\n",
      "ROR ID: https://ror.org/04t4pcw17, Count: 6\n",
      "ROR ID: https://ror.org/05k9pq902, Count: 6\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calculate the most frequent missing and extra ROR IDs\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten the lists of missing and extra ROR IDs\n",
    "missing_ror_ids = [ror_id for case in mismatched_cases for ror_id in case['Missing ROR IDs']]\n",
    "extra_ror_ids = [ror_id for case in mismatched_cases for ror_id in case['Extra ROR IDs']]\n",
    "\n",
    "# Count the frequency of each ROR ID\n",
    "missing_ror_id_counts = Counter(missing_ror_ids)\n",
    "extra_ror_id_counts = Counter(extra_ror_ids)\n",
    "\n",
    "# Get the most common missing and extra ROR IDs\n",
    "most_common_missing = missing_ror_id_counts.most_common(10)  # Adjust the number as needed\n",
    "most_common_extra = extra_ror_id_counts.most_common(10)  # Adjust the number as needed\n",
    "\n",
    "# Print the results\n",
    "print(\"Most frequent missing ROR IDs:\")\n",
    "for ror_id, count in most_common_missing:\n",
    "    print(f\"ROR ID: {ror_id}, Count: {count}\")\n",
    "\n",
    "print(\"\\nMost frequent extra ROR IDs:\")\n",
    "for ror_id, count in most_common_extra:\n",
    "    print(f\"ROR ID: {ror_id}, Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
