{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "      Primary Org Id                              Primary Org Name  \\\n",
      "0           60030612           University of California, San Diego   \n",
      "1           60017604  Siberian Branch, Russian Academy of Sciences   \n",
      "2           60023932               University of Technology Sydney   \n",
      "3           60016849                         University of Toronto   \n",
      "4           60024542          South China University of Technology   \n",
      "...              ...                                           ...   \n",
      "2896        60017246           Eberhard Karls Universität Tübingen   \n",
      "2897        60123796                         Université Paris Cité   \n",
      "2898        60123796                         Université Paris Cité   \n",
      "2899        60025578                             Xidian University   \n",
      "2900        60005816                 South China Normal University   \n",
      "\n",
      "     Primary Org City   Primary Org State Primary Org Country      ArXiv Id  \\\n",
      "0            La Jolla                  CA       United States  2201.00850v2   \n",
      "1         Novosibirsk  Novosibirsk Oblast  Russian Federation  2201.00465v2   \n",
      "2              Sydney                 NSW           Australia  2201.00989v2   \n",
      "3             Toronto                  ON              Canada  2201.00382v3   \n",
      "4           Guangzhou           Guangdong               China  2201.00298v1   \n",
      "...               ...                 ...                 ...           ...   \n",
      "2896         Tubingen   Baden-Wurttemberg             Germany  2201.00069v1   \n",
      "2897            Paris       Ile-de-France              France  2201.00069v1   \n",
      "2898            Paris       Ile-de-France              France  2201.00885v3   \n",
      "2899            Xi'an             Shaanxi               China  2201.00122v2   \n",
      "2900        Guangzhou           Guangdong               China  2201.00623v2   \n",
      "\n",
      "      Affiliation Sequence Number    paper_id  \n",
      "0                               1  2201.00850  \n",
      "1                               2  2201.00465  \n",
      "2                               1  2201.00989  \n",
      "3                               1  2201.00382  \n",
      "4                               1  2201.00298  \n",
      "...                           ...         ...  \n",
      "2896                           35  2201.00069  \n",
      "2897                           25  2201.00069  \n",
      "2898                            6  2201.00885  \n",
      "2899                            1  2201.00122  \n",
      "2900                            1  2201.00623  \n",
      "\n",
      "[2901 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file. Adjust the path if needed.\n",
    "df = pd.read_csv(\"data/2201.00_scopus_931.csv\")\n",
    "\n",
    "# Read the blacklist file (assuming one organization per line)\n",
    "with open(\"data/blacklist_parent_organizations.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    blacklist = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Convert the blacklist to lowercase for case-insensitive matching.\n",
    "blacklist_lower = [org.lower() for org in blacklist]\n",
    "\n",
    "# Assuming the institution name is in a column called 'institution'.\n",
    "institution_col = 'Primary Org Name'\n",
    "\n",
    "# Create a boolean mask for institutions that appear in the blacklist.\n",
    "mask_blacklist = df[institution_col].str.lower().isin(blacklist_lower)\n",
    "\n",
    "# Create a boolean mask for institutions that contain both \"university\" and \"system\" (case-insensitive).\n",
    "mask_university_system = (\n",
    "    df[institution_col].str.lower().str.contains('university', na=False) &\n",
    "    df[institution_col].str.lower().str.contains('system', na=False)\n",
    ")\n",
    "\n",
    "# Create a boolean mask for institutions that contain \"Government of India\" (case-insensitive).\n",
    "mask_govt_india = df[institution_col].str.lower().str.contains('government of india', na=False)\n",
    "\n",
    "# Combine masks: mark for removal if any condition is met.\n",
    "mask_remove = mask_blacklist | mask_university_system | mask_govt_india\n",
    "\n",
    "# Filter out the records to be removed.\n",
    "df_filtered = df[~mask_remove].copy()\n",
    "\n",
    "# Optional: Reset index if desired.\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file.\n",
    "df_filtered.to_csv(\"data/2201.00_scopus_931.csv\", index=False)\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched 'Primary Org Id' values: 27\n",
      "Match rate: 0.9906928645294726\n",
      "JSON export completed. The output is saved as 'data/2201.00_scopus_931/groundTruth.json'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the datasets\n",
    "scopus_data = pd.read_csv('data/2201.00_scopus_931.csv')\n",
    "ror_mapping = pd.read_csv('matching_data/matched_results_ror_api.csv')\n",
    "\n",
    "# Merge the datasets on 'Primary Org Id'\n",
    "merged_data = pd.merge(\n",
    "    scopus_data,\n",
    "    ror_mapping,\n",
    "    on='Primary Org Id',\n",
    "    how='left'  # Use 'left' to keep all rows from scopus_data even if no match is found\n",
    ")\n",
    "\n",
    "# Calculate the number of unmatched rows and print match rate\n",
    "unmatched_count = merged_data['ROR ID'].isna().sum()\n",
    "print(f\"Number of unmatched 'Primary Org Id' values: {unmatched_count}\")\n",
    "print(\"Match rate:\", 1 - unmatched_count / len(merged_data))\n",
    "\n",
    "# Group by 'ArXiv Id' and aggregate non-null 'ROR ID' values into a list\n",
    "result = merged_data.groupby('ArXiv Id')['ROR ID'].apply(lambda x: x.dropna().tolist()).to_dict()\n",
    "\n",
    "# Ensure the directory exists; create it if it's empty or missing\n",
    "output_dir = 'data/2201.00_scopus_931'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the result to a JSON file within the created directory\n",
    "output_path = os.path.join(output_dir, 'groundTruth.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(result, f, indent=4)\n",
    "\n",
    "print(f\"JSON export completed. The output is saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from extractors.trie_extractor import TrieExtractor\n",
    "from utils.file_reader import read_file\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Initialize the TrieExtractor\n",
    "extractor = TrieExtractor(data_path=\"data/1.34_extracted_ror_data.csv\", common_words_path=\"data/common_english_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ROR ID from a given text file based on the paper's ArXiv Id\n",
    "def extract_ror_id(paper_id):\n",
    "    file_path = f\"data/2201_00_text/{paper_id}.txt\"\n",
    "    try:\n",
    "        text = read_file(file_path)\n",
    "        affiliations = extractor.extract_affiliations(text)\n",
    "        # Convert set to list if needed, and return the result if available\n",
    "        if affiliations:\n",
    "            return list(affiliations) if isinstance(affiliations, set) else affiliations\n",
    "        else:\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Path to the CSV file containing the \"ArXiv Id\" column\n",
    "csv_path = \"data/2201.00_scopus_931.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Apply the extraction function to each ArXiv Id with a progress bar\n",
    "tqdm.pandas()\n",
    "df['Extracted ROR ID'] = df['ArXiv Id'].progress_apply(extract_ror_id)\n",
    "\n",
    "# Create a dictionary mapping each ArXiv Id to its extracted ROR ID(s)\n",
    "result = df.set_index('ArXiv Id')['Extracted ROR ID'].to_dict()\n",
    "\n",
    "# Ensure the output directory exists; create it if not\n",
    "output_dir = 'data/2201.00_scopus_931'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the dictionary to a JSON file with keys ordered\n",
    "output_json_path = os.path.join(output_dir, 'result.json')\n",
    "with open(output_json_path, 'w') as f:\n",
    "    json.dump(result, f, indent=4, sort_keys=True)\n",
    "\n",
    "print(f\"Extraction complete. JSON saved as {output_json_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the folder containing the JSON files\n",
    "folder = 'data/2201.00_scopus_931'\n",
    "result_path = os.path.join(folder, 'result.json')\n",
    "ground_truth_path = os.path.join(folder, 'groundTruth.json')\n",
    "\n",
    "# Load the JSON files\n",
    "with open(result_path, 'r') as f:\n",
    "    result_data = json.load(f)\n",
    "\n",
    "with open(ground_truth_path, 'r') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "\n",
    "# Initialize counters\n",
    "total_ror_ids = 0\n",
    "correct_extractions = 0\n",
    "wrong_extractions = 0\n",
    "\n",
    "# Iterate over each ArXiv ID in the ground truth data\n",
    "for arxiv_id, true_val in ground_truth_data.items():\n",
    "    # Skip if ground truth is None; else, ensure we work with a set\n",
    "    if true_val is None:\n",
    "        continue\n",
    "    if isinstance(true_val, list):\n",
    "        gt_set = set(true_val)\n",
    "    else:\n",
    "        gt_set = {true_val}\n",
    "    \n",
    "    # Update total ROR IDs count from ground truth\n",
    "    total_ror_ids += len(gt_set)\n",
    "    \n",
    "    # Get the corresponding extracted value\n",
    "    res_val = result_data.get(arxiv_id)\n",
    "    if res_val is None:\n",
    "        extracted_set = set()\n",
    "    elif isinstance(res_val, list):\n",
    "        extracted_set = set(res_val)\n",
    "    else:\n",
    "        extracted_set = {res_val}\n",
    "    \n",
    "    # Count correct extractions: intersection between ground truth and extraction\n",
    "    correct_extractions += len(gt_set & extracted_set)\n",
    "    # Count wrong extractions: any extracted ROR IDs not in the ground truth\n",
    "    wrong_extractions += len(extracted_set - gt_set)\n",
    "\n",
    "# Calculate accuracy and wrong extraction rate based on total ground truth ROR IDs\n",
    "accuracy = correct_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "wrong_extraction_rate = wrong_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "\n",
    "print(\"Total ground truth ROR IDs:\", total_ror_ids)\n",
    "print(\"Correct extractions:\", correct_extractions)\n",
    "print(\"Wrong extractions:\", wrong_extractions)\n",
    "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "print(\"Wrong extraction rate: {:.2%}\".format(wrong_extraction_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the JSON files\n",
    "folder = 'data/2201.00_scopus_931'\n",
    "result_path = os.path.join(folder, 'result.json')\n",
    "ground_truth_path = os.path.join(folder, 'groundTruth.json')\n",
    "\n",
    "# Load the JSON files\n",
    "with open(result_path, 'r') as f:\n",
    "    result_data = json.load(f)\n",
    "\n",
    "with open(ground_truth_path, 'r') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "\n",
    "# List to store mismatched cases\n",
    "rows = []\n",
    "\n",
    "# Iterate over each ArXiv ID in the ground truth\n",
    "for arxiv_id, gt_val in ground_truth_data.items():\n",
    "    # Convert ground truth value to a set (or empty set if None)\n",
    "    if gt_val is None:\n",
    "        gt_set = set()\n",
    "    elif isinstance(gt_val, list):\n",
    "        gt_set = set(gt_val)\n",
    "    else:\n",
    "        gt_set = {gt_val}\n",
    "    \n",
    "    # Get the corresponding extracted value and convert to a set\n",
    "    res_val = result_data.get(arxiv_id)\n",
    "    if res_val is None:\n",
    "        extracted_set = set()\n",
    "    elif isinstance(res_val, list):\n",
    "        extracted_set = set(res_val)\n",
    "    else:\n",
    "        extracted_set = {res_val}\n",
    "    \n",
    "    # Calculate missing and extra ROR IDs\n",
    "    missing = gt_set - extracted_set  # IDs in ground truth but not in extraction\n",
    "    extra = extracted_set - gt_set    # IDs in extraction but not in ground truth\n",
    "\n",
    "    # Record this case only if there is any mismatch\n",
    "    if missing or extra:\n",
    "        rows.append({\n",
    "            'ArXiv Id': arxiv_id,\n",
    "            'Ground Truth ROR IDs': sorted(list(gt_set)),\n",
    "            'Extracted ROR IDs': sorted(list(extracted_set)),\n",
    "            'Missing ROR IDs': sorted(list(missing)),\n",
    "            'Extra ROR IDs': sorted(list(extra))\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the mismatched cases\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Save the mismatched cases to a CSV file\n",
    "output_csv_path = os.path.join(folder, 'mismatched_cases.csv')\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Mismatched cases CSV saved as {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming 'rows' is the list of mismatched cases generated previously\n",
    "\n",
    "# Create counters for missing and extra ROR IDs\n",
    "missing_counter = Counter()\n",
    "extra_counter = Counter()\n",
    "\n",
    "for row in rows:\n",
    "    missing_counter.update(row['Missing ROR IDs'])\n",
    "    extra_counter.update(row['Extra ROR IDs'])\n",
    "\n",
    "# Get the top 5 most common missing and extra ROR IDs\n",
    "most_common_missing = missing_counter.most_common(5)\n",
    "most_common_extra = extra_counter.most_common(5)\n",
    "\n",
    "print(\"Most frequent missing ROR IDs (Top 5):\")\n",
    "for ror_id, count in most_common_missing:\n",
    "    print(f\"ROR ID: {ror_id}, Count: {count}\")\n",
    "\n",
    "print(\"\\nMost frequent extra ROR IDs (Top 5):\")\n",
    "for ror_id, count in most_common_extra:\n",
    "    print(f\"ROR ID: {ror_id}, Count: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
