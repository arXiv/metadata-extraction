{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53878b9d-f550-47d3-a1b1-17e64dbe63fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476a1e3b-5c85-40d9-94b4-5792d38478e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import logging\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecbc811-6a35-4141-bd20-cb3b935fb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "   filename='extraction.log',\n",
    "   filemode='a',\n",
    "   format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "   level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5e644a-4588-499b-ab54-1bd4fd3f42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tex_files(tar_path, output_dir):\n",
    "    \"\"\"\n",
    "    Extract all .tex files from a .tar.gz archive to a dedicated folder in output_dir.\n",
    "    \"\"\"\n",
    "    archive_name = os.path.splitext(os.path.basename(tar_path))[0]\n",
    "    if archive_name.endswith('.tar'):\n",
    "        archive_name = os.path.splitext(os.path.basename(archive_name))[0]\n",
    "    extract_folder = os.path.join(output_dir, archive_name)\n",
    "    \n",
    "    # Create directory for extracted files if it doesn't exist\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "           for member in tar.getmembers():\n",
    "               if member.isfile() and member.name.endswith(\".tex\"):\n",
    "                   member.name = os.path.basename(member.name)  # Avoid nested directory extraction\n",
    "                   tar.extract(member, path=extract_folder)\n",
    "                   logging.info(f\"Extracted {member.name} from {tar_path} to {extract_folder}\")\n",
    "        logging.info(f\"All .tex files from {tar_path} have been extracted to {extract_folder}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract .tex files from {tar_path}, Error: {e}\")\n",
    "\n",
    "\n",
    "def extract_all_tex_files(dataset_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Extract .tex files from all .tar.gz files in dataset_dir to output_dir.\n",
    "    Each archive's .tex files are placed in a separate subfolder.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "       for file in tqdm(files, position=1):\n",
    "           if file.endswith(\".tar.gz\"):\n",
    "               tar_path = os.path.join(root, file)\n",
    "               extract_tex_files(tar_path, output_dir)\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#    dataset_dir = '/Users/sancho/arxiv/spacy/dataset/2201_samp'  #change it to your dataset address\n",
    "#    output_dir = 'all_tex_files'  #output\n",
    "#    os.makedirs(output_dir, exist_ok=True)\n",
    "#   \n",
    "#    extract_all_tex_files(dataset_dir, output_dir)\n",
    "#    print(f\"every .tex output to the: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c567be0d-8ea1-4323-9352-45e75bd1e53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2301\u001b[m\u001b[m          \u001b[34m2310\u001b[m\u001b[m          \u001b[34m2311\u001b[m\u001b[m          \u001b[34m2312\u001b[m\u001b[m          \u001b[34m2401\u001b[m\u001b[m\n",
      "2301.zip      2310.zip      2311.zip      2312.zip      2401.zip\n",
      "2301_tex.zip  2310_tex.zip  2311_tex.zip  2312_tex.zip  \u001b[34m2402\u001b[m\u001b[m\n",
      "\u001b[34m2301_tex_mix\u001b[m\u001b[m  \u001b[34m2310_tex_mix\u001b[m\u001b[m  \u001b[34m2311_tex_mix\u001b[m\u001b[m  \u001b[34m2312_tex_mix\u001b[m\u001b[m  \u001b[34m2403\u001b[m\u001b[m\n",
      "\u001b[34m2301_text\u001b[m\u001b[m     \u001b[34m2310_text\u001b[m\u001b[m     \u001b[34m2311_text\u001b[m\u001b[m     \u001b[34m2312_text\u001b[m\u001b[m     \u001b[34m2404\u001b[m\u001b[m\n",
      "2301_text.zip 2310_text.zip 2311_text.zip 2312_text.zip\n"
     ]
    }
   ],
   "source": [
    "!ls /Volumes/Neptune/scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117e4cc4-006f-4c89-9c44-b1ee21a17850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004410266876220703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a730431344f40b3bfc5e45b88b7aa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0017359256744384766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 12507,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cd630f2d85475a801493b0bcb9f90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0023708343505859375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 18984,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ababa63540734fae82ad9ef3160afba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0020689964294433594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 17060,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380842de0a824e49a54fbda01b551a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002717733383178711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 16320,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91657112e10b4e15b6b0d99199c4c76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folders = [\n",
    "    (\"/Volumes/Neptune/scratch/2301_tex_mix\", \"/Volumes/Neptune/scratch/2301_tex\"),\n",
    "    (\"/Volumes/Neptune/scratch/2310_tex_mix\", \"/Volumes/Neptune/scratch/2310_tex\"),\n",
    "    (\"/Volumes/Neptune/scratch/2311_tex_mix\", \"/Volumes/Neptune/scratch/2311_tex\"),\n",
    "    (\"/Volumes/Neptune/scratch/2312_tex_mix\", \"/Volumes/Neptune/scratch/2312_tex\"),\n",
    "]\n",
    "\n",
    "for in_dir, out_dir in tqdm(folders, position=0):\n",
    "    extract_all_tex_files(in_dir, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdea9f6-edc2-4067-87bb-1c877b0a31a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66d55a-6888-4ddd-8b0b-e49e4258c608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b3cf8-2255-41c3-9b2e-d42cf0aba906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79963a3-2929-4c8b-a5cc-a0b0e985eda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f2851-5810-4a9b-bd98-d98d7a0ec3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347c4d7-194c-4c6b-8614-e8c7953d9151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662d342-9726-480f-bbea-0510f1742939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf5d0dc-f846-4026-973f-fc35f5488c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "import tarfile\n",
    "import io\n",
    "import importlib\n",
    "import os\n",
    "import regex as re\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e72da197-82da-4b13-88b4-1daf6d04da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import collections as coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e8fd8a-40da-4c8b-b4a5-2a9e753c85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip   #copy text to clipboard for inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9229d15f-0f9a-46fb-8de4-2e6bc4f31736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d172ce28-bd35-4aa2-b7e9-0566d744ae17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0482c820-371e-4b81-a97c-2f10cd5fd72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import TexSoup as TS\n",
    "from TexSoup.tokens import MATH_ENV_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6edafa-3ecb-4b38-b727-d26d6696235d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importlib.reload(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5171153-1316-4310-9251-ee1627762470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_format(text):\n",
    "    '''Apply some substititions to make LaTeX easier to parse'''\n",
    "    source_text = (\n",
    "        text\n",
    "        .replace('\\\\}\\\\', '\\\\} \\\\')  # Due to escape rules \\\\ is equivalent to \\\n",
    "        .replace(')}', ') }')\n",
    "        .replace(')$', ') $')\n",
    "        #.replace(r'\\left [', r'\\left[ ')\n",
    "        #.replace(r'\\left (', r'\\left( ')\n",
    "        #.replace(r'\\left \\{', r'\\left\\{ ')\n",
    "    )\n",
    "    return source_text\n",
    "    #clean_lines = []\n",
    "    #for line in source_text.splitlines(False):\n",
    "    #    cleanline = line.strip()\n",
    "    #    if cleanline.startswith(r'\\newcommand'):\n",
    "    #        cleanline = r'%' + cleanline\n",
    "    #    elif cleanline.startswith(r'\\def'):\n",
    "    #        cleanline = r'%' + cleanline\n",
    "    #    clean_lines.append(cleanline)\n",
    "    #return '\\n'.join(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6324bc82-02a7-4032-b3e5-de19e751be34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_doc_class(wrapped_file, name_match=False):\n",
    "    '''Search for document class related lines in a file  and return a code to represent the type'''\n",
    "    doc_class_pat = re.compile(r\"^\\s*\\\\document(?:style|class)\")\n",
    "    sub_doc_class = re.compile(r\"^\\s*\\\\document(?:style|class).*(?:\\{standalone\\}|\\{subfiles\\})\")\n",
    "\n",
    "    for line in wrapped_file:\n",
    "        if doc_class_pat.search(line):\n",
    "            if name_match:\n",
    "                # we can miss if there are two or more lines with documentclass \n",
    "                # and the first one is not the one that has standalone/subfile\n",
    "                if sub_doc_class.search(line):\n",
    "                    return -99999\n",
    "                return 1 #main_files[tf] = 1\n",
    "            \n",
    "    return 0 #main_files[tf] = 0\n",
    "\n",
    "\n",
    "def find_main_tex_source_in_tar(tar_path, encoding='uft-8'):\n",
    "    '''Identify the main Tex file in a tarfile.\n",
    "    \n",
    "    Args:\n",
    "        tar_path: A gzipped tar archive of a directory containing tex source and support files.\n",
    "    '''\n",
    "    \n",
    "    tex_names = set([\"paper.tex\", \"main.tex\", \"ms.tex\", \"article.tex\"])\n",
    "\n",
    "    with tarfile.open(tar_path, 'r') as in_tar:\n",
    "        tex_files = [f for f in in_tar.getnames() if f.endswith('.tex')]\n",
    "        \n",
    "        # got one file\n",
    "        if len(tex_files) == 1:\n",
    "            return tex_files[0]\n",
    "\n",
    "        main_files = {}\n",
    "        for tf in tex_files:\n",
    "            has_main_name = tf in tex_names\n",
    "            fp = in_tar.extractfile(tf)\n",
    "            wrapped_file = io.TextIOWrapper(fp, newline=None, encoding=encoding) #universal newlines\n",
    "            # does it have a doc class?\n",
    "            # get the type\n",
    "            main_files[tf] = find_doc_class(wrapped_file, name_match = has_main_name)\n",
    "            wrapped_file.close() \n",
    "        \n",
    "        # got one file with doc class\n",
    "        if len(main_files) == 1:\n",
    "            return(main_files.keys()[0])\n",
    "        \n",
    "        # account for multi-file submissions\n",
    "        return(max(main_files, key=main_files.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae8ad8b-8fdd-45f0-92c2-70075213f2d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def soup_from_tar(tar_path, encoding='utf-8', tolerance=0):\n",
    "    tex_main = find_main_tex_source_in_tar(tar_path, encoding=encoding)\n",
    "    with tarfile.open(tar_path, 'r') as in_tar:\n",
    "        fp = in_tar.extractfile(tex_main)\n",
    "        wrapped_file = io.TextIOWrapper(fp, newline=None, encoding=encoding) #universal newlines\n",
    "        source_text = pre_format(wrapped_file.read())\n",
    "        soup = TS.TexSoup(source_text, tolerance=tolerance, skip_envs=MATH_ENV_NAMES)\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2720a35-9ddd-487d-8c83-ea8e41b6e5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def source_from_tar(tar_path, encoding='utf-8', tolerance=None):\n",
    "    tex_main = find_main_tex_source_in_tar(tar_path, encoding=encoding)\n",
    "    with tarfile.open(tar_path, 'r') as in_tar:\n",
    "        fp = in_tar.extractfile(tex_main)\n",
    "        wrapped_file = io.TextIOWrapper(fp, newline=None, encoding=encoding) #universal newlines\n",
    "        source_text = pre_format(wrapped_file.read())\n",
    "        return source_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617d2c-3d30-4546-8d1e-0e3215b32355",
   "metadata": {},
   "source": [
    "## Check a file with parse errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "305709e1-e16c-4002-a2aa-c92ae0af15fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ['Modeling Advection on Directed Graphs using  Mat', \"\\\\'\", 'e', 'rn Gaussian Processes for Traffic Flow']\n",
      " section: ['Introduction']\n",
      " section: ['Understanding the directed graph advection operator']\n",
      " section: ['Directed Graph Advection Mat', \"\\\\'\", 'e', 'rn Gaussian Process (DGAMGP) ']\n",
      " section: ['Numerical Results']\n",
      " section: ['Conclusions']\n",
      " section: ['Upwinding discretizations of linear advection']\n",
      " section: ['Examples of ', '$', 'L_', 'adv', '$', ' on balanced graphs resulting in finite difference discretizations of linear advection']\n",
      " section: ['Additional Experiments']\n"
     ]
    }
   ],
   "source": [
    "infile_path = \"./data/2201_00_all/2201.00001v1.tar.gz\" #'./data/2201_samp/2201.00048v1.tar.gz'\n",
    "\n",
    "text = source_from_tar(infile_path)\n",
    "pyperclip.copy(text)\n",
    "soup = soup_from_tar(infile_path, tolerance=1)\n",
    "\n",
    "\n",
    "title = soup.find('title')\n",
    "if title: print(f\"{title.name}: {title.text}\")\n",
    "for sec in soup.find_all('section'):\n",
    "    print(f' {sec.name}: {sec.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d47212-5b5f-45e3-b73d-27df48322f2f",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ['Classifying Subset Feedback Vertex Set', '\\\\\\\\', ' for ', '$', 'H', '$', '-Free Graphs']\n",
      " section: ['Introduction']\n",
      " section: ['Preliminaries']\n",
      " section: ['The Weighted Variant']\n",
      " section: ['The Unweighted Variant']\n",
      " section: ['Conclusions']\n",
      " section: ['Preliminaries']\n"
     ]
    }
   ],
   "source": [
    "infile_path = \"./data/2201_00_all/2201.00430v1.tar.gz\" #'./data/2201_samp/2201.00048v1.tar.gz'\n",
    "\n",
    "text = source_from_tar(infile_path)\n",
    "pyperclip.copy(text)\n",
    "soup = soup_from_tar(infile_path, tolerance=1)\n",
    "\n",
    "\n",
    "title = soup.find('title')\n",
    "if title: print(f\"{title.name}: {title.text}\")\n",
    "for sec in soup.find_all('section'):\n",
    "    print(f' {sec.name}: {sec.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008ea20-af5e-49c0-9304-72b3f4d0b9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6753f-a9f5-4f79-b054-87d4c46afeb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab5bcbf9-305b-41a7-b283-39562489fe7c",
   "metadata": {},
   "source": [
    "## Quick check a folder of tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c3ebacc-89eb-454a-97c9-5068f77a0d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCAL_DATA_PATH = '/Volumes/Neptune/scratch/2311'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e3e5ab3-f2ac-46f0-98c9-b9b772179adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(f'{LOCAL_DATA_PATH}/*.tar.gz')\n",
    "files_count = len(files)\n",
    "files_count\n",
    "ufiles = set(pathlib.Path(x).name.strip(\"tar.gz\").split('v')[0] for x in files)\n",
    "len(ufiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "999be077-8fdc-48e3-bdd3-65563e0b6e76",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0020580291748046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "errors",
       "rate": null,
       "total": 16840,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f62214f66240e4888fd3c42407c4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "errors:   0%|          | 0/16840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0020372867584228516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Progress",
       "rate": null,
       "total": 16840,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebcdeeee165421e8b3e6ea0c01c3ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/16840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found alt in /Volumes/Neptune/scratch/2401/2401.04531v1.tar.gz\n",
      "Found alt in /Volumes/Neptune/scratch/2403/2403.11782v1.tar.gz\n",
      "Found alt in /Volumes/Neptune/scratch/2403/2403.11782v2.tar.gz\n",
      "Found alt in /Volumes/Neptune/scratch/2404/2404.05317v3.tar.gz\n",
      "Found alt in /Volumes/Neptune/scratch/2404/2404.05317v1.tar.gz\n",
      "Found alt in /Volumes/Neptune/scratch/2404/2404.05317v2.tar.gz\n",
      "Found alt in /Volumes/Neptune/scratch/2404/2404.08812v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(f'{LOCAL_DATA_PATH}/*.tar.gz')\n",
    "files_count = len(files)\n",
    "utf_count = 0\n",
    "latin_count = 0 \n",
    "inc_graphics_count = 0\n",
    "inc_alt_count = 0\n",
    "err_files = {}\n",
    "\n",
    "TOLERANCE = 1\n",
    "\n",
    "def update_counts(text, tar_file):\n",
    "    global inc_alt_count\n",
    "    global inc_graphics_count\n",
    "    if \"alt=\" in text:\n",
    "        inc_alt_count += 1\n",
    "        print(f\"Found alt in {tar_file}\")\n",
    "        \n",
    "    if r\"\\usepackage{graphicx}\" in text:\n",
    "        inc_graphics_count += 1\n",
    "\n",
    "with tqdm(total=files_count, desc=\"errors\") as err_prog:\n",
    "    for tar_file in tqdm(files, desc=\"Progress\", display=True):\n",
    "        # Is it unicode?\n",
    "        text = \"\"\n",
    "        try:\n",
    "            text = source_from_tar(tar_file, encoding='utf-8', tolerance=TOLERANCE)\n",
    "            utf_count += 1\n",
    "            update_counts(text, tar_file)\n",
    "            continue\n",
    "        except EOFError as eof:\n",
    "            err_files[tar_file] = type(eof)\n",
    "            _ = err_prog.update(1)\n",
    "            continue\n",
    "        except UnicodeDecodeError as ue:\n",
    "            pass\n",
    "        except KeyboardInterrupt as KB_err:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            err_files[tar_file] = type(e)\n",
    "            _ = err_prog.update(1)\n",
    "            continue\n",
    "\n",
    "        # Is it something else?\n",
    "        try:\n",
    "            text = source_from_tar(tar_file, encoding='latin-1', tolerance=TOLERANCE)\n",
    "            latin_count += 1\n",
    "            update_counts(text, tar_file)\n",
    "            continue\n",
    "        except KeyboardInterrupt as KB_err:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            err_files[tar_file] = type(e)\n",
    "            _ = err_prog.update(1)\n",
    "            pass\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4a830f1-7cfa-4f64-9a50-e2e911b2c9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16840 processed, 14 failures.\n",
      "UTF8: 16705; Latin1: 121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/Volumes/Neptune/scratch/2312/2312.14430v2.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2312/2312.07078v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2312/2312.14430v4.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2312/2312.03652v1.tar.gz': AttributeError,\n",
       " '/Volumes/Neptune/scratch/2312/2312.14430v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2312/2312.14430v3.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2312/2312.05574v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2401/2401.07831v2.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2401/2401.00660v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2401/2401.07831v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2401/2401.14152v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2403/2403.19693v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2403/2403.09803v1.tar.gz': ValueError,\n",
       " '/Volumes/Neptune/scratch/2403/2403.14585v1.tar.gz': ValueError}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{files_count} processed, {len(err_files)} failures.\")\n",
    "print(f\"UTF8: {utf_count}; Latin1: {latin_count}\")\n",
    "err_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "804e2907-e2cf-4a2b-adbb-a1a3f4fc1737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16840 processed, 10854 used graphicx package, 7 used alt.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{files_count} processed, {inc_graphics_count} used graphicx package, {inc_alt_count} used alt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a08396b8-60d4-48ca-b287-882d86277ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msource_from_tar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/2301/2301.01083v2.tar.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOLERANCE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36msource_from_tar\u001b[0;34m(tar_path, encoding, tolerance)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msource_from_tar\u001b[39m(tar_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     tex_main \u001b[38;5;241m=\u001b[39m \u001b[43mfind_main_tex_source_in_tar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(tar_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m in_tar:\n\u001b[1;32m      4\u001b[0m         fp \u001b[38;5;241m=\u001b[39m in_tar\u001b[38;5;241m.\u001b[39mextractfile(tex_main)\n",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m, in \u001b[0;36mfind_main_tex_source_in_tar\u001b[0;34m(tar_path, encoding)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(main_files\u001b[38;5;241m.\u001b[39mkeys()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# account for multi-file submissions\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmain_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_files\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "source_from_tar('./data/2301/2301.01083v2.tar.gz', encoding='utf-8', tolerance=TOLERANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb1363-5793-4c00-9722-dc83fb062e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b416e6-c4c7-4c64-9efc-ceeaee847e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e85ffb-2596-412a-9cfb-8ef432a52c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scratch below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186eda5-1417-46f8-ba91-99ab6b5e4ae5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da25ad7a-48f8-4e88-b3c2-e681da8009a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "[Line: 0, Offset: 42281] \"displaymath\" env expecting \\]. Reached end of file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m source_from_tar(infile_path)\n\u001b[1;32m      5\u001b[0m pyperclip\u001b[38;5;241m.\u001b[39mcopy(text)\n\u001b[0;32m----> 6\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43msoup_from_tar\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m title \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36msoup_from_tar\u001b[0;34m(tar_path, encoding, tolerance)\u001b[0m\n\u001b[1;32m      5\u001b[0m wrapped_file \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(fp, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;66;03m#universal newlines\u001b[39;00m\n\u001b[1;32m      6\u001b[0m source_text \u001b[38;5;241m=\u001b[39m pre_format(wrapped_file\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m----> 7\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTexSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMATH_ENV_NAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m soup\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/__init__.py:89\u001b[0m, in \u001b[0;36mTexSoup\u001b[0;34m(tex_code, skip_envs, tolerance)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mAt a high-level, parses provided Tex into a navigable, searchable\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mstructure. This is accomplished in two steps:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03mSOUP\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m MathModeTracker\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 89\u001b[0m parsed, src \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtex_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TexNode(parsed, src\u001b[38;5;241m=\u001b[39msrc)\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/tex.py:22\u001b[0m, in \u001b[0;36mread\u001b[0;34m(tex, skip_envs, tolerance)\u001b[0m\n\u001b[1;32m     20\u001b[0m buf \u001b[38;5;241m=\u001b[39m tokenize(buf)\n\u001b[1;32m     21\u001b[0m buf \u001b[38;5;241m=\u001b[39m read_tex(buf, skip_envs\u001b[38;5;241m=\u001b[39mskip_envs, tolerance\u001b[38;5;241m=\u001b[39mtolerance)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTexEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[tex]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m, tex\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/data.py:897\u001b[0m, in \u001b[0;36mTexEnv.__init__\u001b[0;34m(self, name, begin, end, contents, args, preserve_whitespace, position)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, begin, end, contents\u001b[38;5;241m=\u001b[39m(), args\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    877\u001b[0m              preserve_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    878\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Initialization for Tex environment.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \n\u001b[1;32m    880\u001b[0m \u001b[38;5;124;03m    :param str name: name of environment\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;124;03m    '^^\\\\$**'\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_whitespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin \u001b[38;5;241m=\u001b[39m begin\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end \u001b[38;5;241m=\u001b[39m end\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/data.py:646\u001b[0m, in \u001b[0;36mTexExpr.__init__\u001b[0;34m(self, name, contents, args, preserve_whitespace, position)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m TexArgs(args)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreserve_whitespace \u001b[38;5;241m=\u001b[39m preserve_whitespace\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m position\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:227\u001b[0m, in \u001b[0;36mread_tex\u001b[0;34m(buf, skip_envs, tolerance)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Parse all expressions in buffer\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m:param Buffer buf: a buffer of tokens\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m:rtype: Iterable[TexExpr]\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mhasNext():\n\u001b[0;32m--> 227\u001b[0m     expr \u001b[38;5;241m=\u001b[39m \u001b[43mread_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSKIP_ENV_NAMES\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# update signatures for newly discovered commands\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     update_signatures(expr)\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:290\u001b[0m, in \u001b[0;36mread_expr\u001b[0;34m(src, skip_envs, tolerance, mode, is_arg)\u001b[0m\n\u001b[1;32m    288\u001b[0m         read_skip_env(src, expr, is_arg)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m         \u001b[43mread_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     expr \u001b[38;5;241m=\u001b[39m TexCmd(name, args\u001b[38;5;241m=\u001b[39margs, position\u001b[38;5;241m=\u001b[39mc\u001b[38;5;241m.\u001b[39mposition)\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:462\u001b[0m, in \u001b[0;36mread_env\u001b[0;34m(src, expr, skip_envs, tolerance, mode)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    461\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m     contents\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    463\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m src\u001b[38;5;241m.\u001b[39mhasNext() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;241m!=\u001b[39m expr\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m tolerance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:290\u001b[0m, in \u001b[0;36mread_expr\u001b[0;34m(src, skip_envs, tolerance, mode, is_arg)\u001b[0m\n\u001b[1;32m    288\u001b[0m         read_skip_env(src, expr, is_arg)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m         \u001b[43mread_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     expr \u001b[38;5;241m=\u001b[39m TexCmd(name, args\u001b[38;5;241m=\u001b[39margs, position\u001b[38;5;241m=\u001b[39mc\u001b[38;5;241m.\u001b[39mposition)\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:462\u001b[0m, in \u001b[0;36mread_env\u001b[0;34m(src, expr, skip_envs, tolerance, mode)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    461\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m     contents\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    463\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m src\u001b[38;5;241m.\u001b[39mhasNext() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;241m!=\u001b[39m expr\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m tolerance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:274\u001b[0m, in \u001b[0;36mread_expr\u001b[0;34m(src, skip_envs, tolerance, mode, is_arg)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_arg) \u001b[38;5;129;01mand\u001b[39;00m c\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;129;01min\u001b[39;00m MATH_TOKEN_TO_ENV\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    273\u001b[0m     expr \u001b[38;5;241m=\u001b[39m MATH_TOKEN_TO_ENV[c\u001b[38;5;241m.\u001b[39mcategory]([], position\u001b[38;5;241m=\u001b[39mc\u001b[38;5;241m.\u001b[39mposition)\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_math_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m==\u001b[39m TC\u001b[38;5;241m.\u001b[39mEscape:\n\u001b[1;32m    276\u001b[0m     name, args \u001b[38;5;241m=\u001b[39m read_command(src, tolerance\u001b[38;5;241m=\u001b[39mtolerance, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:393\u001b[0m, in \u001b[0;36mread_math_env\u001b[0;34m(src, expr, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m     contents\u001b[38;5;241m.\u001b[39mappend(read_expr(src, tolerance\u001b[38;5;241m=\u001b[39mtolerance, mode\u001b[38;5;241m=\u001b[39mMODE_MATH))\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m src\u001b[38;5;241m.\u001b[39mhasNext() \u001b[38;5;129;01mor\u001b[39;00m src\u001b[38;5;241m.\u001b[39mpeek()\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m!=\u001b[39m expr\u001b[38;5;241m.\u001b[39mtoken_end:\n\u001b[0;32m--> 393\u001b[0m     \u001b[43munclosed_env_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeek\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mnext\u001b[39m(src)\n\u001b[1;32m    395\u001b[0m expr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m*\u001b[39mcontents)\n",
      "File \u001b[0;32m~/miniconda3/envs/cforge/lib/python3.10/site-packages/TexSoup/reader.py:367\u001b[0m, in \u001b[0;36munclosed_env_handler\u001b[0;34m(src, expr, end)\u001b[0m\n\u001b[1;32m    365\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstead got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m end \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached end of file.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    366\u001b[0m line, offset \u001b[38;5;241m=\u001b[39m clo(src\u001b[38;5;241m.\u001b[39mposition)\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Line: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Offset: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m env expecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    368\u001b[0m     line, offset, expr\u001b[38;5;241m.\u001b[39mname, expr\u001b[38;5;241m.\u001b[39mend, explanation))\n",
      "\u001b[0;31mEOFError\u001b[0m: [Line: 0, Offset: 42281] \"displaymath\" env expecting \\]. Reached end of file."
     ]
    }
   ],
   "source": [
    "infile_path = \"./data/2201_00_all/2201.00740v1.tar.gz\" #'./data/2201_samp/2201.00048v1.tar.gz'\n",
    "#infile_path = \"./data/2201_01_all/2201.01050v1.tar.gz\" #'./data/2201_samp/2201.00048v1.tar.gz'\n",
    "\n",
    "text = source_from_tar(infile_path)\n",
    "pyperclip.copy(text)\n",
    "soup = soup_from_tar(infile_path, tolerance=1)\n",
    "\n",
    "title = soup.find('title')\n",
    "if title: print(f\"{title.name}: {title.text}\")\n",
    "for sec in soup.find_all('section'):\n",
    "    print(f' {sec.name}: {sec.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8fd897e-8a1c-44e8-bda8-f59f39e30ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TOLERANCE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m source_from_tar(infile_path)\n\u001b[1;32m      4\u001b[0m pyperclip\u001b[38;5;241m.\u001b[39mcopy(text)\n\u001b[0;32m----> 5\u001b[0m soup \u001b[38;5;241m=\u001b[39m soup_from_tar(infile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, tolerance\u001b[38;5;241m=\u001b[39m\u001b[43mTOLERANCE\u001b[49m)\n\u001b[1;32m      8\u001b[0m title \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TOLERANCE' is not defined"
     ]
    }
   ],
   "source": [
    "infile_path = \"./data/2201_00_all/2201.00430v1.tar.gz\" #'./data/2201_samp/2201.00048v1.tar.gz'\n",
    "\n",
    "text = source_from_tar(infile_path)\n",
    "pyperclip.copy(text)\n",
    "soup = soup_from_tar(infile_path, encoding='utf-8', tolerance=TOLERANCE)\n",
    "\n",
    "\n",
    "title = soup.find('title')\n",
    "if title: print(f\"{title.name}: {title.text}\")\n",
    "for sec in soup.find_all('section'):\n",
    "    print(f' {sec.name}: {sec.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6ba56-8143-418f-b8ff-6a341d3036c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tar_path = \"./data/2201_samp/2201.00008v2.tar.gz\"\n",
    "encoding = \"utf-8\"\n",
    "with tarfile.open(tar_path, 'r') as in_tar:\n",
    "    tex_files = [f for f in in_tar.getnames() if f.endswith('.tex')]\n",
    "\n",
    "    # got one file\n",
    "    if len(tex_files) == 1:\n",
    "        pass #return tex_files[0]\n",
    "\n",
    "    main_files = {}\n",
    "    for tf in tex_files:\n",
    "        fp = in_tar.extractfile(tf)\n",
    "        wrapped_file = io.TextIOWrapper(fp, newline=None, encoding=encoding) #universal newlines\n",
    "        # does it have a doc class?\n",
    "        # get the type\n",
    "        main_files[tf] = find_doc_class(wrapped_file)\n",
    "        wrapped_file.close() \n",
    "\n",
    "    # got one file with doc class\n",
    "    if len(main_files) == 1:\n",
    "        pass #return(main_files.keys()[0])\n",
    "\n",
    "    # account for multi-file submissions\n",
    "    #return(max(main_files, key=main_files.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66887585-19d1-4123-b7f3-033669132321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a976e7d-0b45-493c-a11a-332475a297ed",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_class_pat = re.compile(r\"^\\s*\\\\document(?:style|class)\")\n",
    "\n",
    "with tarfile.open(tar_path, 'r', encoding='utf-8') as in_tar:\n",
    "    #in_tar.getnames()\n",
    "    fp = in_tar.extractfile('main.tex')\n",
    "    wrapped_file = io.TextIOWrapper(fp, newline=None, encoding='utf-8') #universal newlines\n",
    "    for line in wrapped_file:\n",
    "        if doc_class_pat.search(line):\n",
    "            print(line)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91077a-6e59-4b2e-a18a-f6e42308260a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(wrapped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8b266-bf93-4630-96cc-7df4384ea622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537486a-b084-4984-ac94-98d7d57e8f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c109f4-c5f0-4203-a6ac-2e01d52cbce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7fc63084-89a4-4025-b281-37484cfe6f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Discover the file list:\n",
    "input_zip = '2201.00007v1.zip'\n",
    "infile_path = os.path.join(LOCAL_DATA_PATH, input_zip)\n",
    "if True: #False:\n",
    "    with zipfile.ZipFile(infile_path, \"r\") as in_zip:\n",
    "        files = in_zip.infolist()\n",
    "        for file_info in files:\n",
    "            if not '/.' in file_info.filename:\n",
    "                print(file_info.filename)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42c354ee-5fbf-4fdb-a9ae-5a6734811272",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "main_tex = '2201.00007v1/main.tex'\n",
    "with zipfile.ZipFile(infile_path, \"r\") as in_zip:\n",
    "    with in_zip.open('2201.00007v1/main.tex') as in_tex:\n",
    "        #print(in_tex.read())\n",
    "        wrapped_file = io.TextIOWrapper(in_tex, newline=None, encoding='utf-8') #universal newlines\n",
    "        source_text = pre_format(wrapped_file.read())\n",
    "        soup = TS.TexSoup(source_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f76d8f8a-1108-427d-8ba0-8929708576ef",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "title = soup.find('title')\n",
    "print(f\"{title.name}: {title.text}\")\n",
    "for sec in soup.find_all('section'):\n",
    "    print(f' {sec.name}: {sec.text}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9c4d1-97a9-4f76-8766-3f44fe3d30a5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_example=r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\begin{document}\n",
    "% \\renewcommand{\\shorttitle}{Avoiding Catastrophe}\n",
    "\\end{document}\n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "#print(min_example)\n",
    "TS.TexSoup(r'\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9a622-30ec-494e-a74e-bc0bc3e71cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TS.TexSoup(r'\\def\\be{\\foo{equation}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27395fa4-dafb-44c1-a1a6-9ef62e920672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TS.TexSoup(r'\\renewcommand{\\shorttitle}{Avoiding Catastrophe}')\n",
    "min_example = r\"\\newenvironment{inlinemath}{$}{$}\".strip()\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "#print(min_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d4883-0e0c-48c8-a2f1-3f03623dc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_example = r\"In practice, the matrix $\\left [ 4 \\right]\\Inv\\M{D}^{(1)}_n $\".strip()\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "#print(min_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef35d04-3cb7-4c84-8f27-10cfd07060f7",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_example = r\"In practice, the matrix $\\left[ 4 \\right]\\Inv\\M{D}^{(1)}_n $\"\n",
    "\n",
    "\n",
    "cats = TS.category.categorize(min_example)\n",
    "tokens = list(TS.tokens.tokenize(cats))\n",
    "\n",
    "char_codes = list(TS.category.categorize(min_example))\n",
    "\n",
    "buf = TS.reader.Buffer(TS.tokens.tokenize(TS.category.categorize(r'\\left[ 4 \\right]')))\n",
    "TS.reader.read_command(buf, n_required_args=-1, mode='mode:math', skip=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9deadd-c5ab-49f2-b955-5820e7cbc2d1",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_example = r\"$ t \\in [0,1] $$ t \\in [0,1] $\"\n",
    "\n",
    "\n",
    "cats = TS.category.categorize(min_example)\n",
    "tokens = list(TS.tokens.tokenize(cats))\n",
    "\n",
    "char_codes = list(TS.category.categorize(min_example))\n",
    "\n",
    "with pd.option_context('display.max.columns', None, 'display.max_colwidth', 0):\n",
    "    pd.DataFrame({'char':char_codes, 'code':(x.category for x in char_codes)}).transpose()\n",
    "    pd.DataFrame({'tokens':tokens})\n",
    "\n",
    "buf = TS.reader.Buffer(TS.tokens.tokenize(TS.category.categorize(min_example)))\n",
    "TS.reader.read_command(buf, n_required_args=-1, mode='mode:math', skip=3, tolerance=1)\n",
    "\n",
    "buf = TS.reader.Buffer(TS.tokens.tokenize(TS.category.categorize(min_example)))\n",
    "TS.read(buf, tolerance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69f130-a920-4bb7-aa06-aa4c9befa418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max.columns', None, 'display.max_colwidth', 0):\n",
    "    pd.DataFrame({'char':char_codes, 'code':(x.category for x in char_codes)}).transpose()\n",
    "    pd.DataFrame({'tokens':tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257478aa-2037-49ef-8b70-d5cde86ad095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_example = r\"In practice, the matrix $\\left [\\M{D}^{(1)}_n(\\M{D}^{(1)}_n)\\Tra\\right]\\Inv\\M{D}^{(1)}_n $\"\n",
    "print(min_example)\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "TS.TexSoup(min_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4658922-e3ad-4407-b85b-a28a49778c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_example=r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\begin{document}\n",
    "% \\renewcommand{\\shorttitle}{Avoiding Catastrophe}\n",
    "\\end{document}\n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "#print(min_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d580c-1063-4beb-b86c-742533e87c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_example=r\"\"\"\n",
    "\\def\\bean {\\begin{foo}}  \\def\\eean {\\end{foo}}\n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "TS.TexSoup(min_example)\n",
    "print(min_example)\n",
    "min_example=r\"\"\"\n",
    "we {use $A=8B$ and $s=1$, then the scalar field becomes same with (\\Ref{scalarfield}) and\n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#TS.TexSoup(min_example)\n",
    "print(min_example)\n",
    "print(pre_format(min_example))\n",
    "BRACKETS_DELIMITERS = {\n",
    "    '(', ')', '<', '>', '[', ']', '{', '}', r'\\{', r'\\}', '.' '|', r'\\langle',\n",
    "    r'\\rangle', r'\\lfloor', r'\\rfloor', r'\\lceil', r'\\rceil', r'\\ulcorner',\n",
    "    r'\\urcorner', r'\\lbrack', r'\\rbrack'\n",
    "}\n",
    "# TODO: looks like left-right do have to match\n",
    "SIZE_PREFIX = ('left', 'right', 'big', 'Big', 'bigg', 'Bigg')\n",
    "PUNCTUATION_COMMANDS = {command + opt_space + bracket\n",
    "                        for command in SIZE_PREFIX\n",
    "                        for opt_space in {'', ' '}\n",
    "                        for bracket in BRACKETS_DELIMITERS.union({'|', '.'})}\n",
    "PUNCTUATION_COMMANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c83d2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d061f2d-c423-4d5a-bc7e-ca9e06581940",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "min_example=r\"\"\"\n",
    "\\def\\bean {\\begin{eqnarray*}}  \\def\\eean {\\end{eqnarray*}}\n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "the interval $t\\in[0,1)$. \n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example))\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\beq\n",
    "[\\chF,\\chG\\}=\\{\\partial\\chF,\\chG\\}.\n",
    "\\eeq\n",
    "\n",
    "derivation $\\CA\\mapsto [\\CB,\\CA]$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following characterizations of UAL chains are all equivalent:\n",
    "\\begin{itemize}\n",
    "    \\item[(1)] A skew-symmetric function $\\cha:\\Lambda^{q+1}\\ra\\mfkdal$ defines an element of $C_{q}(\\mfkdal) $ if $\\|\\cha\\|_{\\alpha}<\\infty$ for any $\\alpha \\in \\NN$.\n",
    "    \\item[(2)] A skew-symmetric function $\\cha:\\Lambda^{q+1}\\ra\\mfkdal$ defines an element of $C_{q}(\\mfkdal) $ if there is a function $b(r) \\in \\Orf$  such that for any $j_0,...,j_q$ the observable $\\cha_{j_0...j_q}$ is $b$-localized at $j_a$ for any $a \\in \\{0,1,...,q\\}$.\n",
    "    \\item[(3)] $C_{q}(\\mfkdal) $ is the completion of $C_q(\\mfkdl) $ with respect to the norms $\\|\\cdot\\|_{\\alpha}$.\n",
    "\\end{itemize}\n",
    "\\end{lemma}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "\\newcommand\\const{\\operatorname{const}}\n",
    "\"\"\".strip() #.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "\\newcommand{\\beq}{\\begin{equation}}\n",
    "\\newcommand{\\eeq}{\\end{equation}}\n",
    "\\newcommand{\\chF}{{\\mathsf f}}\n",
    "\\newcommand{\\chG}{{\\mathsf g}}\n",
    "\\beq  \n",
    "[\\chF,\\chG\\}=\\{\\partial\\chF,\\chG\\}.\n",
    "\\eeq\n",
    "derivation $\\CA\\mapsto [\\CB,\\CA]$. \n",
    "\"\"\".strip().replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "\\[\n",
    "r_p=d(p,\\cdot)\\colon \\Gamma \\to [0,\\infty)|~ p \\in M\\}\n",
    "\\]\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "$\\bigl[ a \\bigr)$\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "\n",
    "$\\varepsilon\\in]0,\\varepsilon_\\star[$,  \n",
    "\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    "\\[\n",
    "i\\colon [0,\\infty) \n",
    "\\]\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff873e-9a41-4291-bd8f-d7c760c4cef2",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_example=r\"\"\"\n",
    "\\newcommand\\1{{\\mathds 1}}\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# !! This bug was specific to my fork\n",
    "min_example=r\"\"\"\n",
    "\\newcommand{\\linebreakand}{%\n",
    "    \\end{@IEEEauthorhalign}\n",
    "    \\hfill\\mbox{}\\par\n",
    "    \\mbox{}\\hfill\\begin{@IEEEauthorhalign}\n",
    "    }\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "min_example=r\"\"\"\n",
    " $S \\subseteq \\{0\\} \\bigcup [1,\\infty) $ if $z^*_2=1$.  \n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# two inline math envs next to eachother\n",
    "# !! probably not fixable given the approach used in TexSoup (needs stateful tokenization)\n",
    "min_example=r\"\"\"\n",
    "$\\rm{W_{cyc} }\\geq 0$$\\;\\;\\square$\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# \\verb{char}...{char} is also an issue for parser\n",
    "# !! probably not fixable given the approach used in TexSoup (needs stateful tokenization)\n",
    "min_example=r\"\"\"\n",
    "\\verb+$TEXMF/tex/latex/elsevier/+, %$%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# does not handle missing optional braces around arguments\n",
    "min_example=r\"\"\"\n",
    "$\\sqrt {\\frac 3 2} >p >1$\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# \\verb{char}...{char} is also an issue for parser\n",
    "# !! probably not fixable given the approach used in TexSoup (needs stateful tokenization)\n",
    "min_example=r\"\"\"\n",
    "&$\\rm{N_{Diskbb}}$$(\\times 10^4) $\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# \\verb{char}...{char} is also an issue for parser\n",
    "# !! probably not fixable given the approach used in TexSoup (needs stateful tokenization)\n",
    "min_example=r\"\"\"\n",
    "$\\frac{j+1+\\epsilon}{m^{\\alpha}}[$\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# \\verb{char}...{char} is also an issue for parser\n",
    "# !! probably not fixable given the approach used in TexSoup (needs stateful tokenization)\n",
    "min_example=r\"\"\"\n",
    "$1\\le k< \\frac n2 $ \n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# math nested in text in math\n",
    "# !! probably not fixable given the approach used in TexSoup (needs stateful tokenization)\n",
    "min_example=r\"\"\"\n",
    "$$\n",
    "\\sum_{S: |S|=\\lfloor q/2 \\rfloor,\\lceil q/2 \\rceil} \\beta_S \\geq  \\begin{cases} \n",
    "0.76 & \\quad \\text{if $q=5$}\\\\\n",
    "0.80  & \\quad \\text{if $q\\geq 7$}.\n",
    "\\end{cases},\n",
    "$$\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "# math nested in text in math\n",
    "# !! probably not fixable given the approach used in TexSoup (needs stateful tokenization)\n",
    "min_example=r\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{figure}\n",
    "\\centering\n",
    "\\begin{minipage}[b]{0.4\\textwidth}\n",
    "\n",
    "\\end{minipage}\n",
    "\\qquad\n",
    "\\begin{minipage}[b]{0.2\\textwidth}\n",
    "\\begin{tikzpicture}[scale=0.8] \\draw (-2,-1)--(-2,0)--(0,1) (-3,-2.2)--(-3,0.2)--(-1,0.2)--(-1,-2.2)--(-3,-2.2) (2,0)--(2,-1) (-2.5,-2) to[out=120,in=220] (-2,0) (-1.5,-2) to[out=60,in=320] (-2,0); \\draw[dotted] (-2.5,-2)--(-1.5,-2); \\draw[very thick] (0,-1)--(0,1)--(2,0); \\draw[dashed] (-2.5,-2)--(-2,-1)--(-1.5,-2); \\draw[fill=white] (-2,0) circle [radius=3pt] (-2,-1) circle [radius=3pt] (2,-1) circle [radius=3pt]; \\draw[fill=black] (2,0) circle [radius=3pt] (0,1) circle [radius=3pt] (0,0) circle [radius=3pt] (0,-1) circle [radius=3pt] (-2.5,-2) circle [radius=3pt] (-1.5,-2) circle [radius=3pt]; \\node[above] at (0,1) {$u\\in T$}; \\node[left] at (-2,0) {$v_1$}; \\node[right] at (0,0) {$v_2$}; \\node[right] at (2,0) {$v_3$}; \\node[left] at (-2,-1) {$x_1$}; \\node[right] at (0,-1) {$x_2$}; \\node[right] at (2,-1) {$x_3$}; \\node[left] at (-2.5,-2) {$y$}; \\node[right] at (-1.5,-2) {$y'$};\\node[above] at (-2,0.3) {$$};\n",
    "\\end{tikzpicture}\n",
    "\\end{minipage}\n",
    "\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "We first compute a $\\leq$$1$-part solution, $2$-part solution and $3$-part solution for $(G,T,w) $ of maximum weight. By Lemmas~\\ref{l-1part},~\\ref{l-2part} and~\\ref{l-3part}, respectively, this takes polynomial time. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\".strip()#.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }')\n",
    "TS.TexSoup(pre_format(min_example), tolerance=1)\n",
    "#print(min_example)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.DataFrame(np.random.randint(0,100,size=(10, 3)), columns=list('ABC')).to_csv('~/Expire/test_console_upload.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bb2c2cc0-fb23-4e5e-af5a-f97ba0eeeea5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "B.lower() -> copy of B\n",
       "\n",
       "Return a copy of B with all ASCII characters converted to lowercase.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "byte_string = b\"Hello World\"\n",
    "byte_string.lower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee3d3640-2254-475f-9828-1fe3b7d57069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\\\xc3\\x80'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'\\Xc3\\x80'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8d3bde8d-f0df-4ffd-be2d-c86d19e3ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_string = b\"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ef3ad03-9d4f-4a4b-8cfc-8afa2744a81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48656c6c6f20576f726c64'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_string.hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d646b460-2cce-41e9-9130-eaaa98e29c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b1000001'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(int(b'A'.hex(),16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c6264cd2-8b34-4cdf-9ab2-4beeb60f338e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b1100001'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(int(b'a'.hex(),16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a13718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
