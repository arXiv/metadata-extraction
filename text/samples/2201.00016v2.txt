
TRANSLOG: A Unified Transformer-based Framework for

Log Anomaly Detection

Hongcheng Guo1‚àó , Xingyu Lin4(cid:63) , Jian Yang1 , Yi Zhuang2 , Jiaqi Bai1 , Tieqiao

Zheng3 , Liangfan Zheng3 , Weichao Hou3 , Bo Zhang3 , Zhoujun Li1‚Ä†
1State Key Lab of Software Development Environment, Beihang University
2Bio-robot and human-computer interaction laboratory, Waseda University

3Cloudwise Research 4University of Southern California

{hongchengguo, jiaya, jiaqi, lizj}@buaa.edu.cn, linxingy@usc.edu,

syouyi2020@asagi.waseda.jp {steven.zheng,liangfan.zheng,william.hou,bowen.zhang}@cloudwise.com

Abstract

Log anomaly detection is a key component
in
the field of artificial intelligence for IT opera-
tions (AIOps). Considering log data of variant do-
mains, retraining the whole network for unknown
domains is inefficient in real industrial scenarios
especially for low-resource domains. However,
previous deep models merely focused on extract-
ing the semantics of log sequences in the same
domain, leading to poor generalization on multi-
domain logs. To alleviate this issue, we propose
a unified Transformer-based framework for Log
anomaly detection (TRANSLOG), which is com-
prised of the pretraining and adapter-based tuning
stage. Our model is first pretrained on the source
domain to obtain shared semantic knowledge of log
data. Then, we transfer the pretrained model to the
target domain via adapter-based tuning. The pro-
posed method is evaluated on three public datasets
including one source domain and two target do-
mains. Experimental results demonstrate that our
simple yet efficient approach, with fewer trainable
parameters and lower training costs in the target
domain, achieves state-of-the-art performance on
three benchmarks1.

1 Introduction
With the rapid development of large-scale IT systems, numer-
ous companies have an increasing demand for high-quality
cloud services. Anomaly detection [Breier and Branisov¬¥a,
2015] is a critical substage to monitoring data peculiarly for
logs, which describe detailed system events at runtime and
the intention of users in the large-scale services [Zhang et
al., 2015]. The field of artificial intelligence for IT opera-
tions (AIOps) [Dang et al., 2019] intends to empower IT op-
erations by integrating advanced deep learning algorithms to
meet these challenges, which ensures the stability of company
data and maintain high efficiency simultaneously.

‚àóEqual contribution.
‚Ä†Corresponding author.
1We will release the pretrained model and code.

Figure 1: The same anomaly from multiple domains. The top part
denotes the "Unusual End of Program" anomaly from three domains
including BGL, Thunderbird, and Red Storm while the bottom part
is the "Program Not Running" from four domains including BGL,
Thunderbird, Spirit, and Liberty.

Large-scale services are usually implemented by hundreds
of developers, it is error-prone to detect anomalous logs from
a local perspective. In this case, some automatic detection
methods based on machine learning are proposed [Xu et al.,
2010]. Due to the development of IT services, the volume
of log data has grown to the point where traditional ap-
proaches are infeasible. Therefore, research has turned to
deep learning methods [Zhang et al., 2016; Du et al., 2017;
Zhang et al., 2019; Meng et al., 2019]. As log messages
are half-structured and have their semantics, which is similar
to natural language corpus, language models like LSTM and
Transformer are leveraged to obtain semantics in logs. Re-
cently proposed methods even adopt fashion pretrained mod-
els like BERT [Devlin et al., 2018], GPT2 [Radford et al.,
2018] for better embedding representation.

We observe that logs from different sources have the same
anomalous categories. Despite being different in morphology
and syntax, logs of multiple domains are semantically similar.
For example, in Figure 1, three sources (BGL, Thunderbird,
Red Storm) all have the anomaly called the unusual end of
program, thus we naturally think if the model can identify the

BGL:data storage interruptrts: kernel terminatedfor reason 1004rts: bad message header: [...]Thunderbird: kernel: mptscsih: ioc0: attempting task abort! (sc=00000101bddee480)Red Storm: DMT 310 Command Aborted: SCSI cmd:2A LUN 2 DMT 310 T:299 a: [...]Unusual End of ProgramBGL: rts panic! -stopping executionThunderbird: pbs mom: Bad file descriptor (9) in tm request, job [job] not runningSpirit: kernel: GM: LANaiis not running. Allowing port=0 open for debuggingLiberty: kernel: GM: LANaiis not running. Allowing port=0 open for debuggingProgram Not Runningsame anomalies in all domains with shared semantic knowl-
edge. However, existing approaches mostly focus on a single
domain, when new components from a different/similar do-
main are introduced to the system, they lack the ability to ac-
commodate such unseen log messages. In addition, we need
to consider the continuous iteration of log data when system
upgrades, which is costly to retrain different copies of the
model. Therefore, a method based on transfer learning is re-
quired to perform well on logs from multiple domains.

In this paper, we address the problems above via a two-
stage solution called TRANSLOG. TRANSLOG is capable of
preserving the shared semantic knowledge between different
domains. More specifically, we first create a neural network
model based on the self-attention mechanism, which is pre-
trained on the source domain to obtain common semantics of
log sequences. Second, TRANSLOG utilizes a flexible plugin-
in component called adapter to transfer knowledge from the
source domain to the target domain.

Generally, the main contributions of this work are listed as
follows: (i) We propose TRANSLOG, an end-to-end frame-
work using Transformer encoder architecture to automati-
cally detect log anomalies. (ii) With only a few additional
trainable parameters on the target domain, TRANSLOG al-
lows a high degree of parameter-sharing while reducing time
and calculation consumption. (iii) Our TRANSLOG performs
well under different amounts of training data, especially when
the training data is low-resource. (iv) The proposed approach
is evaluated against three public datasets: HDFS, BGL, and
Thunderbird. TRANSLOG reaches state-of-the-art perfor-
mance on all these three datasets.

2 Background
Log Parsing The purpose of log parsing is to convert un-
structured log data into the structured event template by re-
moving parameters and keeping keywords [Jiang et al., 2008;
Makanju et al., 2009]. The previous work Drain [He et al.,
2017] adopts a fixed-depth tree structure to split the log data
and extracts structured templates. Spell [Du and Li, 2016]
applies the longest common subsequence algorithm to parse
logs efficiently. In Figure 2, we utilize Drain to extract all the
templates, and then each log message and the corresponding
template is matched. Next, the whole log template sequence
is fed into anomaly detection models.
Adapter-based Tuning Adapter-based tuning [Houlsby et
al., 2019; Bapna and Firat, 2019] is proved to be a parameter-
efficient alternative in many NLP tasks. The structure of
adapters is lightweight, which is usually composed of sim-
ple projection layers. Adapters are always inserted be-
tween transformer layers [Vaswani et al., 2017]. When tun-
ing the model on downstream tasks, only the parameters
of adapters are updated while the weights of the pretrained
model are frozen. Thus, though utilizing much less trainable
parameters compared to full fine-tuning [Devlin et al., 2018;
Stickland and Murray, 2019], adapter-based tuning reaches
comparable performance. In this paper, we design our adapter
layer as one down-projection layer, one activation layer, and
one up-projection layer in Figure 4.

Figure 2: Logs and Templates. The top part is unstructured logs, we
adopt Drain algorithm to extract log templates,then we match each
log with its template, which is the middle part. The bottom part is
structured inputs.

Log Anomaly Detection There are two main methods for
log anomaly detection, including supervised and unsuper-
vised methods. Supervised methods are often classification-
based methods [Breier and Branisov¬¥a, 2015; Huang et al.,
2020; Lu et al., 2018; Wittkopp et al., 2021b]. LogRo-
bust [Zhang et al., 2019] utilizes both normal and abnor-
mal log data for training based on the Bi-LSTM architec-
ture. Furthermore, Neurallog [Le and Zhang, 2021] uses
BERT to transform raw log messages into semantic em-
beddings without log parsing. However, obtaining system-
specific labeled samples is costly and impractical. Some
unsupervised methods [Xu et al., 2010; Yang et al., 2021;
Wittkopp et al., 2021a] have been proposed to alleviate such
burden. DeepLog [Du et al., 2017] utilizes the LSTM net-
work to forecast the next log sequence with the ranked prob-
abilities. Besides, LogAnomaly [Meng et al., 2019] utilizes
the embeddings of logs to capture the semantic information.
Although these methods attain the improvement of perfor-
mance, they ignore sharing semantics between multiple log
sources, mainly focusing on tackling the single log source
setting. Our TRANSLOG leverages such semantic knowledge
efficiently based on the Transformer-adapter architecture.

3 TRANSLOG
In this section, we describe the general framework for log
anomaly detection, named TRANSLOG. The architecture of
the TRANSLOG is shown in Figure 3, which contains two
stages: pretraining and adapter-based tuning. The following
parts start with the definition of the problem, and then the
components of the backbone model are presented. Afterward,
we illustrate the exhaustive procedure of two stages.
3.1 Problem Definition
Log anomaly detection problem is defined as a dichotomy
problem. The model is supposed to determine whether the in-

ùêø1:TIMES8 crond(pam_unix)[2915]: session closed for user rootùêø2: TIMESdn228/dn228 crond(pam_unix)[2915]: session opened for user root by (uid=0)ùêø3: TIMES(root) CMD (run-parts /etc/cron.hourly)ùêø4: TIMESsession closed for user rootùêø5: TIMESsession opened for user root by (uid=0)ùêø1:session closed for user <*> ùêø2:session opened for user <*> by <*>ùêø3:(root) CMD (<*> <*>)ùêø4:session closed for user <*> ùêø5:session opened for user <*> by <*>Unstructured Logsùëá1: session closed for user <*> ùëá2: session opened for user <*> by <*>ùëá3: (root) CMD (<*> <*>)ùêø1ùêø2ùêø3ùêø4ùêø5Log templatesMappingDrain parsingStructured InputsFormattingFigure 3: Overview of our proposed architecture. All log event sequence is first fed into the pretrained language model to extract the
representations. The Transformer encoder is trained on the high-resource source-domain dataset to acquire shared semantic information.
Then, we initialize the Transformer encoder and only tune the parameters of the adapter on the target-domain dataset to transfer the knowledge
from the source domain to the target domain.

i

t

k=1 . Then, Ssrc

put log is abnormal or normal. For the source domain, assum-
ing that through preprocessing, we achieve the vector repre-
sentations of Ksrc log sequences, which is denoted as Ssrc =
{Sk}Ksrc
}T src
i = {V src
t=1 denotes the i-th log se-
is the length of the i-th log sequence. For
quence, where T src
the target domain, Stgt = {Stgt
k }Ktgt
k=1 denotes the represen-
j = {V tgt
tations of Ktgt log sequences. Stgt
t=1 denotes
the i-th log sequence, where T tgt
is the length of the i-th
log sequence. Therefore, the training procedure is defined as
follows. We first pretrain the model on the source-domain
dataset as below:

t }T tgt

i

i

i

fp(yiSsrc

(1)
where fp represents the pretraining stage,Œò is the parameter
of the model in pretraining stage. Then, the model is trans-
ferred to the target-domain as below:

; Œò)),

i

fa(yjSsrc

j

; Œòf , Œ∏a).

(2)
where fa represents the adapter-based tuning stage. Œòf is
the parameter of the transformer encoder transferred from
the pretraining stage, which is frozen in adapter-based tuning
stage. Œ∏a is the parameter of the adapter. y is the groundtruth.
Through Equation 1 and 2, TRANSLOG learns the semantic
representation of template sequences between domains.
3.2 Backbone Model
Feature Extractor The feature extractor converts session
sequences (template sequence) to vectors with the same di-
mension d. Here we use the pretrained sentence-bert[Reimers
and Gurevych, 2019] model to get the template sequence rep-
resentation. Recently some methods extract semantic repre-
sentation from raw log messages, they believe it could prevent
the loss of information due to log parsing errors. However,
embedding every log message is not realistic considering a
large amount of log data. Studies also show that almost all
anomalies could be detected by template sequence, even if
there are parsing errors. Thus, we only embed all existing
log templates. Each session has l fixed length, so through the
layer, we can obtain the XRl√ód for each session.

Figure 4: Encoder with light adapter. Where N is the number of
transformer layers. The left part describes the traditional transformer
encoder inserted by adapters, the right part is our light adapter,
which is composed of the down- and up-projection layers.

Encoder with Light Adapter As shown in Figure 4, To
better encode the corresponding feature of inputs, we use the
transformer encoder as the backbone model. By doing so, our
encoder with self-attention mechanism overcomes the limita-
tions of RNN-based models. The core self-attention mecha-
nism is formally written as:

QK T(cid:112)d/heads

Attention(Q, K, V ) = sof tmax(

)V.

(3)

where heads is the number of the heads, d denotes the di-
mension of the input, and Q, K, V represent queries, keys,
and values, respectively.

The order of a log sequence conveys information of the
program execution sequence. Wrong execution order is also
considered abnormal. Thus, constant positional embedding
is also used.Component after self-attention layer and feedfor-
ward layer is the adapter. It's a lightweight neural network be-
tween the transformer layers. When tuning a pretrained lan-
guage model to a new domain, adapters are inserted. During
adapter-based tuning, only a few parameters of the adapters
are updated on the target domain. More specifically, we use
down- and up-scale neural networks as the adapter. Two pro-

instruction cache parity error corrected<*> double-hummer alignment exceptions<*> double-hummer alignment exceptionsLog Event SequenceSource Domainsession closed for user rootsession opened for user root by (uid=<*>)(root) CMD <*> <*>Class Label (Normal/Abnomal)ClassifierTransformer Encoder with Adapter Log Event Sequence............Target DomainFeature ExtractorTransformer EncoderClassifierClass Label (Normal/Abnomal)Feature ExtractorPretrained   LMPretrainingAdapter-based TuningParameter InitializationLayer NormAdapterMulti-headedAttention+FFNAdapter+Layer NormTransformer layer√óNDown-projectionUp-projection+Adapterjection layers in adapter first map hidden vector from dimen-
sion d to dimension m and then map it back to d. The adapter
also has a skip-connection operation internally. The output
vector h(cid:48) of the adapter is calculated as follow:
h(cid:48) = Wuptanh(Wdownh) + h.

(4)
where h ‚àà Rd represents a given hidden vector. Wdown ‚àà
Rm√ód and Wup ‚àà Rd√óm is the down-projection and the up-
projection matrix respectively, by setting m << d, we limit
the number of parameters added per adapter, which is the core
to reduce trainable parameters while retaining semantic infor-
mation to the maximum extent.
3.3 Pretraining
Inspired by the BERT model [Devlin et al., 2018], which
takes a self-supervised method to learn general language fea-
tures that further be utilized to serve different downstream
tasks, we acquire the common reason for log anomaly with
the stacked transformer encoder. In this stage, the pretrained
model learns the commonalities among different anomalies
from the semantic level, which contributes significantly to
the anomalous detection for new log sources. More specifi-
cally, the objective of this stage is the same as anomaly detec-
tion, which is a supervised classification task without adapters
in the model. Then, the parameters of the transformer en-
coder, which is trained during this stage, are shared to the
next stage. After parameter initialization, we freeze these pa-
rameters during the adapter-based tuning stage.
3.4 Adapter-based Tuning
When tuning a pretrained model from the source domain
to a target domain, the way of adapter-based tuning lever-
ages the knowledge obtained from the pretraining stage
with lightweight adapters, which are neural networks like
[Houlsby et al., 2019].
In this paper, our adapter is com-
posed of one down-projection layer, one activation layer, and
one up-projection layer in Figure 4. Through the pretraining
stage, we achieve the pretrained model, thus in this second
stage, we plug adapters into the transformer layers of the pre-
trained model, afterward, only the parameters of the adapters
are updated during target domain adaption. Parameters of
the multi-headed attention and the feedforward layers in the
pretrained model are frozen. Unlike fine-tuning, TRANSLOG
provides a plug-in mechanism to reuse the pretrained model
with only a few additional trainable parameters, without up-
dating the entire model for a new domain in this stage.
3.5 Training Strategy
In this work, we both adopt BCE loss for two stages. Thus,
we define the objective loss of the pretraining stage as below.

[log P (yx; Œò)],

Lp = ‚àíEx,y‚ààDsrc

x,y

(5)
where Lp represents the loss in the pre-training stage. Œò is the
parameter of the whole model in the pretraining stage. x and
x,y represents
y are the input data and label respectively, Dsrc
the data coming from the source domain. Then, we define the
objective loss in the adapter-based tuning stage as below.

La = ‚àíE

x,y‚ààDtgt

x,y

[log P (yx; Œòf , Œ∏a)].

(6)

Dataset
HDFS
BGL
Thunderbird

Category
Distributed

Supercomputer
Supercomputer

#Messages

#Anomaly

#Templates

11M
5M
10M

17K
20K
123K

49
423
1292

Table 1: A summary of the datasets used in this work. Messages are
the raw log strings. Samples are log sequences extracted by ID or
sliding window of size 20.

where La is the loss function in the adapter-based tuning
stage. Œòf is the parameter of the encoder module trained
in the pretraining stage, which is frozen in the adapter-based
tuning stage. Œ∏a is the parameter of the adapter. Dtgt
x,y repre-
sents the data coming from the target domain.

4 Experiments
In this section, the comprehensive settings of the experiment
are illustrated. Afterward, we experiment on three public
datasets coming from LogHub [He et al., 2020]. Compared
with baseline methods, our TRANSLOG reaches the state-of-
the-art performance on all datasets.
Datasets We conduct experiments on three public datasets,
which is described in Table 1. 10M/11M/5M continuous
log messages from Thunderbird/HDFS/BGL are separately
leveraged, which is used in prior work [Yao et al., 2020;
Le and Zhang, 2021]. HDFS [Xu et al., 2010] dataset is gen-
erated and collected from the Amazon EC2 platform through
running Hadoop-based map-reduce jobs.
It contains mes-
sages about blocks that assign a unique ID to the raw logs.
Thunderbird and BGL datasets[Oliner and Stearley, 2007]
contain logs collected from a two supercomputer system at
Sandia National Labs (SNL) in Albuquerque. The log con-
tains alert and non-alert messages identified by alert category
tags. Each log message in the datasets was manually labeled
as anomalous or not.
Preprocessing Different datasets require preprocessing
correspondingly. We extract log sequences by block IDs for
HDFS, since logs in HDFS with the same block ID are cor-
related. BGL and Thunderbird do not have such IDs, so we
utilize a sliding window(size of 20) without overlap to gener-
ate a log sequence. 1 shows the detail of datasets. We adopt
Drain[He et al., 2017] with specifically designed regex to do
log parsing, due to its high efficiency. Number of anomaly is
counted based on window. Windows containing anomalous
message are considered as anomalies, thus it's less than the
number of anomalous log messages. For each dataset, we se-
lect the first 80% (according to the timestamps of logs) log
sequences for training and the rest 20% for testing.
Implementation Details
In the experiment, we try a dif-
ferent number of transformer encoder layers in {1, 2, 4}. The
number of attention heads is 8, and the size of the feedforward
network that takes the output of the multi-head self-attention
mechanism is 3072. We optimize using Adam way whose
learning rate is scheduled by OneCycleLR, with Œ≤1 = 0.9,
Œ≤2 = 0.99, and Œµ = 10‚àí8. All runs are trained on 4
NVIDIA v100 with a batch size of 64. For each dataset,

Dataset

Method

Precision Recall F1 Score

HDFS

BGL

LR
SVM

DeepLog

LogAnomaly
LogRobust
Neurallog
TRANSLOG

LR
SVM

DeepLog

LogAnomaly
LogRobust
NeuralLog
TRANSLOG

LR
SVM

DeepLog

Thunderbird LogAnomaly

Logrobust
NeuralLog
TRANSLOG

0.96
0.96
0.95
0.96
0.98
0.96
0.99
0.78
0.89
0.90
0.97
0.62
0.61
0.98
0.46
0.34

-

0.61
0.61
0.93
0.99

0.91
0.97
0.96
0.94
0.98
1
0.99
0.79
0.86
0.83
0.94
0.96
0.78
0.98
0.91
0.91

-

0.78
0.78
1
0.99

0.93
0.97
0.96
0.96
0.98
0.98
0.99
0.78
0.87
0.86
0.96
0.73
0.68
0.98
0.61
0.50

-

0.68
0.68
0.96
0.99

Table 2: Experimental results compared with baseline models on
Thunderbird, BGL and HDFS. The best results are highlighted.

we tune the maximum learning of OneCycleLR scheduler in
{1e ‚àí 5, 5e ‚àí 5, 1e ‚àí 6}.
Baselines and Evaluation We compare TRANSLOG with
the six baseline methods, including Logistic Regression(LR),
Support Vector Machine(SVM), Deeplog [Du et al., 2017],
LogAnomaly [Meng et al., 2019], LogRobust [Zhang et al.,
2019] and Neurallog [Le and Zhang, 2021] on the three
datasets. These methods are in two categories: machine
learning and neural network approaches. Traditional ap-
proaches usually build models by transforming the log se-
quence into log count vectors while neural network ap-
proaches leverage word or contextual embeddings to repre-
In our experiments, we use precision
sent log sequences.
T P +F N ) and F1 score ( 2‚àóP recision‚àóRecall
(
T P +F P ), recall (
P recision+Recall )
T P
to compare our method and previous baselines.
Main Results To test the effectiveness of TRANSLOG, we
compare the proposed algorithm with baseline methods on
HDFS, Thunderbird, and BGL benchmarks. As is shown in
TABLE. 2. Our TRANSLOG achieves the highest F1 score
on all three datasets, confirming the effectiveness and gener-
alization of TRANSLOG. To obtain our main results, BGL
is pretrained as the source domain in the first stage, while
HDFS and Thunderbird are selected as the target domain in
the adapter-based tuning stage. We provide a thorough anal-
ysis of our model in Section 5.

T P

5 Analysis
In this section, we conduct the ablation study in four aspects
for a penetrating analysis of TRANSLOG, including the ef-
fect of the pretrained model, the gap between pretrained log
models, the efficiency of adapter-based tuning, and the low-
resource study.
Effect of Pretrained Log Model To demonstrate the fea-
sibility of transferring semantic information between various

Figure 5: Loss and F1 score on the dev set w.r.t training steps. The
left/right part represents Loss/F1 score of the HDFS/Thunderbird.
We compare two ways of training including training from scratch
and fine-tuning from model pretrained on BGL. All results are using
1-layer Transformer encoder and the same learning rate.

domains, we compare the performance of two training tech-
niques, namely training from scratch and fine-tuning. Here,
the way of fine-tuning is to update the parameter of the whole
pretrained model. We choose BGL as the source domain for
its variety in log templates and its huge data volume. Besides,
from the perspective of system kinds, HDFS is a distributed
system while Thunderbird is a supercomputer system similar
to BGL. Thus, experiments on such different log data distri-
butions can demonstrate both the transferability of semantic
knowledge and the effectiveness of TRANSLOG.

We compare two methods in terms of their rate of con-
vergence and final results. Figure 5 displays the loss curves
and F1 score curves w.r.t training steps. The results show
that fine-tuning converges faster than training from scratch,
which demonstrates that semantic knowledge the pretrained
model learned is valuable, besides it also shows the power
of the model to transfer information between domains. We
furthermore discover that fine-tuning is more stable with a
smoother curve, which is more obvious on Thunderbird, illus-
trating that similar domains share semantics more efficiently.
Due to TRANSLOG achieves good performance even
trained from scratch, the final F1 scores of two methods are
close. By observing the F1 score curve, we discover that fine-
tuning requires fewer training steps to gain the best result,
which is noteworthy for reducing costs in industrial scenes.
Gap between Pretrained Log Models We analyzed the ef-
fectiveness of the pretrained log model. The experimental
results show the feasibility of pre-training, but we can not
explain whether the performance improvement comes from
the pretraining way or the pretrained model itself. Therefore,
in this part, we adopt different pretrained log models to ana-
lyze the gap between pretrained log models. Specifically, we
utilize BGL and Thunderbird as the source data respectively.
Figure 6 shows the loss curves of the two training strategies

05101520LossHDFSfrom scratchfine-tuning0204060LossThunderbird7.3k steps0.00.20.40.60.81.0F1 Score6.3k steps0.00.20.40.60.81.0F1 ScoreMethod

Fine-tuning

Adapter

Layers
1
2
4
1
2
4

Parameters HDFS Thunderbird
0.997
0.997
0.998
0.990
0.996
0.996

7.2M 0.998
14.3M 0.998
28.5M 0.997
0.4M 0.997
0.6M 0.997
1M 0.998

Table 3: Transfer result using fine-tuning and adapter based fine-
tuning. Layers is the number of transformer encoder layers. Param-
eters is the number of trainable parameters in the model.

sults on the BGL dataset. We consider tasks with fewer than
50k training examples as low-resource tasks, as log data is
easy to obtain and be labeled manually. We train models for
30 epochs to make sure they are sufficiently trained.

We find that adapter-based tuning consistently outperforms
training and fine-tuning. The improvement is more significant
when the training size is small. With the number of train-
ing samples increasing, both methods will gradually catch up
and finally achieve similar results. Another finding is that the
quality of the model across runs is more robust, with a sim-
ilar standard deviation across different training sizes. How-
ever, training and fine-tuning yield large variances when the
number of the training sample is 5k and 20k.

To summarize, adapter tuning is highly parameter-efficient,
and shared parameters contain semantic information that
helps the model to detect anomalies. Training adapters with
sizes less than 5%, performance decreases nearly 1%.

Figure 7: Test performance on BGL (pretrained on Thunderbird)
w.r.t.the number of training examples. 5k, 10k, 20k, 50k correspond-
ing to the font 2.5%, 5%, 10%, 25% training data respectively. We
show mean and standard deviation across 3 runs for all methods.

6 Conclusion
In this paper, we propose TRANSLOG, a unified transformer-
based framework for log anomaly detection, which contains
the pretraining stage and the adapter-based tuning stage. Ex-
tensive experiments demonstrate that our TRANSLOG, with
fewer trainable parameters and lower training costs, outper-
forms all previous baselines. We foresee the semantic migra-
tion between log sources for a unified multiple sources detec-
tion.

Figure 6: Loss on the dev set w.r.t training steps. The upper/bottom
results are based on parameters pretrained on BGL/Thunderbird,
thus BGL/Thunderbird are not shown. All results are using 1-layer
Transformer encoder and the same learning rate.

on the target source. From the curves, we can see that the fine-
tuning loss of each model decreases faster than training from
scratch. At the same time, comparing the two pre-training
models, it is obvious that, for the HDFS dataset, the model
pretrained on BGL brings greater performance improvement.
In conclusion, different pre-training models provide different
gains for model performance, which is the gap between pre-
trained log models.
Efficiency of Adapter-based Tuning Although we have
verified that parameters transfer could accelerate convergence
without reducing performance, fine-tuning each component
is expensive and inconvenient. Thus, we adopt a parameter-
efficient strategy, called adapter-based tuning, to allow a high
degree of sharing knowledge between domains. By utilizing
the adapter, we acquire a compact model for log anomaly de-
tection by adding a few additional parameters.

To confirm the efficiency of our TRANSLOG, general trans-
fer performance of fine-tuning and adapter-based tuning are
compared. Experiments are based on a model trained on the
BGL and Thunderbird datasets. On each dataset, we utilize
batch size 64 and tune the model in learning rate selected
from {1e‚àí 5, 5e‚àí 5, 1e‚àí 6}. Results show that 5e‚àí 5 is the
most satisfactory for HDFS, 1e‚àí5 is the best for Thunderbird.
We conduct the ablation study by adjusting the number of en-
coder layers in {1, 2, 4}. Table 3 summarizes the results. We
observe that our TRANSLOG with adapters generates a com-
petitive score but adopts 3.5%‚àí5.5% of the parameters in the
whole original model. In addition, experiments on the various
number of transformer encoders are executed. Results indi-
cate that more encoder layers for fine-tuning do not always
generate better results. Simultaneously, adapter-based tuning
performs more robust when we stack more encoder layers.
Low-resource Study To verify the influence of training
size, we plot the performances with a varying number of
training samples in Figure 7. It presents the comparison re-

0510152025LossHDFSfrom scratchfine-tuning0204060LossThunderbird7.3k steps0510152025LossHDFS6.3k steps0204060LossBGL5k10k20k50kTraining Samples0.00.10.20.30.40.50.60.7F1 Scorefrom scratchfine-tuningadapterReferences
[Bapna and Firat, 2019] Ankur Bapna and Orhan Firat. Sim-
ple, scalable adaptation for neural machine translation. In
EMNLP 2019, pages 1538 -- 1548, 2019.

[Breier and Branisov¬¥a, 2015] Jakub

Branisov¬¥a.
data mining techniques.
Applications. 2015.

Jana
Anomaly detection from log files using
In Information Science and

Breier

and

[Dang et al., 2019] Yingnong Dang, Qingwei Lin, and Peng
Huang. Aiops: real-world challenges and research innova-
tions. In ICSE 2019, 2019.

[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-
ton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understand-
ing. NAACL 2019, 2018.

[Du and Li, 2016] Min Du and Feifei Li. Spell: Streaming

parsing of system event logs. In ICDM 2016, 2016.

[Du et al., 2017] Min Du, Feifei Li, Guineng Zheng, and
Vivek Srikumar. Deeplog: Anomaly detection and diag-
In CCS
nosis from system logs through deep learning.
2017, 2017.

[He et al., 2017] Pinjia He, Jieming Zhu, Zibin Zheng, and
Michael R Lyu. Drain: An online log parsing approach
with fixed depth tree. In ICWS 2017, pages 33 -- 40, 2017.
[He et al., 2020] Shilin He, Jieming Zhu, Pinjia He, and
Michael R. Lyu. Loghub: A large collection of sys-
tem log datasets towards automated log analytics. CoRR,
abs/2008.06448, 2020.

[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis-
law Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
In ICML
Parameter-efficient transfer learning for nlp.
2019, 2019.

[Huang et al., 2020] Shaohan Huang, Yi Liu, Carol Fung,
Rong He, Yining Zhao, Hailong Yang, and Zhongzhi
Luan. Hitanomaly: Hierarchical transformers for anomaly
detection in system log. TNSM, 17(4):2064 -- 2076, 2020.
[Jiang et al., 2008] Zhen Ming Jiang, Ahmed E. Hassan, Par-
minder Flora, and Gilbert Hamann. Abstracting execution
logs to execution events for enterprise applications (short
paper). In QSIC 2008, pages 181 -- 186, 2008.

[Le and Zhang, 2021] Van-Hoang Le and Hongyu Zhang.
Log-based anomaly detection without log parsing. arXiv
preprint arXiv:2108.01955, abs/2108.01955, 2021.

[Lu et al., 2018] Siyang Lu, Xiang Wei, Yandong Li, and
Liqiang Wang. Detecting anomaly in big data system logs
using convolutional neural network. In DASC 2018, pages
151 -- 158, 2018.

[Makanju et al., 2009] Adetokunbo Makanju, A. Nur Zincir-
Heywood, and Evangelos E. Milios. Clustering event logs
In KDD 2009, pages 1255 -- 
using iterative partitioning.
1264, 2009.

[Meng et al., 2019] Weibin Meng, Ying Liu, Yichen Zhu,
Shenglin Zhang, Dan Pei, Yuqing Liu, Yihao Chen, Ruizhi

Zhang, Shimin Tao, Pei Sun, et al. Loganomaly: Unsuper-
vised detection of sequential and quantitative anomalies in
unstructured logs. In IJCAI 2019, 2019.

[Oliner and Stearley, 2007] Adam J. Oliner and Jon Stearley.
What supercomputers say: A study of five system logs. In
DSN 2007, pages 575 -- 584, 2007.

[Radford et al., 2018] Alec Radford, Karthik Narasimhan,
Improving language

Tim Salimans, and Ilya Sutskever.
understanding by generative pre-training. 2018.

[Reimers and Gurevych, 2019] Nils Reimers

and Iryna
Sentence-bert: Sentence embeddings us-
In EMNLP 2019, pages

Gurevych.
ing siamese bert-networks.
3980 -- 3990, 2019.

[Stickland and Murray, 2019] Asa Cooper Stickland and Iain
Murray. BERT and pals: Projected attention layers for
efficient adaptation in multi-task learning. In ICML 2019,
pages 5986 -- 5995, 2019.

[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. NIPS 2017, 2017.

[Wittkopp et al., 2021a] Thorsten Wittkopp, Alexander
Acker, Sasho Nedelkoski, Jasmin Bogatinovski, Dominik
Scheinert, Wu Fan, and Odej Kao. A2log: Attentive
augmented log anomaly detection. CoRR, 2021.

[Wittkopp et al., 2021b] Thorsten Wittkopp, Philipp Wies-
ner, Dominik Scheinert, and Alexander Acker. Loglab:
Attention-based labeling of log data anomalies via weak
supervision. In ICSOC 2021, pages 700 -- 707, 2021.

[Xu et al., 2010] Wei Xu, Ling Huang, Armando Fox, David
Patterson, and Michael I Jordan. Detecting large-scale sys-
In ICML 2010,
tem problems by mining console logs.
2010.

[Yang et al., 2021] Lin Yang, Junjie Chen, Zan Wang, Wei-
jing Wang, Jiajun Jiang, Xuyuan Dong, and Wenbin
Zhang. Semi-supervised log-based anomaly detection via
probabilistic label estimation. In ICSE 2021, pages 1448 -- 
1460, 2021.

[Yao et al., 2020] Kundi Yao, Heng Li, Weiyi Shang, and
Ahmed E. Hassan. A study of the performance of general
compressors on log files. ESE, 25(5):3043 -- 3085, 2020.

[Zhang et al., 2015] Shenglin Zhang, Ying Liu, Dan Pei,
Yu Chen, Xianping Qu, Shimin Tao, and Zhi Zang. Rap-
idand robust impact assessment of software changes in
large internet-based services. In ENET 2015, 2015.

[Zhang et al., 2016] Ke Zhang, Jianwu Xu, Martin Renqiang
Min, Guofei Jiang, Konstantinos Pelechrinis, and Hui
Zhang. Automated it system failure prediction: A deep
learning approach. In BigData 2016, 2016.

[Zhang et al., 2019] Xu Zhang, Yong Xu, Qingwei Lin,
Bo Qiao, Hongyu Zhang, Yingnong Dang, Chunyu Xie,
Xinsheng Yang, Qian Cheng, Ze Li, et al. Robust log-
In FSE
based anomaly detection on unstable log data.
2019, 2019.

