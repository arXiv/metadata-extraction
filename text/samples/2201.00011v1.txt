
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

An Efficient Federated Distillation Learning

System for Multi-task Time Series Classification

Huanlai Xing, Member, IEEE, Zhiwen Xiao, Rong Qu, Senior Member, IEEE, Zonghai Zhu,

and Bowen Zhao

Abstract -- This paper proposes an efficient federated distillation learning system (EFDLS) for multi-task time series classification
(TSC). EFDLS consists of a central server and multiple mobile users, where different users may run different TSC tasks. EFDLS has
two novel components, namely a feature-based student-teacher (FBST) framework and a distance-based weights matching (DBWM)
scheme. Within each user, the FBST framework transfers knowledge from its teacher's hidden layers to its student's hidden layers via
knowledge distillation, with the teacher and student having identical network structure. For each connected user, its student model's
hidden layers' weights are uploaded to the EFDLS server periodically. The DBWM scheme is deployed on the server, with the least
square distance used to measure the similarity between the weights of two given models. This scheme finds a partner for each
connected user such that the user's and its partner's weights are the closest among all the weights uploaded. The server exchanges
and sends back the user's and its partner's weights to these two users which then load the received weights to their teachers' hidden
layers. Experimental results show that the proposed EFDLS achieves excellent performance on a set of selected UCR2018 datasets
regarding top-1 accuracy.

Index Terms -- Deep Learning, Data Mining, Federated Learning, Knowledge Distillation, Time Series Classification.

!

1 INTRODUCTION

T IME series data is a series of time-ordered data points

associated with one or more time-dependent variables
and has been successfully applied to areas such as anomaly
detection [1], [2], traffic flow forecasting [3], service match-
ing [4], stock prediction [5], electroencephalogram (ECG)
detection [6] and parking behavior prediction [7]. A signif-
icant amount of research attention has been dedicated to
time series classification (TSC) [8]. For example, Wang et al.
[9] introduced a fully convolutional network (FCN) for local
feature extraction. Zhang et al. [10] devised an attentional
prototype network (TapNet) to capture rich representations
from the input. Karim et al. [11] proposed a long short-
term memory (LSTM) fully convolutional network (FCN-
LSTM) for multivariate TSC. A robust temporal feature
network (RTFN) hybridizing temporal feature network and
LSTM-based attention network was applied to extracting
both the local and global patterns of data [12]. Li et al.
[13] put forward a shapelet-neural network approach to
mine highly-diversified representative shapelets from the
input. Lee et al. [14] designed a learnable dynamic temporal
pooling method to reduce the temporal pooling size of the
hidden representations obtained.

TSC algorithms are usually data-driven, where data

• H. Xing, Z. Zhu, and B. Zhao are with the School of Computing
and Artificial Intelligence, Southwest Jiaotong University, Chengdu
611756, China (Emails: hxx@home.swjtu.edu.cn; zzhu@swjtu.edu.cn;
cn16bz@icloud.com).
• Z. Xiao is with Southwest Jiaotong University, Chengdu 611756, China,
and Chengdu University of Information Technology, Chengdu 610103,
China (Email: xiao1994zw@163.com).

the School of Computer Science, Univer-
sity of Nottingham, Nottingham NG7 2RD 455356, UK (Email:
rong.qu@nottingham.ac.uk)

• R. Qu is with the

Manuscript received XX, XX; revised XX, XX (Corresponding author: Zhi-
wen Xiao).

comes from various application domains. Some data may
contain private and sensitive information, such as bank
account and ECG. However, traditional data collection op-
erations could not well protect such information, easily
resulting in users' privacy leakage during the data collec-
tion and distribution processes involved in model training.
To overcome the problem above, Google [15], [16], [17]
invented federated learning (FL). FL allows users to col-
lectively harvest the advantages of shared models trained
from their local data without sending original data to others.
FederatedAveraging (FedAvg), federated transfer learning
(FTL) and federated knowledge distillation (FKD) are the
three mainstream research directions.

FedAvg calculates the average weights of the models
of all users and shares the weights with each user in
the FL system [18]. For instance, Ma et al. [19] devised a
communication-efficient federated generalized tensor fac-
torization for electronic health records. Liu et al. [20] used
a federated adaptation framework to leverage the spar-
sity property of neural networks for generating privacy-
preserving representations. A hierarchical personalized FL
method aggregated heterogeneous user models, with pri-
vacy heterogeneity and model heterogeneity considered
[21]. Yang et al. [22] modified the FedAvg method using
partial networks for COVID-19 detection.

FTL introduces transfer learning techniques to promote
knowledge transfer between different users, increasing sys-
tem accuracy [23]. For example, Yang et al. [24] developed an
FTL framework, FedSteg, for secure image steganalysis. An
FTL method with dynamic gradient aggregation was pro-
posed to weight the local gradients during the aggregation
step when handling speech recognition tasks [25]. Majeed et
al. [26] proposed an FTL-based structure to address traffic
classification problems.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

Unlike FedAvg and FTL, FKD takes the average of all
users' weights as the weights for all teachers and transfers
each teacher's knowledge to its corresponding student via
knowledge distillation (KD) [27]. A group knowledge trans-
fer training algorithm was adopted to train small convolu-
tional neural networks (CNNs) and transfer their knowl-
edge to a prominent server-side CNN [28]. Mishra et al.
[29] proposed a resource-aware FKD approach for network
resource allocation. Sohei et al. [30] devised a distillation-
based semi-supervised FL framework for communication-
efficient collaborative training with private data. Nowadays,
FKD is attracting increasingly more research attention.

In addition, there is a variety of FL-based algorithms
in the literature. For instance, Chen et al. [31] applied
asynchronous learning and temporally weighted aggrega-
tion to enhancing system's performance. Sattler et al. [32]
presented a sparse ternary compression method to meet
various requirements of FL environment. A cooperative
game involving a gradient algorithm was designed to tackle
image classification and speech recognition tasks [33]. An
ensemble FL system used a randomly selected subset of
clients to learn multiple global models against malicious
clients [34]. Hong et al. [35] combined adversarial learning
and FL to produce federated adversarial debiasing for fair
and transferable representations. Zhou et al. [36] proposed a
privacy-preserving distributed contextual federated online
learning framework with big data support for social recom-
mender systems. Pan et al. [37] put forward a multi-granular
federated neural architecture search framework to enable
the automation of model architecture search in a federated
and privacy-preserved setting.

Most FL algorithms are developed around single-task
problems, where multiple users work together to complete
a task, e.g., COVID-19 detection [22], traffic classification
[26] or speech recognition [25]. It is quite challenging to
directly apply these algorithms to multi-task problems un-
less efficient knowledge sharing among different tasks is
enabled. Unfortunately, TSC is usually multi-task-oriented.
Time series data is collected from various application do-
mains, such as ECG, traffic flow, human activity recognition.
Each time series dataset has specific characteristics, e.g.,
length and variance, which may differ significantly from
others. Thus, time series data is highly imbalanced and
strongly non-independent, and identically distributed (Non-
I.I.D.). In multi-task learning, it is commonly recognized
that knowledge sharing among different tasks helps increase
the efficiency and accuracy of each task [38]. For most TSC
algorithms, how to securely share knowledge of similar expertise
among different tasks is still challenging. In other words, user
privacy and knowledge sharing are two critical issues that need
to be carefully addressed when devising practical multi-task TSC
algorithms. To the best of our knowledge, FL for multi-task
TSC has not received sufficient research attention.

We present an efficient federated distillation learning
system (EFDLS) for multi-task TSC. This system consists of a
central server and a number of mobile users running various
TSC tasks simultaneously. Given two arbitrary users, they
run either different tasks (e.g., ECG and motion) or the
same task with different data sources to mimic real-world
applications. EFDLS is characterized by a feature-based
student-teacher (FBST) framework and a distance-based

weights matching (DBWM) scheme. The FBST framework
is deployed on each user, where the student and teacher
models have identical network structure. Within each user,
its teacher's hidden layers' knowledge is transferred to its
student's hidden layers, helping the student mine high-
quality features from the data. The DBWM scheme is de-
ployed on the EFDLS server, where the least square distance
(LSD) is used to measure the similarity between the weights
of two models. When all connected users' weights are
uploaded completely, for an arbitrary connected user, the
DBWM scheme finds the one with the most similar weights
among all connected users. After that, the server sends the
connected user's weights to the found one that then loads
the weights to its teacher model's hidden layers.

Our main contributions are summarized below.
• We propose EFDLS for multi-task TSC, where each
user runs one TSC task at a time and different users
may run different TSC tasks. The data generated on
different users is different. EFDLS aims at provid-
ing secure knowledge sharing of similar expertise
among different tasks. This problem has not attracted
enough research attention.
In EFDLS, feature-based knowledge distillation is
used for knowledge transfer within each user. Unlike
the traditional FKD that adopts the average weights
of all users to supervise the feature extraction process
in each user, EFDLS finds the one with the most sim-
ilar expertise (i.e., a partner) for each user according
to LSD and offers knowledge sharing between the
user and its partner.

•

• Experimental results demonstrate that EFDLS out-
performs six state-of-the-art FL algorithms con-
sidering 44 well-known datasets selected in the
UCR 2018 archive regarding the mean accuracy,
'win'/'tie'/'lose' measure, and AVG rank, which are
all based on the top-1 accuracy. That shows the
effectiveness of EFDLS in addressing TSC problems.
The rest of the paper is organized below. Section 2
reviews the existing TSC algorithms. Section 3 overviews
the architecture of EFDLS and describes its key components.
Section 4 provides and analyzes the experimental results,
and conclusion is drawn in Section 5.

2 RELATED WORK
A large number of traditional and deep learning algorithms
have been developed for TSC.

2.1 Traditional Algorithms
Two representative streams of algorithms are distance- and
feature-based. For distance-based algorithms,
it is quite
common to combine the dynamic time warping (DTW) and
nearest neighbor (NN), e.g., DT WA, DT WI and DT WD
[39]. Besides, a significant number of DTW-NN-based en-
semble algorithms taking advantage of DTW and NN have
been proposed in the community. For example, Line et al.
[40] presented an elastic ensemble (EE) algorithm for feature
extraction, with 11 types of 1-NN-based elastic distance
considered. A collective of the transformation-based ensem-
ble (COTE) with 37 NN-based classifiers was adopted to

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 1. The schematic diagram of EFDLS. Note that 'FBST Framework' and 'DBWM Scheme' denote the feature-based student-teacher framework
deployed on each user and the distance-based weights matching scheme run on the server. 'Conv x 9 128' represents a 1-dimensional convolutional
neural network, where its filter size and channel sizes are set to 9 and 128. 'BN' is a batch normalization module, and 'ReLU' is the rectified linear
unit activation function.

address various TSC problems [41]. The hierarchical vote
collective of transformation-based ensembles (HIVE-COTE)
[42] and local cascade ensemble [43] are two representative
ensemble algorithms in the literature.

For feature-based algorithms, their aim is to capture
sufficient discriminate features from the given data. For
instance, Line et al. [44] introduced a shapelet transforma-
tion method to find representative shapelets that reflected
the trend of raw data. A bag-of-features representation
framework was applied to extracting the information at
different locations of sequences [45]. Dempster et al. [46]
applied minimally random convolutional kernel transform
to exploring the transformed features from data. In addi-
tion, the learned pattern similarity [47], bag of symbolic
Fourier approximation symbols [48], hidden-unit logistic
model [49], time series forest [50], and multi-feature dic-
tionary representation and ensemble learning [51] are also
representative algorithms.

2.2 Deep Learning Algorithms
By unfolding the internal representation hierarchy of data,
deep learning algorithms focus on extracting the intrinsic
connections among representations. Most of the existing
deep learning models are either single-network- or dual-
network-based [12]. A single-network-based model captures
the significant correlations within the representation hierar-
chy of data by one (usually hybridized) network, e.g., FCN
[9], ResNet [9], shapelet-neural network [13], InceptionTime

[52], dynamic temporal pooling [14], multi-process collab-
orative architecture [53], and multi-scale attention convo-
lutional neural network [54]. In contrast, a dual-network-
based model usually consists of two parallel networks, i.e.,
local-feature extraction network (LFN) and global-relation
extraction network (GRN), such as FCN-LSTM [11], RTFN
[12], ResNet-Transformer [55], RNTS [56], and TapNet [10].
Almost all algorithms above emphasized single-task
TSC, e.g., traffic or gesture classification. However, TSC
usually involves multiple tasks in real-world scenarios, like
various applications with different TSC tasks run on dif-
ferent mobile devices in a mobile computing environment.
Enabling efficient knowledge sharing of similar expertise
among different tasks helps increase the average accuracy
of these tasks. Nevertheless, sharing knowledge among dif-
ferent TSC tasks securely and efficiently is still a challenge.
That is what FL aims for.

3 EFDLS
This section first overviews the architecture of EFDLS. Then,
it introduces the feature-based student-teacher framework,
distance-based weights matching scheme, and communica-
tion overhead.

3.1 System Overview
EFDLS is a secure distributed system for multi-task TSC.
There is a central server and multiple mobile users. Let Ntot

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

and Nconn denote the numbers of total and connected users
in the system, respectively, where Nconn ≤ Ntot. Each user
runs one TSC task at a time and different users might run
different TSC tasks. For two arbitrary users, they run two
different tasks, such as gesture and ECG classification, or
the same task with different data sources.

The overview of EFDLS is shown in Fig. 1. In the
system, users train their models locally based on knowledge
distillation and share their model weights with users with
similar expertise via the server. We propose FBST, a feature-
based student-teacher framework that is deployed on each
user as its learning model. Within each user, its teacher's
hidden layers' knowledge is transferred to its student's
hidden layers. For each connected user, its student model's
hidden layers' weights are uploaded to the EFDLS server
periodically. We propose DBWM, a distance-based weights
matching scheme deployed on the server, with the LSD
adopted to measure the similarity between the weights
of two given models. After the weights of all connected
users are uploaded completely, for each connected user, the
DBWM scheme is launched to find the one with the most
similar weights among all connected users. In this way,
every user has a partner to match with. For each connected
user, its uploaded weights are sent to its partner that then
loads these weights to its teacher model's hidden layers.
The server's role looks like a telecom-network switch. The
EFDLS system allows users to benefit from knowledge
sharing without sacrificing security and privacy.

3.2 Feature-based Student-Teacher Framework

In the FBST framework, the student and teacher models
have identical network structure. Within each user, feature-
based KD promotes knowledge transfer from the teacher's
hidden layers to its student's hidden layers, helping the
student capture rich and valuable representations from the
input data.

3.2.1 Feature Extractor

The feature extractor contains multiple hidden layers and
a classifier, as shown in Fig. 1. The hidden layers are
responsible for local-feature extraction,
including three
Convolutional Blocks (i.e., ConvBlock1, ConvBlock2, and
ConvBlock3), an average pooling layer, and a dense (i.e.,
fully-connected) layer. Each ConvBlock consists of a 1-
dimensional CNN (Conv) module, a batch normalization
(BN) module, and a rectified linear unit activation (ReLU)
function, defined as:

fconvblock(x) = frelu(fbn(Wconv ⊗ x + bconv))

(1)

instance and batch size, respectively. fbn(xbn) is defined in
Eq. (2)

4

fbn(xbn) = fbn(x1, x2, ..., xNbn)

x2 − µ
δ + ζ

+ β, ..., α

xNbn − µ
δ + ζ

+ β)

= (α

+ β, α

1

x1 − µ
Nbn(cid:88)
δ + ζ
(cid:118)(cid:117)(cid:117)(cid:116)Nbn(cid:88)

Nbn

i=1

xi

µ =

δ =

(xi − µ)2

i=1

(2)
where, α ∈ R+ and β ∈ R are the parameters to be learned
during training. ζ > 0 is an arbitrarily small number.

The classifier is composed of a dense layer and a Softmax
function, mapping high-level features extracted from the
hidden layers to the corresponding label.

3.2.2 Knowledge Distillation
Feature-based KD regularizes a student model by transfer-
ring knowledge from the corresponding teacher's hidden
layers to the student's hidden layers [57]. For an arbitrary
user, its student model captures sufficient discriminate rep-
resentations from the data under its teacher model's super-
vision.

Let OT,1

, and OT,4

be the outputs of Con-
vBlock 1, ConvBlock 2, ConvBlock 3, and the dense layer of
the teacher's hidden layers. Let OS,1
, and OS,4
be the outputs of ConvBlock 1, ConvBlock 2, ConvBlock 3,
and the dense layer of the student's hidden layers. Follow-
ing the previous work [28], we define the KD loss, LKD
, of
Ui as:

, OS,2

, OS,3

, OT,2

, OT,3

i

i

i

i

i

i

i

i

i

LKD
i =

OT,m

i − OS,m

i

2

(3)

4(cid:88)

m=1

For Ui, its total loss, Li, consists of KD loss, LKD

, and
supervised loss, LSup
. As the previous studies in [10], [11],
[12] suggested, LSup
uses the cross-entropy function to
measure the average difference between the ground truth
labels and their prediction vectors, as shown in Eq. (4).

i

i

i

LSup
i = − 1
Nseg

Nseg(cid:88)

j=1

yjlog(yj)

(4)

where, Nseg is the number of input vectors, and yi and yj
are the ground label and prediction vector of the j-th input
vector, respectively.

The total loss, Li, is defined as:

Li =  × LSup

i + (1 − ) × LKD

(5)
where,  ∈ (0, 1) is a coefficient to balance LSup
and LKD
.
In this paper, we set  = 0.9 (More details can be found in
Section 4.3).

i

i

i

where, Wconv and bconv are the weight and bias matrices of
CNN, respectively. ⊗ represents the convolutional computa-
tion operation. fbn and frelu denote the batch normalization
and ReLU functions, respectively.
Let xbn = {x1, x2, ..., xNbn} denote the input of batch
normalization (BN), where xi and Nbn stand for the i-th

3.3 Distance-based Weights Matching
The least square distance (LSD) is used to calculate the sim-
ilarity between the weights of two given models. When the
weights uploaded by all the connected users are received,
the DBWM scheme immediately launches the weights
matching process to find a partner for each connected user.



d =

d1
d2
...

 =



 (7)

i

i

and W T,k

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
3.3.1 Least Square Distance
Let F LEs denote the maximum number of federated learn-
ing epochs. Let W S,k
be the weights of the
student and teacher models of Ui at the k-th federated
learning epoch, k = 1, 2, ..., F LEs. Denote the hidden
layers' weights of the student and teacher models of Ui by
W Shidden,k
, respectively.
i
To be specific, W Shidden,k
consists of the weights of Con-
vBlock 1, ConvBlock 2, ConvBlock 3, and the dense layer,
, and W S4,k
namely, W S1,k
. So, we have
}.
, W S4,k
W Shidden,k

, W S2,k
= {W S1,k

and W Thidden,k

⊂ W T,k

⊂ W S,k

i
, W S2,k

, W S3,k

, W S3,k

At the k-th federated learning epoch, user Ui, i =
1, 2, ..., Nconn, uploads its student model's hidden layers'
weights, W Shidden,k
, to the server. The server stores the
uploaded weights in the weight set W defined in Eq. (6).

i

i

i

i

i

i

i

i

i

i

i

i

i

W = [W Shidden,k

1

, W Shidden,k

2

, ..., W Shidden,k

Nconn

]

(6)

The server then calculates the weights' square distance

set, d, based on W. d is defined as:

d1,2
d2,1
...

d1,Nconn
d2,Nconn

...
...
...
... dNconn,Nconn−1

...

dNconn

dNconn,1

where, di,j (i, j ∈ 1, ..., Nconn, i (cid:54)= j) is the square distance
between W Shidden,k

, as defined in Eq. (8).

and W Shidden,k

i

di,j = W Shidden,k
W Sm,k

4(cid:88)

=

i

j

− W Shidden,k
2
− W Sm,k

j

2

(8)

j

i

m=1

We adopt the argmin function to return the index of the
smallest distance for each row in d and obtain the index set,
ID. ID is defined in Eq. (9).

ID = argmin(d) = [ID1, ID2, ..., IDNconn]

(9)

where, IDi is the index of the smallest distance for Ui.

Based on ID, we easily obtain the LSD weight set, WLSD,

from W. WLSD is defined in Eq. (10).

WLSD = [W LSD,k

1

, W LSD,k

2

, ..., W LSD,k
Nconn

]

= [W(ID1), W(ID2), ..., W(IDNconn)]

(10)

where, W LSD,k
the k-th federated learning epoch.

i

are the weights matched with those of Ui at

Once Ui receives W LSD,k

from the server, Ui loads these
weights to its teacher's hidden layers at the beginning of the
next federated learning epoch, as defined in Eq. (11).

i

W Thidden,k+1

i

← W LSD,k

i

(11)

Alg. 1 and Alg. 2 show the user and server implementa-

tion procedures, respectively.

3.4 Communication Overhead
EFDLS does not launch the DBWM scheme unless the
weights from all the Nconn connected users are received.
It helps reduce the interaction between the server and
users, promoting the system's service efficiency. For user
Ui, i = 1, 2, ..., Nconn, we analyze the communication over-
head of uploading and downloading its weights. Denote the

5

i

i

i

;

else

⊂ W S,k

if k == 1 then

// The student model is trained alone
Obtain W S,k
// Upload its hidden layers' weights to server
Upload W Shidden,k

after the initial local training;

if receiveServer(Active)==1 then

Initialize all global variables;
for k = 1 to F LEs do

Algorithm 1 EFDLS User Implementation Procedure
1: procedure USERPROCEDURE(Ui, F LEs)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end procedure

// Connect to the EFDLS server
Receive W LSD,k
Load W LSD,k
Compute Li by Eq. (5);
Update W S,k+1
Upload W Shidden,k+1

Disconnect from the EFDLS server.

using the gradient decent;

to the teacher model;

⊂ W S,k+1

end for

end if

end if

else

;

;

i

i

i

i

i

// Run on the server;
Clear and initialize W;
for i = 1 to Nconn do

Initialize all global variables;
Set W = ∅;
for k = 1 to F LEs do

Algorithm 2 EFDLS Server Implementation Procedure
1: procedure SERVERPROCEDURE(Ntot, Nconn, F LEs)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end procedure

// Receive model weights from users;
Receive W Shidden,k
Include W Shidden,k

end for
for i = 1 to Nconn do

Obtain W LSD,k
Send W LSD,k

;
in W.

end for

end for

to Ui.

i

i

i

i

based on W by Eqs. (6)-(10);

bandwidth requirement for uploading the student model's
hidden layers' weights of Ui once by BW . Clearly, the band-
width requirement for downloading the student model's
hidden layers' weights from the server once is also BW .
That is because, for an arbitrary connected user, the weights
uploaded to and those downloaded from the server are
of the same size, given that each user has exactly the
same model structure. At each federated learning epoch,
the bandwidth requirement for user Ui, i = 1, 2, ..., Nconn
is estimated as BW + BW = 2BW . For Ui, its total
communication overhead is in proportion to 2BW · F LEs.
Hence, the total communication overhead is proportional to
2BW · F LEs · Nconn.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

Details of 44 selected datasets from the UCR 2018.

TABLE 1

Scale

Short

Medium

Long

Vary

Dataset

Chinatown

MelbournePedestrian
SonyAIBORobotSur.2
SonyAIBORobotSur.1
DistalPhalanxO.A.G
DistalPhalanxO.C.
DistalPhalanxTW

TwoLeadECG

MoteStrain

ECG200

CBF

DodgerLoopDay
DodgerLoopGame

DodgerLoopWeekend

CricketX
CricketY
CricketZ
FaceFour

Ham
Meat
Fish
Beef

OliveOil

Car

Lightning2
Computers

Mallat

Phoneme

StarLightCurves

MixedShapesRegularT.
MixedShapesSmallT.

ACSF1

SemgHandG.Ch2

AllGestureWiimoteX
AllGestureWiimoteY
AllGestureWiimoteZ
GestureMidAirD1
GestureMidAirD2
GestureMidAirD3
GesturePebbleZ1
GesturePebbleZ2
PickupGestureW.Z

PLAID

ShakeGestureW.Z

Train

20
1194
27
20
400
600
400
23
20
100
30
78
20
20
390
390
390
24
109
60
175
30
30
60
60
250
55
214
1000
500
100
100
300
300
300
300
208
208
208
132
146
50
537
50

Test
345
2439
953
601
139
276
139
1139
1252
100
900
80
138
138
390
390
390
88
105
60
175
30
30
60
61
250
2345
1896
8236
2425
2425
100
600
700
700
700
130
130
130
172
158
50
537
50

Class

2
10
2
2
3
2
6
2
2
2
3
7
2
2
12
12
12
4
2
3
7
5
4
4
2
2
8
39
3
5
5
10
2
10
10
10
26
26
26
6
6
10
11
10

SeriesLength

24
24
65
70
80
80
80
82
84
96
128
288
288
288
300
300
300
350
431
448
463
470
570
577
637
720
1024
1024
1024
1024
1024
1460
1500
Vary
Vary
Vary
Vary
Vary
Vary
Vary
Vary
Vary
Vary
Vary

Simulated

Type
Traffic
Traffic
Sensor
Sensor
Image
Image
Image
ECG
Sensor
ECG

Sensor
Sensor
Sensor
Motion
Motion
Motion
Image
Spectro
Spectro
Image
Spectro
Spectro
Sensor
Sensor
Device

Simulated

Sensor
Sensor
Image
Image
Device
Spectrum

Sensor
Sensor
Sensor

Trajectory
Trajectory
Trajectory

Sensor
Sensor
Sensor
Device
Sensor

4 PERFORMANCE EVALUATION
This section first introduces the experimental setup and
performance metrics and then focuses on the ablation study.
Finally, the performance of EFDLS is evaluated.

4.1 Experimental Setup
4.1.1 Data Description
The UCR 2018 archive is one of the most popular time series
repositories with 128 datasets of different lengths in various
application domains [58]. Following the previous work [53],
we divide the UCR 2018 archive into 4 categories with
respect to dataset length, namely, 'short', 'medium', 'long',
and 'vary'. The length of a 'short' dataset is no more than
200. That of a 'medium' one varies from 200 to 500. A 'long'
one has a length of over 500 while a 'vary' one has an indef-
inite length. The 128 datasets are composed of 41 'short' , 32

'medium', 44 'long', and 11 'vary' datasets. Unfortunately,
our limited computing resources do not allow us to consider
the whole 128 datasets (detailed hardware specification
can be found in Subsection Implementation Details). There
are seven algorithms for performance comparison and the
average training time on the 128 datasets costed more than
32 hours for a single federated learning epoch. So, we select
11 datasets from each category, resulting in 44 datasets
selected. More details are found in Table 1.

4.1.2 Implementation Details
Following previous studies [8], [9], [10], [11], [53], we set
the decay value of batch normalization to 0.9. We use the
L2 regularization to avoid overfitting during the training
process. Meanwhile, we adopt the AdamOptimizer with
Pytorch1, where the initial learning rate is set to 0.0001.

1. https://pytorch.org/

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

Experimental results of different algorithms on 44 datasets when Nconn = 44 and Ntot = 44.

TABLE 2

Dataset

Chinatown

MelbournePedestrian
SonyAIBORobotSur.2
SonyAIBORobotSur.1
DistalPhalanxO.A.G
DistalPhalanxO.C.
DistalPhalanxTW

TwoLeadECG
MoteStrain
ECG200

CBF

DodgerLoopDay
DodgerLoopGame

DodgerLoopWeekend

CricketX
CricketY
CricketZ
FaceFour

Ham
Meat
Fish
Beef

OliveOil

Car

Lightning2
Computers

Mallat
Phoneme

StarLightCurves

MixedShapesRegularT.
MixedShapesSmallT.

ACSF1

SemgHandG.Ch2

AllGestureWiimoteX
AllGestureWiimoteY
AllGestureWiimoteZ
GestureMidAirD1
GestureMidAirD2
GestureMidAirD3
GesturePebbleZ1
GesturePebbleZ2
PickupGestureW.Z

PLAID

ShakeGestureW.Z

Win
Tie
Lose
Best

MeanACC
AVG rank

Baseline
0.9623
0.9139
0.8961
0.8652
0.6763
0.75
0.6547
0.7463
0.7788
0.86
0.987
0.575
0.6884
0.8261
0.5897
0.5051
0.6205
0.6477
0.7143
0.8667
0.5657
0.7667
0.8333
0.5833
0.7869
0.78
0.7446
0.2231
0.9534
0.8586
0.8029
0.77
0.7067
0.2643
0.2585
0.2886
0.5538
0.4231

0.3

0.4419
0.4241
0.56
0.203
0.92
4
1
39
5

0.6622
3.5455

FedAvg
0.2754

0.1
0.383
0.5707
0.1079
0.417
0.1295
0.4996
0.5391
0.36
0.3333
0.15
0.5217
0.7391
0.0692
0.0949
0.0846
0.1591
0.4857
0.3333
0.1371

0.2
0.167
0.233
0.459
0.5

0.1254
0.02
0.1429
0.1889
0.1889

0.1
0.65
0.1
0.1
0.1

0.0384
0.0384
0.0384
0.1628
0.1519

0.0615

0.1

0.1
0
0
44
0

7.5

0.2377

0.1
0.383
0.6619
0.1079
0.6619
0.1295
0.4996
0.5391
0.36
0.5911
0.15
0.5217
0.7391
0.1371
0.1357
0.0846
0.1591
0.4857
0.3333
0.1371

0.2
0.167
0.233
0.459
0.5

0.1254
0.02
0.1429
0.1889
0.1889
0.19
0.65
0.1
0.1
0.1

0.0384
0.0384
0.0384
0.1628
0.1519

0.0615

0.1

0.1
0
0
44
0

0.2557
7.3409

0.2754

FedAvgM FedGrad
0.9623
0.7784
0.8363
0.7887
0.6187
0.6776
0.554
0.7305
0.6933

FTL
0.9665
0.8486
0.8688
0.8236
0.6259
0.7464
0.6259
0.7287
0.7923
0.84
0.973
0.55
0.7826
0.8841
0.5667

0.5

0.5692
0.6591
0.7048
0.8333
0.5771

0.7

0.8667
0.5667
0.7869
0.688
0.7638
0.2147
0.9519
0.8384
0.7942
0.82
0.72
0.2729
0.3186
0.2671
0.5462
0.4154
0.2693
0.4767
0.5126
0.62
0.2198
0.96
3
2
39
5

0.6604
3.9204

FTLS
0.9537
0.8922
0.9035
0.8702
0.6475
0.7465
0.6547
0.7278
0.8283
0.85
0.9922
0.525
0.7609
0.8913
0.6128
0.4949

0.6

0.6932
0.7143
0.8333

0.6
0.7

0.8667
0.5833
0.8033
0.748
0.7539
0.2247
0.9584
0.8598
0.8062
0.89
0.7383
0.3043
0.3029
0.29
0.5538
0.4462
0.2615
0.4826
0.557
0.6

0.2253
0.92
7
1
36
8

0.6743
2.8977

FKD
0.9275
0.9379
0.915
0.8369
0.6691
0.7536
0.6835
0.8112
0.8163
0.87
0.9922
0.5125
0.7609
0.913
0.659
0.5538
0.6692
0.6932
0.7048

0.9
0.6
0.7

0.8333
0.5667
0.7541
0.788
0.7906
0.2859
0.9571
0.8643
0.8318
0.87
0.6867
0.2929
0.2529
0.4014
0.4615
0.4692
0.2231

0.6013

0.5

0.7

0.2924
0.96
10
1
33
11

0.6878
2.6364

EFDLS
0.9478
0.9453
0.8961
0.8819
0.6475
0.7428
0.6403
0.7665
0.8203
0.85
0.9956
0.5375
0.7464
0.9203
0.6718
0.5974
0.7256
0.6818
0.6952
0.917
0.6229
0.7667
0.8333
0.6333
0.7869
0.804
0.8299
0.2954
0.9582
0.8907
0.8388
0.88
0.72
0.2914
0.2829
0.3786
0.5769
0.5308
0.2769
0.4883
0.5886
0.74
0.2589
0.96
18
2
24
20

0.7014
2.1478

0.8

0.5911
0.3875
0.6232
0.7319
0.2256
0.1949
0.2256
0.4659
0.6762
0.7333
0.2857
0.5667

0.7
0.5

0.7705
0.584
0.4141
0.1108
0.5062
0.2223
0.2421
0.19
0.555
0.1371
0.1357
0.1343
0.0923
0.0923
0.0923
0.2558
0.2722
0.24
0.0615

0.1
0
0
44
0

0.4445
6.0113

All experiments were conducted on a desktop with an
Nvidia GTX 1080Ti GPU with 11GB memory, and an AMD
R5 1400 CPU with 16G RAM under the Ubuntu 18.04 OS.

4.2 Performance Metrics
To evaluate FL algorithms' performance, we use three
well-known metrics:
'win'/'tie'/'lose', mean accuracy
(MeanACC), and AVG rank, all based on the top-1 accuracy.
For an arbitrary algorithm, its 'win', 'tie', and 'lose' values
indicate on how many datasets it is better than, equal to,

and worse than the others, respectively; its 'best' value is
the summation of the corresponding 'win' and 'tie' values.
The AVG rank score reflects the average difference between
the accuracy values of a model and the best accuracy values
among all models [9], [10], [11], [12], [56].

4.3 Ablation Study
We use the 44 UCR2018 datasets above to study the impact
of parameter settings on the performance of EFDLS. Assume
there are 44 users in the system, i.e., Ntot = 44. Each user

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

4.4 Experimental Analysis
To evaluate the overall performance of EFDLS, we compare
it with seven benchmark algorithms listed below against
'Win'/'Lose'/'Tie', MeanACC, and AVG rank.

•

•

•

• Baseline: the single-task TSC algorithm with the
feature extractor in Fig. 1 deployed on each user.
Note that each user has a unique dataset to run and
knowledge sharing among them is disabled.
FedAvg: the FederatedAveraging method using the
feature extractor in Fig. 1 [18].
FedAvgM: the modified FedAvg using the feature
extractor in Fig. 1 [27].
FedGrad: the federated gradient method using the
feature extractor in Fig. 1 [16].
FTL: the federated transfer learning method using
the feature extractor in Fig. 1 [23].
FTLS: FTL [23] based on the DBWM scheme using
the feature extractor in Fig. 1.
FKD: the federated knowledge distillation using the
feature extractor in Fig. 1 [27], [28]. For fair com-
parison, FKD uses the same student-teacher network
structure as EFDLS.

•

•

•

Table 2 shows the top-1 accuracy results with various
algorithms on 44 UCR2018 datasets when Nconn = 44 and
Ntot = 44. To visualize the differences between EFDLS and
the others, Fig. 4 depicts the accuracy plots of EFDLS against
each of the remaining algorithms on 44 datasets. In addition,
the AVG rank results are shown in Fig. 5.

First of all, we study the effectiveness of knowledge shar-
ing among users by comparing EFDLS with Baseline. One
can observe that EFDLS beats Baseline in every aspect,
including 'Win'/'Lose'/'Tie', MeanACC, and AVG rank.
For example, the former wins 18 out of 44 datasets while
the latter wins only 4. The accuracy plot of EFDLS vs.
Baseline in Fig. 4(a) also supports the finding above. The
main difference between EFDLS and Baseline is that the
latter only uses standalone feature extractors which do
not share the locally collected knowledge with each other.
On the other hand, with sufficient knowledge sharing of
similar expertise among users enabled, EFDLS improves the
system's generalization ability and thus achieves promising
multi-task TSC performance.

Secondly, we study the effectiveness of the FBST frame-
work by comparing EFDLS with FTLS. It is easily seen that
EFDLS outperforms FTLS regarding the 'best', MeanACC,
and AVG rank values. The accuracy plot of EFDLS vs.
FTLS in Fig. 4(f) also supports this. The FBST framework
allows efficient knowledge transfer from teacher to stu-
dent, helping the student capture sufficient discriminate
representations from the input data. On the contrary, the
FTLS's learning model lacks of self-generalization, leading
to deteriorated performance during knowledge sharing.

Thirdly, we study the effectiveness of

the DBWM
scheme by comparing EFDLS with FKD. Apparently, EFDLS
overweighs FKD with respect to 'best', MeanACC, and
AVG rank. It is backed by the accuracy plot of EFDLS vs.
FTLS in Fig. 4(g). As mentioned before, at each federated
learning epoch, the DBWM scheme finds a partner for each
user and then EFDLS offers weights exchange between each
pair of connected users, which realizes knowledge sharing

Fig. 2. MeanACC results obtained by EFDLS with different ratios of
Nconn to Ntot on 44 datasets when Ntot = 44.

Fig. 3. MeanACC results with different  values on 44 datasets when
Nconn = 44 and Ntot = 44.

runs a TSC task with data coming from a specific dataset.
For any two users, if they run identical tasks, e.g., motion
recognition, their data sources come from different datasets,
e.g., CricketX and CricketY. In the experiments, each user's
data comes from one of the 44 datasets.

4.3.1 Impact of Nconn
To investigate the impact of Nconn on the EFDLS's perfor-
mance, we select four ratios of Nconn to Ntot, namely 40%,
60%, 80%, and 100%. For example, 40% means there are 18
connected users for weights uploading, given Ntot = 44.
The MeanACC results obtained by EFDLS with different
Nconn values on 44 datasets are shown in Fig. 2. One can
easily observe that a larger Nconn tends to result in a higher
MeanACC value. That is because as Nconn increases, more
amount of time series data is made use of by the system and
thus more discriminate representations are captured.

4.3.2 Impact of 
 is a coefficient to balance each connected user's supervised
and KD losses in EFDLS. Fig. 3 shows the MeanACC results
with different  values when Nconn = 44 and Ntot = 44. It is
seen that  = 0.90 results in the highest MeanACC score, i.e.,
0.7014. That means  = 0.90 is appropriate to reduce each
user's entropy on its data during training.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

Fig. 4. Accuracy plot results showing the performance difference between two given algorithms. (a) Baseline vs. EFDLS; (b) FedAvg vs. EFDLS;
(c) FedAvgM vs. EFDLS; (d) FedGrad vs. EFDLS; (e) FTL vs. EFDLS; (f) FTLS vs. EFDLS; (g) FKD vs. EFDLS.

of similar expertise among different users. In contrast, FKD
adopts the average weights to supervise the feature extrac-
tion process in each user. It is likely to lead to catastrophic
forgetting in a user whose weights significantly differ from
the average weights.

Last but not least, we compare EFDLS with all the
seven algorithms. One can easily observe that our EFDLS
is no doubt the best among all algorithms for comparison
since ours obtains the highest MeanACC and 'best' values,
namely 0.7014 and 20, and the smallest AVG rank value,
namely 2.1478. The FKD takes the second position when
considering its 'best', MeanACC, and AVG rank values,
namely, 11, 0.6878, and 2.6364. On the other hand, FedAvg
and its variant, FedAvgM, are the two worst algorithms. The
following explains the reasons behind the findings above.
When faced with the multi-task TSC problem, each user
runs one TSC task, and different users may run different
TSC tasks. The FBST framework and the DBWM scheme
help EFDLS to realize fine-grained knowledge sharing be-
tween any pair of users with the most similar expertise.
FKD uses the average of all users' weights to guide each
user to capture valuable features from the data, promoting
coarse-grained knowledge sharing among users. On the
other hand, FedAvg and FedAvgM simply take the average
weights of all users as each user's weights, which may
cause catastrophic forgetting and hence poor performance
on multi-task TSC.

5 CONCLUSION
The FBST framework promotes knowledge transfer from a
teacher's to its student's hidden layers, helping the student
capture instance-level representations from the input. The
DBWM scheme finds a partner for each user in terms of
similarity between their uploaded weights, enabling knowl-
edge sharing of similar expertise among different users.

With FBST and DBWM, the proposed EFDLS securely shares
knowledge of similar expertise among different tasks for
multi-task time series classification. Experimental results
show that compared with six benchmark FL algorithms,
EFDLS is a winner on 44 datasets with respect to the
MeanACC and AVG rank metrics and on 20 datasets in
terms of the 'best' measure. In particular, compared with
single-task Baseline, EFDLS obtains 32/4/8 regarding the
'win'/'tie'/'lose' metric. That reflects the potential of EFDLS
to be applied to multi-task TSC problems in various real-
world domains.

REFERENCES
[1] G. Pang and C. Aggarwal, "Toward explainable deep anomaly

detection," In Proc. ACM KDD'21, p. 4056 -- 4057, 2021.
J. Li, H. He, H. He, L. Li, and Y. Xiang, "An end-to-end framework
with multisource monitoring data for bridge health anomaly iden-
tification," IEEE Trans. Instrum. Meas., vol. 70, pp. 1 -- 9, 2021.

[2]

[3] X. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Sheng, H. Xiong, and
L. Akoglu, "A comprehensive survey on graph anomaly detection
with deep learning," IEEE Trans. Knowl. Data Eng., pp. 1 -- 1, 2021.
[4] H. Tong and J. Zhu, "New peer effect-based approach for service
matching in cloud manufacturing under uncertain preferences,"
Appl. Soft Comput., vol. 94, pp. 1 -- 17, 2020.

[5] L. Shi, Z. Teng, L. Wang, Y. Zhang, and A. Binder, "Deepclue:
Visual interpretation of text-based deep stock prediction," IEEE
Trans Knowl. Data Eng., vol. 31, no. 6, pp. 1094 -- 1108, 2019.

[6] D. Nahmias and K. Kontson, "Easy perturbation eeg algorithm
for spectral importance (easypeasi): A simple method to identify
important spectral features of eeg in deep learning models," In
Proc. ACM KDD'21, p. 2398 -- 2406, 2021.

[7] F. Zhang, Y. Liu, N. Feng, C. Yang, J. Zhai, S. Zhang, B. He, J. Lin,
X. Zhang, and X. Du, "Periodic weather-aware lstm with event
mechanism for parking behavior prediction," IEEE Trans. Knowl.
Data Eng., pp. 1 -- 1, 2021.

[8] H. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller,
"Deep learning for time series classification: a review," Data Min.
Knowl. Disc., vol. 33, pp. 917 -- 963, 2019.

[9]  --  -- , "Time series classification from scratch with deep neural
networks: A strong baseline," In Proc. IEEE IJCNN 2017, pp. 1578 -- 
1585, 2017.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

Fig. 5. Critical difference diagram of the average ranks of various FL algorithms on 44 datasets.

[10] X. Zhang, Y. Gao, J. Lin, and C.-T. Lu, "Tapnet: Multivariate
time series classification with attentional prototypical network,"
In Proc. AAAI 2020, pp. 6845 -- 6852, 2020.

[11] F. Karim, S. Majumdar, H. Darabi, and S. Harford, "Multivariate
lstm-fcns for time series classification," Neural Networks, vol. 116,
pp. 237 -- 245, 2019.

[12] Z. Xiao, X. Xu, H. Xing, S. Luo, P. Dai, and D. Zhan, "Rtfn: A robust
temporal feature network for time series classification." Inform.
Sciences, vol. 571, pp. 65 -- 86, 2021.

[13] G. Li, B. Choi, J. Xu, S. Bhowmick, K.-P. Chun, and G. Wong,
"Shapenet: A shapelet-neural network approach for multivariate
time series classification," In Proc. AAAI 2021, vol. 35, no. 9, pp.
8375 -- 8383, 2021.

[14] D. Lee, S. Lee, and H. Yu, "Learnable dynamic temporal pooling
for time series classification," In Proc. AAAI 2021, vol. 35, no. 9, pp.
8288 -- 8296, 2021.

[15] B. Arcas, G. Bacon, K. Bonawitz, and et al, "Federated
learning: Collaborative machine learning without centralized
training data," https://ai.googleblog.com/2017/04/federated-learning-
collaborative.html, 2017.

[16] Q. Yang, Y. Liu, T. Chen, and Y. Tong, "Federated machine learn-
ing: Concept and applications," ACM Trans. Intell. Syst. Technol.,
vol. 10, no. 2, pp. 1 -- 19, 2019.

[17] Q. Li, Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He,
"A survey on federated learning systems: Vision, hype and reality
for data privacy and protection," IEEE Trans. Knowl. Data Eng., pp.
1 -- 1, 2021.

[18] M. McMahan, E. Moore, D. Ramage, S. Hampson, and B. Arcas,
"Communication-efficient learning of deep networks from decen-
tralized data," In Proc. AISTATS 2017, pp. 1 -- 11, 2017.

[19] J. Ma, Q. Zhang, J. Lou, L. Xiong, and J. Ho, "Communication ef-
ficient federated generalized tensor factorization for collaborative
health data analytics," In Proc. 30th The Web Conference 2021, 2021.
[20] B. Liu, Y. Guo, and X. Chen, "Pfa: Privacy-preserving federated
adaptation for effective model personalization," In Proc. 30th The
Web Conference 2021, 2021.

[21] J. Wu, Z. Huang, Y. Ning, H. Wang, E. Chen, J. Yi, and B. Zhou,
"Hierarchical personalized federated learning for user modeling,"
In Proc. 30th The Web Conference 2021, 2021.

[22] Q. Yang, J. Zhang, W. Hao, G. Spell, and L. Carin, "Flop: Federated
learning on medical datasets using partial networks," In Proc.
ACM KDD'21, 2021.

[23] Y. Liu, Y. Kang, C. Xing, T. Chen, and Q. Yang, "A secure federated
transfer learning framework," IEEE Intell. Syst., vol. 35, no. 4, pp.
70 -- 82, 2020.

[24] H. Yang, H. He, W. Zhang, and X. Cao, "Fedsteg: A federated
transfer learning framework for secure image steganalysis," IEEE
Trans. Netw. Sci. Eng., vol. 8, no. 2, pp. 1084 -- 1094, 2021.

[25] D. Dimitriadis, K. Kumatani, R. Gmyr, Y. Gaur, and S. Eskimez,
"Federated transfer learning with dynamic gradient aggregation,"
arXiv preprint arXiv:2008.02452, 2020.

[26] U. Majeed, S. Hassan, and C. Hong, "Cross-silo model-based
secure federated transfer learning for flow-based traffic classifi-
cation," In Proc. ICOIN 2021, 2021.

[27] H. Seo, J. Park, S. Oh, M. Bennis, and S.-L. Kim, "Federated

knowledge distillation," arXiv preprint arXiv:2011.02367, 2020.

[28] C. He, M. Annavaram, and S. Avestimehr, "Group knowledge
transfer: Federated learning of large cnns at the edge," In Proc.
NeurIPS 2020, 2020.

[29] R. Mishra, H. Gupta, and T. Dutta, "A network resource aware
federated learning approach using knowledge distillation," In
Proc. INFOCOM 2021, 2021.

[30] S.

Itahara, T. Nishio, Y. Koda, M. Morikura, and K. Ya-
mamoto, "Distillation-based semi-supervised federated learning
for communication-efficient collaborative training with non-iid
private data," IEEE Trans. Mobile Comput., pp. 1 -- 1, 2021.

[31] Y. Chen, X. Sun, and Y. Jin, "Communication-efficient federated
deep learning with layer-wise asynchronous model update and
temporally weighted aggregation," IEEE Trans. Neur. Net. Learn.
Sys., vol. 31, no. 10, pp. 4229 -- 4238, 2020.

[32] F. Sattler, S. Wiedemann, K.-R. M uller, and W. Samek, "Robust and
communication-efficient federated learning from non-i.i.d. data,"
IEEE Trans. Neur. Net. Learn. Sys., vol. 31, no. 9, pp. 3400 -- 3413,
2020.

[33] L. Nagalapatti and R. Narayanam, "Game of gradients: Mitigat-
ing irrelevant clients in federated learning," In Proc. AAAI 2021,
vol. 35, no. 10, pp. 9046 -- 9054, 2021.

[34] X. Cao, J. Jia, and N. Gong, "Provably secure federated learning
against malicious clients," In Proc. AAAI 2020, vol. 35, no. 8, pp.
6885 -- 6893, 2020.

[35] J. Hong, Z. Zhu, S. Yu, Z. Wang, H. Dodge, and J. Zhou, "Federated
adversarial debiasing for fair and transferable representations," In
Proc. ACM KDD'21, vol. 1, no. 1, August 2021.

[36] P. Zhou, L. Wang, L. Guo, S. Gong, and B. Zheng, "A privacy-
preserving distributed contextual federated online learning frame-
work with big data support in social recommender systems," IEEE
Trans. Knowl. Data Eng., vol. 33, no. 3, pp. 824 -- 838, 2021.

[37] Z. Pan, L. Hu, W. Tang, J. Li, Y. He, and Z. Liu, "Privacy-
preserving multi-granular federated neural architecture search a
general framework," IEEE Trans. Knowl. Data Eng., pp. 1 -- 1, 2021.
[38] M. Crawshaw, "Multi-task learning with deep neural networks: A

survey," arXiv preprint arXiv: 2009.09796, 2020.

[39] A. Ruiz, M. Flynn, and A. Bagnall, "Benchmarking multivari-
ate time series classification algorithms," arXiv preprint arXiv:
2007.13156, 2020.

[40] J. Lines and A. Bagnall, "Time series classification with ensembles
of elastic distance measures," Data Min. Knowl. Disc., vol. 29, p.
565 -- 592, 2015.

[41] A. Bagnall, J. Lines, J. Hills, and A. Bostrom, "Time series classi-
fication with cote: the collective of transformation-based ensem-
bles," In Proc. ICDE 2016, pp. 1548 -- 1549, 2016.

[42] J. Lines, S. Taylor, and A. Bagnall, "Time series classification with
hive-cote: the hierarchical of transformation-based ensembles,"
ACM Trans. Knowl. Discov. D, vol. 21, no. 52, pp. 1 -- 35, 2018.

[43] K. Fauvel, E. Fromont, V. Masson, P. Faverdin, and A. Termier, "Lo-
cal cascade ensemble for multivariate data classification," arXiv
preprint arXiv:2005.03645, 2020.

[44] J. Lines, L. Davis, J. Hills, and A. Bagnall, "A shapelet transform

for time series classification," In Proc. ACM KDD'12, 2012.

[45] M. Baydogan, G. Runger, and E. Tuv, "A bag-of-features frame-
work to classify time series," IEEE Trans. Pattern Anal., vol. 35,
no. 11, pp. 2796 -- 2802, 2013.

[46] A. Dempster, D. Schmidt, and G. Webb, "Minirocket: A very fast
(almost) deterministic transform for time series classification," In
Proc. ACM KDD'21, 2021.

[47] M. Baydogan and G. Runger, "Time series representation and
similarity based on local auto patterns," Data Min. Knowl. Disc.,
vol. 30, p. 476 -- 509, 2016.

[48] J. Large, A. Bagnall, S. Malinowski, and R. Tavenard, "From bop to
boss and beyond: time series classification with dictionary based
classifier," arXiv preprint arXiv:1809.06751, 2018.

[49] W. Pei, H. Dibeklioglu, D. Tax, and L. van der Maaten, "Multivari-
ate time-series classification using the hidden-unit logistic model,"
IEEE Trans. Neur. Net. Lear., vol. 29, no. 4, pp. 920 -- 931, 2018.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

[50] H. Deng, G. Runger, E. Tuv, and M. Vladimir, "A time series forest
for classification and feature extraction," Inform. Sciences, vol. 239,
p. 142 -- 153, 2013.

[51] B. Bai, G. Li, S. Wang, Z. Wu, and W. Yan, "Time series clas-
sification based on multi-feature dictionary representation and
ensemble learning," Expert Syst. Appl., vol. 169, pp. 1 -- 10, 2021.

[52] H. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. Schmidt, J. Weber,
G. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean, "Incep-
tiontime: finding alexnet for time series classification," Data Min.
Knowl. Disc., vol. 34, p. 1936 -- 1962, 2020.

[53] Z. Xiao, X. Xu, H. Zhang, and E. Szczerbicki, "A new multi-process
collaborative architecture for time series classification," Knowl.-
Based Syst., vol. 220, pp. 1 -- 11, 2021.

[54] W. Chen and K. Shi, "Multi-scale attention convolutional neural
network for time series classification," Neural Networks, vol. 136,
pp. 126 -- 140, 2021.

[55] S. Huang, L. Xu, and C. Jiang, "Artificial intelligence and ad-
vanced time series classification: Residual attention net for cross-
domain modeling," Fintech with Artificial Intelligence, Big Data, and
Blockchain, Blockchain Technologies, 2021.

[56] Z. Xiao, X. Xu, H. Xing, R. Qu, F. Song, and B. Zhao, "Rnts:
Robust neural temporal search for time series classification," In
Proc. IJCNN 2021, 2021.

[57] J. Guo, B. Yu, S. Maybank, and D. Tao, "Knowledge distillation: A

survey," arXiv preprint arXiv: 2006.05525, 2020.

[58] H. Dau, A. Bagnall, C.-C. M. Yeh, Y. Zhu, S. Gharghabi,
C. Ratanamahatana, and E. Keogh, "The ucr time series archive,"
IEEE/CAA Journal of Automatica Sinica, vol. 6, no. 6, pp. 1293 -- 1305,
2019.

