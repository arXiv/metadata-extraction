{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete: 'Russia' replaced with 'Russian Federation'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"matching_data/ror_id.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace \"Russia\" with \"Russian Federation\" in the \"Country Name\" column\n",
    "df[\"Country Name\"] = df[\"Country Name\"].replace(\"Russian Federation\", \"Russia\")\n",
    "\n",
    "# Save the modified CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Preprocessing complete: 'Russian Federation' replaced with 'Russia'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete: 'Russian Federation' replaced with 'Russia'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"matching_data/scopus_id.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace \"Russia\" with \"Russian Federation\" in the \"Country Name\" column\n",
    "df[\"Country Name\"] = df[\"Country Name\"].replace(\"Russian Federation\", \"Russia\")\n",
    "\n",
    "# Save the modified CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Preprocessing complete: 'Russian Federation' replaced with 'Russia'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1, exact matching only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed. Results saved to 'matching_data/matched_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Step 1: Read ror_id.csv and create the hashmap\n",
    "ror_map = {}\n",
    "\n",
    "with open('/home/rubiscol/metadata-extraction/matching_data/ror_id.csv', mode='r', encoding='utf-8') as ror_file:\n",
    "    ror_reader = csv.DictReader(ror_file)\n",
    "    for row in ror_reader:\n",
    "        country = row['Country Name']\n",
    "        institution = row['Institution Name']\n",
    "        ror_id = row['ROR ID']\n",
    "        \n",
    "        if country not in ror_map:\n",
    "            ror_map[country] = {}\n",
    "        ror_map[country][institution] = ror_id\n",
    "\n",
    "# Step 2: Read scopus_id.csv and match ROR IDs\n",
    "matched_data = []\n",
    "\n",
    "with open('/home/rubiscol/metadata-extraction/matching_data/scopus_id.csv', mode='r', encoding='utf-8') as scopus_file:\n",
    "    scopus_reader = csv.DictReader(scopus_file)\n",
    "    for row in scopus_reader:\n",
    "        country = row['Country Name']\n",
    "        primary_org = row['Primary Org Name']\n",
    "        scopus_id = row['Scopus ID']\n",
    "        \n",
    "        ror_id = None\n",
    "        if country in ror_map and primary_org in ror_map[country]:\n",
    "            ror_id = ror_map[country][primary_org]\n",
    "        \n",
    "        matched_data.append({\n",
    "            'Scopus ID': scopus_id,\n",
    "            'Primary Org Name': primary_org,\n",
    "            'Country Name': country,\n",
    "            'ROR ID': ror_id\n",
    "        })\n",
    "\n",
    "# Step 3: Save the matched data to a new CSV file\n",
    "with open('/home/rubiscol/metadata-extraction/matching_data/matched_results.csv', mode='w', encoding='utf-8', newline='') as output_file:\n",
    "    fieldnames = ['Scopus ID', 'Primary Org Name', 'Country Name', 'ROR ID']\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for row in matched_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Matching completed. Results saved to 'matching_data/matched_results.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2, exact matching combined with elastic searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ROR data: 225636it [00:00, 361624.02it/s]\n",
      "Matching Scopus data: 13365it [00:00, 274127.46it/s]\n",
      "Writing results: 100%|██████████| 13365/13365 [00:00<00:00, 281810.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching completed. Results saved to 'matching_data/matched_results_elastic.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "# Step 1: Read ror_id.csv and create the hashmap\n",
    "ror_map = {}\n",
    "\n",
    "with open('matching_data/ror_id.csv', mode='r', encoding='utf-8') as ror_file:\n",
    "    ror_reader = csv.DictReader(ror_file)\n",
    "    for row in tqdm(ror_reader, desc=\"Reading ROR data\"):  # Add progress bar\n",
    "        country = row['Country Name']\n",
    "        institution = row['Institution Name']\n",
    "        ror_id = row['ROR ID']\n",
    "        \n",
    "        if country not in ror_map:\n",
    "            ror_map[country] = {}\n",
    "        ror_map[country][institution] = ror_id\n",
    "\n",
    "# Step 2: Read scopus_id.csv and match ROR IDs\n",
    "matched_data = []\n",
    "\n",
    "with open('matching_data/scopus_id.csv', mode='r', encoding='utf-8') as scopus_file:\n",
    "    scopus_reader = csv.DictReader(scopus_file)\n",
    "    for row in tqdm(scopus_reader, desc=\"Matching Scopus data\"):  # Add progress bar\n",
    "        country = row['Country Name']\n",
    "        primary_org = row['Primary Org Name']\n",
    "        scopus_id = row['Scopus ID']\n",
    "        \n",
    "        ror_id = None\n",
    "        if country in ror_map:\n",
    "            # Try exact match first\n",
    "            if primary_org in ror_map[country]:\n",
    "                ror_id = ror_map[country][primary_org]\n",
    "            # else:\n",
    "            \n",
    "        \n",
    "        matched_data.append({\n",
    "            'Scopus ID': scopus_id,\n",
    "            'Primary Org Name': primary_org,\n",
    "            'Country Name': country,\n",
    "            'ROR ID': ror_id\n",
    "        })\n",
    "\n",
    "# Step 3: Save the matched data to a new CSV file\n",
    "with open('matching_data/matched_results.csv', mode='w', encoding='utf-8', newline='') as output_file:\n",
    "    fieldnames = ['Scopus ID', 'Primary Org Name', 'Country Name', 'ROR ID']\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for row in tqdm(matched_data, desc=\"Writing results\"):  # Add progress bar\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Matching completed. Results saved to 'matching_data/matched_results_elastic.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating ROR IDs:   2%|▏         | 284/13365 [01:07<1:01:58,  3.52it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse\n",
    "import time\n",
    "from tqdm import tqdm  # Import progress bar\n",
    "\n",
    "# Constants\n",
    "CSV_FILE = \"/home/rubiscol/metadata-extraction/matching_data/matched_results.csv\"\n",
    "ROR_API_ENDPOINT = \"https://api.dev.ror.org/organizations\"\n",
    "ES_RESERVED_CHARS = [\"+\", \"-\", \"&\", \"|\", \"!\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\", \"^\", '\"', \"~\", \"*\", \"?\", \":\", \"\\\\\", \"/\"]\n",
    "RATE_LIMIT = 1800  # Max API calls allowed in 5 minutes\n",
    "TIME_WINDOW = 300   # 5 minutes in seconds\n",
    "\n",
    "\n",
    "def search_institution(name):\n",
    "    \"\"\"Search for an institution's ROR ID by name.\"\"\"\n",
    "    for char in ES_RESERVED_CHARS:\n",
    "        name = name.replace(char, \"\\\\\" + char)\n",
    "    \n",
    "    params = {'query': name}\n",
    "    response = requests.get(f\"{ROR_API_ENDPOINT}?{urllib.parse.urlencode(params)}\").json()\n",
    "    \n",
    "    if response.get('number_of_results', 0) > 0:\n",
    "        return response['items'][0]['id']\n",
    "    return None\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Count API calls to respect rate limits\n",
    "api_calls = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each row with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Updating ROR IDs\"):\n",
    "    if pd.isna(row['ROR ID']):  # If ROR ID is empty\n",
    "        ror_id = search_institution(row['Primary Org Name'])\n",
    "        if ror_id:\n",
    "            df.at[index, 'ROR ID'] = ror_id\n",
    "            df.to_csv(CSV_FILE, index=False)  # Save after each API call\n",
    "        \n",
    "        # Rate limiting logic\n",
    "        api_calls += 1\n",
    "        if api_calls >= RATE_LIMIT:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time < TIME_WINDOW:\n",
    "                sleep_time = TIME_WINDOW - elapsed_time\n",
    "                print(f\"Rate limit reached. Sleeping for {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            api_calls = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "print(\"CSV file updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with empty ROR ID: 130\n",
      "Percentage of rows with empty ROR ID: 0.97%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"/home/rubiscol/metadata-extraction/matching_data/matched_results.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a mask for rows where 'ROR ID' is empty (NaN or only whitespace)\n",
    "empty_mask = df[\"ROR ID\"].isna() | (df[\"ROR ID\"].astype(str).str.strip() == \"\")\n",
    "empty_count = empty_mask.sum()\n",
    "\n",
    "# Calculate the total number of rows\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate the percentage of rows with empty ROR ID\n",
    "percentage_empty = (empty_count / total_rows * 100) if total_rows > 0 else 0\n",
    "\n",
    "print(\"Number of rows with empty ROR ID:\", empty_count)\n",
    "print(\"Percentage of rows with empty ROR ID: {:.2f}%\".format(percentage_empty))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
